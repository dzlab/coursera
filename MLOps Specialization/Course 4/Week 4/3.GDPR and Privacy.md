# GDPR and Privacy

## Responsible AI
emerging issues concerning responsible AI and what you as a developer can do to ensure that your models and applications are as responsible as possible.
 
### Responsible AI Practices
The development of AI is creating new opportunities to improve the lives of people around the world from business to health care to education and beyond. But at the same time it's also raising new questions about the best way to build fairness, interpret ability, privacy and security into these systems. These questions are far from solved and are extremely active areas of research and development. I encourage you to commit to following the development of this field and working to make sure that your models and applications are as responsible as you can make them. They will never be perfect, but there is already a lot that you can do with more tools and techniques being developed constantly. 
![image](https://user-images.githubusercontent.com/1645304/134751778-8ccb3d83-9d0c-49f4-9c98-03dab490f687.png)

### Human-centered design

The way actual users experience your system is essential to assessing the true impact of its predictions, recommendations and decisions. For example, you should design your features with appropriate disclosures, built in clarity and control is crucial to a good user experience.

Often it's also a good idea to consider augmentation and assistance, producing a single answer can be appropriate where there is a high probability that the answer satisfies a diversity of users and use cases but in other cases it may be better for your system to suggest a few options to the user. In fact it can even be easier since it's often much more difficult to achieve good precision at one answer top one versus precision and a few answers Maybe top three.

Try to plan for modeling potential adverse feedback early in the design process followed by specific live testing and iteration for a small fraction of traffic before full deployment.

And finally engage with a diverse set of users and different use case scenarios and incorporate that feedback both before and throughout your project development. This will build a rich variety of user's perspectives into the project and increase the number of people who benefit from the technology and help you catch potential issues early.

![image](https://user-images.githubusercontent.com/1645304/134751901-326e2a7c-d30e-4629-818c-c86e4c582a5c.png)

### Identify Multiple Metrics

A fairly simple technique is to use several metrics rather than a single one, which can help you understand trade offs between different kinds of errors and experiences.
Consider metrics including:
- feedback from users surveys,
- quantities that track overall system performance,

Also short and long term product health for example,
- click through rate and
- customer lifetime value respectively and
- false positive and false negative rates sliced across different subgroups.

Of course the metrics that you select are important, you should try to ensure that your metrics are appropriate for the context and goals of your system. For example, a fire alarm system should have high recall even if that means the occasional false alarm.

![image](https://user-images.githubusercontent.com/1645304/134752218-da442fe2-d9cf-4ec8-8344-9e207f45e23c.png)


### Analyze your raw data carefully

Of course, as always it all comes back to the data ML models will reflect the data that they're trained on. So analyze your raw data carefully to ensure you understand it in cases where this is not possible.

For example with sensitive raw data, understand your input data as much as possible while respecting privacy. For example, by computing aggregate anonymized summaries, consider whether your data was sampled in a way that represents your users. For example, if your application will be used by people of all ages, but you only have training data from senior citizens, it might not work that well for other age groups.

Imagine doing music recommendations when all of your data is from senior citizens. My guess is that it might not perform that well for tweens, sometimes you're using your model to predict a proxy label for the actual target that you're interested in because labelling for the actual target is difficult or impossible. In these cases consider the relationship between the data labels that you have and the actual thing that you're trying to predict.

Are there problematic gaps? For example, if you're using data label X as a proxy to predict target Y, in which case is is the gap between X and Y problematic.


![image](https://user-images.githubusercontent.com/1645304/134752380-a98c087f-29d1-4841-ab2b-43041cdea78f.png)

## Reading: Responsible AI
New technologies always bring new challenges. Ensuring that your applications adhere to responsible AI is a must. Please read this [resource](https://ai.google/responsibilities/responsible-ai-practices/) to keep yourself updated with this fascinating active research  subject.

## Legal Requirements for Secure and Private AI
A legal side to practicing responsible AI. They are already a legal requirements in some countries and regions, and this trend is growing. Exposure to civil liability is another concern.

### Legal Implications of Data Security and Privacy
Training data, prediction requests, or both, can contain very sensitive information about people. For prediction request, those people are your users.

Privacy of sensitive data should be protected:
- This includes not only respecting the legal and regulatory requirements,
- but also considering social norms and typical individual expectations.

What safeguards do you need to put in place to ensure the privacy of individuals considering that ML models may remember or reveal aspects of the data that they've been exposed to?

What steps are needed to ensure users have adequate transparency and control of their data?

It's not just up to you to decide what is required:
- In Europe, for example, you need to comply with the General Data Protection Regulations, or GDPR, and
- in California, you need to comply with the California Consumer Privacy Act, or CCPA.

![image](https://user-images.githubusercontent.com/1645304/134752516-4dd8dbea-ecd5-45af-9312-e1a3aa2c363f.png)

### General Data Protection Regulation (GDPR)
The General Data Protection Regulation, or GDPR, was enacted by the EU in 2016 and became a model for many national laws outside the EU, including Chile, Japan, Brazil, South Korea, Argentina, and Kenya. It regulates the data protection and privacy in the European Union and the European Economic Area.

The GDPR gives individuals control over their personal data and requires that companies should protect the data of employees and consumers.

When data processing is based on consent, the data subject, usually an individual person, has the right to revoke their consent at any time.

![image](https://user-images.githubusercontent.com/1645304/134752611-420a76f8-1fb3-49c4-a9a0-e752cf7dd4be.png)

### California Consumer Privacy Act (CCPA)
In California, Consumer Privacy Act, or CCPA, was modeled after the GDPR and has similar goals, including:
- enhancing the privacy rights and
- consumer protections for residents of California.

It states that users have the right to know:
- what personal data is being collected about them, including whether the personal data is sold or disclosed in some way,
- who supplied their data and
- who received their data.

Users can access the personal data which a company has for them, block the sale of their data, and request a business to delete their data.

![image](https://user-images.githubusercontent.com/1645304/134752676-604b54e7-ebc7-42f1-b24a-fe4bfd72bd4a.png)

### Securit and Privacy Harms from ML Models
Security and privacy are closely linked for some problems or harms and machine learning. Informational harms are caused when information is allowed to leak from the model. There are at least three different types of informational harms, including:
- **membership inference**, where an attacker can determine whether or not an individual's data was included in the training set.
- **Model inversion**, where the attackers actually able to recreate the training set, and
- **model extraction**, where an attacker is able to recreate the model itself.

Behavioral harms are caused when an attacker is able to change the behavior of the model itself. This includes:
- poisoning attacks, where the attacker is able to insert malicious data into the training set, and
- evasion attacks where the attacker makes small changes to prediction requests to cause the model to make bad predictions.

![image](https://user-images.githubusercontent.com/1645304/134753247-731ac3a9-a8b6-417d-b7a0-7c96ec15be52.png)

### Defenses
It's important to defend your model against attacks, as well as ensuring privacy and security of user data. Let's discuss a few approaches for defending against attacks.

![image](https://user-images.githubusercontent.com/1645304/134753331-72ae4ab4-328c-4807-9507-b79703e0ac69.png)

### Cryptography
Let's discuss a few approaches for defending against attacks.

You should consider privacy enhancing technologies such as Secure Multi-Party Computation, or SMPC, or Fully Homomorphic Encryption, or FHE, when training and serving your models. Briefly,
- SMPC enables multiple systems to collaborate securely to train and/or serve a model while keeping the actual data secure through the use of shared secrets. 
- FHE, on the other hand, enables developers to train their models on encrypted data without decrypting it first.

FHE in particular allows users to send an encrypted prediction requests and receive back an encrypted results. During the entire process, the data is never decrypted except by the user. However, you should be aware that currently, FHE is very computationally expensive. The goal here is that using cryptography, you can protect the confidentiality of your training data.

![image](https://user-images.githubusercontent.com/1645304/134754351-d31050e6-6752-4780-b426-6e1dc343fb57.png)

### Differential Privacy
Roughly, a model is differentially private if an attacker seeing its predictions cannot tell if a particular user's information was included in the training data. By implementing differential privacy, you can responsibly train models on private data. It provides provable guarantees of privacy, mitigating the risk of exposing sensitive training data.

Let's briefly discuss three different approaches to implementing differential privacy:
- Differentially-Private Stochastic Gradient Descent , or DP-SGD,
- Private Aggregation of Teacher Ensembles or PATE, and
- Confidential and Private Collaborative learning, or CaPC. 

![image](https://user-images.githubusercontent.com/1645304/134754503-d7aa4630-580e-4536-8cf3-bbdc662c5d06.png)

### Differentially-Private Stochastic Gradient Descent (DP-SGD)
If an attacker is able to get a copy of a normally trained model, then they can use the weights to extract private information. Differentially-Private Stochastic Gradient Descent, or DP-SGD, eliminates that possibility by applying differential privacy throughout training.

It does that by modifying the minibatch stochastic optimization process by adding noise. The result is a trained model which retains differential privacy because of the post-processing immunity property of differential privacy.

Post-processing immunity is a fundamental property of differential privacy. It means that regardless of how you process the models predictions, you can't affect their privacy guarantees. 

![image](https://user-images.githubusercontent.com/1645304/134754556-9df16171-5a1e-4926-81bd-a656aed41f4e.png)

### Private Aggregation of Teacher Ensembles (PATE)
Next, let's take a look at Private Aggregation of Teacher Ensembles, or PATE. PATE begins by dividing up sensitive data into k partitions with no overlaps.

It then trains k models on that data separately as teacher models, and then aggregates the results in an aggregate teacher model. This is the same teacher-student used for knowledge distillation.

During the aggregation for the aggregate teacher, you will add noise to the output in a way that won't affect the resulting predictions. All of these models and the sensitive data are not available to end users, including attackers. For deployment, you will create a student model. To train the student model, you'll take unlabeled public data and feed it to the aggregate teacher model. The output of this process is labeled data, which maintains privacy. You use this data as the training set for the student model.

After training, you will discard everything on the left side of this diagram and deploy only the student model for use. 


![image](https://user-images.githubusercontent.com/1645304/134754613-8a4580db-98e6-4603-84dc-543cfa7f9584.png)


### Confidential and Private Collaborative learning (CaPC)
Confidential and Private Collaborative learning, or CaPC, enables multiple developers using different data to collaborate to improve their model accuracy without sharing information. This preserves both privacy and confidentiality.

To do that, it applies techniques and principles from both cryptography and differential privacy. This includes using Homomorphic Encryption, or HE, to encrypt the prediction requests that each collaborating model receives so that information in the prediction request is not leaked. It then uses PATE to add noise to the predictions from each of the collaborating models and uses voting to arrive at a final prediction, again, without leaking information.

A great example of how CaPC can be used is to consider a group of hospitals who want to collaborate to improve each other's models and predictions. Because of healthcare privacy laws, they can't share information directly. But using CaPC, they can achieve better results while preserving the privacy and confidentiality of their patients.
 

![image](https://user-images.githubusercontent.com/1645304/134754701-51cf5a59-8712-45d5-8690-b72dbdeba216.png)
