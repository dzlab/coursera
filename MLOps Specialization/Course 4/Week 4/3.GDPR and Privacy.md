# GDPR and Privacy

## Responsible AI
emerging issues concerning responsible AI and what you as a developer can do to ensure that your models and applications are as responsible as possible.
 
### Responsible AI Practices
The development of AI is creating new opportunities to improve the lives of people around the world from business to health care to education and beyond. But at the same time it's also raising new questions about the best way to build fairness, interpret ability, privacy and security into these systems. These questions are far from solved and are extremely active areas of research and development. I encourage you to commit to following the development of this field and working to make sure that your models and applications are as responsible as you can make them. They will never be perfect, but there is already a lot that you can do with more tools and techniques being developed constantly. 
![image](https://user-images.githubusercontent.com/1645304/134751778-8ccb3d83-9d0c-49f4-9c98-03dab490f687.png)

### Human-centered design

The way actual users experience your system is essential to assessing the true impact of its predictions, recommendations and decisions. For example, you should design your features with appropriate disclosures, built in clarity and control is crucial to a good user experience.

Often it's also a good idea to consider augmentation and assistance, producing a single answer can be appropriate where there is a high probability that the answer satisfies a diversity of users and use cases but in other cases it may be better for your system to suggest a few options to the user. In fact it can even be easier since it's often much more difficult to achieve good precision at one answer top one versus precision and a few answers Maybe top three.

Try to plan for modeling potential adverse feedback early in the design process followed by specific live testing and iteration for a small fraction of traffic before full deployment.

And finally engage with a diverse set of users and different use case scenarios and incorporate that feedback both before and throughout your project development. This will build a rich variety of user's perspectives into the project and increase the number of people who benefit from the technology and help you catch potential issues early.

![image](https://user-images.githubusercontent.com/1645304/134751901-326e2a7c-d30e-4629-818c-c86e4c582a5c.png)

### Identify Multiple Metrics

A fairly simple technique is to use several metrics rather than a single one, which can help you understand trade offs between different kinds of errors and experiences.
Consider metrics including:
- feedback from users surveys,
- quantities that track overall system performance,

Also short and long term product health for example,
- click through rate and
- customer lifetime value respectively and
- false positive and false negative rates sliced across different subgroups.

Of course the metrics that you select are important, you should try to ensure that your metrics are appropriate for the context and goals of your system. For example, a fire alarm system should have high recall even if that means the occasional false alarm.

![image](https://user-images.githubusercontent.com/1645304/134752218-da442fe2-d9cf-4ec8-8344-9e207f45e23c.png)


### Analyze your raw data carefully

Of course, as always it all comes back to the data ML models will reflect the data that they're trained on. So analyze your raw data carefully to ensure you understand it in cases where this is not possible.

For example with sensitive raw data, understand your input data as much as possible while respecting privacy. For example, by computing aggregate anonymized summaries, consider whether your data was sampled in a way that represents your users. For example, if your application will be used by people of all ages, but you only have training data from senior citizens, it might not work that well for other age groups.

Imagine doing music recommendations when all of your data is from senior citizens. My guess is that it might not perform that well for tweens, sometimes you're using your model to predict a proxy label for the actual target that you're interested in because labelling for the actual target is difficult or impossible. In these cases consider the relationship between the data labels that you have and the actual thing that you're trying to predict.

Are there problematic gaps? For example, if you're using data label X as a proxy to predict target Y, in which case is is the gap between X and Y problematic.


![image](https://user-images.githubusercontent.com/1645304/134752380-a98c087f-29d1-4841-ab2b-43041cdea78f.png)

## Reading: Responsible AI
New technologies always bring new challenges. Ensuring that your applications adhere to responsible AI is a must. Please read this [resource](https://ai.google/responsibilities/responsible-ai-practices/) to keep yourself updated with this fascinating active research  subject.

## Legal Requirements for Secure and Private AI
A legal side to practicing responsible AI. They are already a legal requirements in some countries and regions, and this trend is growing. Exposure to civil liability is another concern.

### Legal Implications of Data Security and Privacy
Training data, prediction requests, or both, can contain very sensitive information about people. For prediction request, those people are your users.

Privacy of sensitive data should be protected:
- This includes not only respecting the legal and regulatory requirements,
- but also considering social norms and typical individual expectations.

What safeguards do you need to put in place to ensure the privacy of individuals considering that ML models may remember or reveal aspects of the data that they've been exposed to?

What steps are needed to ensure users have adequate transparency and control of their data?

It's not just up to you to decide what is required:
- In Europe, for example, you need to comply with the General Data Protection Regulations, or GDPR, and
- in California, you need to comply with the California Consumer Privacy Act, or CCPA.

![image](https://user-images.githubusercontent.com/1645304/134752516-4dd8dbea-ecd5-45af-9312-e1a3aa2c363f.png)

### General Data Protection Regulation (GDPR)
The General Data Protection Regulation, or GDPR, was enacted by the EU in 2016 and became a model for many national laws outside the EU, including Chile, Japan, Brazil, South Korea, Argentina, and Kenya. It regulates the data protection and privacy in the European Union and the European Economic Area.

The GDPR gives individuals control over their personal data and requires that companies should protect the data of employees and consumers.

When data processing is based on consent, the data subject, usually an individual person, has the right to revoke their consent at any time.

![image](https://user-images.githubusercontent.com/1645304/134752611-420a76f8-1fb3-49c4-a9a0-e752cf7dd4be.png)

### California Consumer Privacy Act (CCPA)
In California, Consumer Privacy Act, or CCPA, was modeled after the GDPR and has similar goals, including:
- enhancing the privacy rights and
- consumer protections for residents of California.

It states that users have the right to know:
- what personal data is being collected about them, including whether the personal data is sold or disclosed in some way,
- who supplied their data and
- who received their data.

Users can access the personal data which a company has for them, block the sale of their data, and request a business to delete their data.

![image](https://user-images.githubusercontent.com/1645304/134752676-604b54e7-ebc7-42f1-b24a-fe4bfd72bd4a.png)

### Securit and Privacy Harms from ML Models
Security and privacy are closely linked for some problems or harms and machine learning. Informational harms are caused when information is allowed to leak from the model. There are at least three different types of informational harms, including:
- **membership inference**, where an attacker can determine whether or not an individual's data was included in the training set.
- **Model inversion**, where the attackers actually able to recreate the training set, and
- **model extraction**, where an attacker is able to recreate the model itself.

Behavioral harms are caused when an attacker is able to change the behavior of the model itself. This includes:
- poisoning attacks, where the attacker is able to insert malicious data into the training set, and
- evasion attacks where the attacker makes small changes to prediction requests to cause the model to make bad predictions.

![image](https://user-images.githubusercontent.com/1645304/134753247-731ac3a9-a8b6-417d-b7a0-7c96ec15be52.png)

### Defenses
It's important to defend your model against attacks, as well as ensuring privacy and security of user data. Let's discuss a few approaches for defending against attacks.

![image](https://user-images.githubusercontent.com/1645304/134753331-72ae4ab4-328c-4807-9507-b79703e0ac69.png)

### Cryptography
Let's discuss a few approaches for defending against attacks.

You should consider privacy enhancing technologies such as Secure Multi-Party Computation, or SMPC, or Fully Homomorphic Encryption, or FHE, when training and serving your models. Briefly,
- SMPC enables multiple systems to collaborate securely to train and/or serve a model while keeping the actual data secure through the use of shared secrets. 
- FHE, on the other hand, enables developers to train their models on encrypted data without decrypting it first.

FHE in particular allows users to send an encrypted prediction requests and receive back an encrypted results. During the entire process, the data is never decrypted except by the user. However, you should be aware that currently, FHE is very computationally expensive. The goal here is that using cryptography, you can protect the confidentiality of your training data.

![image](https://user-images.githubusercontent.com/1645304/134754351-d31050e6-6752-4780-b426-6e1dc343fb57.png)

### Differential Privacy
Roughly, a model is differentially private if an attacker seeing its predictions cannot tell if a particular user's information was included in the training data. By implementing differential privacy, you can responsibly train models on private data. It provides provable guarantees of privacy, mitigating the risk of exposing sensitive training data.

Let's briefly discuss three different approaches to implementing differential privacy:
- Differentially-Private Stochastic Gradient Descent , or DP-SGD,
- Private Aggregation of Teacher Ensembles or PATE, and
- Confidential and Private Collaborative learning, or CaPC. 

![image](https://user-images.githubusercontent.com/1645304/134754503-d7aa4630-580e-4536-8cf3-bbdc662c5d06.png)

### Differentially-Private Stochastic Gradient Descent (DP-SGD)
If an attacker is able to get a copy of a normally trained model, then they can use the weights to extract private information. Differentially-Private Stochastic Gradient Descent, or DP-SGD, eliminates that possibility by applying differential privacy throughout training.

It does that by modifying the minibatch stochastic optimization process by adding noise. The result is a trained model which retains differential privacy because of the post-processing immunity property of differential privacy.

Post-processing immunity is a fundamental property of differential privacy. It means that regardless of how you process the models predictions, you can't affect their privacy guarantees. 

![image](https://user-images.githubusercontent.com/1645304/134754556-9df16171-5a1e-4926-81bd-a656aed41f4e.png)

### Private Aggregation of Teacher Ensembles (PATE)
Next, let's take a look at Private Aggregation of Teacher Ensembles, or PATE. PATE begins by dividing up sensitive data into k partitions with no overlaps.

It then trains k models on that data separately as teacher models, and then aggregates the results in an aggregate teacher model. This is the same teacher-student used for knowledge distillation.

During the aggregation for the aggregate teacher, you will add noise to the output in a way that won't affect the resulting predictions. All of these models and the sensitive data are not available to end users, including attackers. For deployment, you will create a student model. To train the student model, you'll take unlabeled public data and feed it to the aggregate teacher model. The output of this process is labeled data, which maintains privacy. You use this data as the training set for the student model.

After training, you will discard everything on the left side of this diagram and deploy only the student model for use. 


![image](https://user-images.githubusercontent.com/1645304/134754613-8a4580db-98e6-4603-84dc-543cfa7f9584.png)


### Confidential and Private Collaborative learning (CaPC)
Confidential and Private Collaborative learning, or CaPC, enables multiple developers using different data to collaborate to improve their model accuracy without sharing information. This preserves both privacy and confidentiality.

To do that, it applies techniques and principles from both cryptography and differential privacy. This includes using Homomorphic Encryption, or HE, to encrypt the prediction requests that each collaborating model receives so that information in the prediction request is not leaked. It then uses PATE to add noise to the predictions from each of the collaborating models and uses voting to arrive at a final prediction, again, without leaking information.

A great example of how CaPC can be used is to consider a group of hospitals who want to collaborate to improve each other's models and predictions. Because of healthcare privacy laws, they can't share information directly. But using CaPC, they can achieve better results while preserving the privacy and confidentiality of their patients.
 

![image](https://user-images.githubusercontent.com/1645304/134754701-51cf5a59-8712-45d5-8690-b72dbdeba216.png)

## GDPR and CCPA
Check the [GDPR](https://gdpr.eu/) and [CCPA](https://oag.ca.gov/privacy/ccpa) websites  out to learn more about its regulations and compliance. 

## Anonymization and Pseudonymisation

Anonymization and pseudonymisation are some of the most well established ways of protecting privacy. 

### Data Anonymization
he GDP are includes many regulations to preserve privacy of user data and includes the definitions of many of the terms that it uses. This includes two terms that I'll discuss now anonymization and pseudonymisation.

![image](https://user-images.githubusercontent.com/1645304/134754812-ee635c1a-ceaa-403b-9f8e-a7ae80eb7718.png)

### Data Anonymization
Anonymization removes personally identifiable information or P II from data sets so that people who the data describes remain anonymous for the GDPR. Recital 26 defines acceptable data anonymous station to be irreversible and done in such a way that is impossible to identify the person.

It's impossible to derive insights or discrete information even by the party responsible for anonymization. Once data has been acceptably anonymous sized, the GDP are no longer applies to that data.

![image](https://user-images.githubusercontent.com/1645304/134754844-4b6d63bb-5fa0-4a09-b7dc-2fe23ec13745.png)

### Pseudonymisation
Pseudonymisation is a bit different. This is a reversible process, meaning that it's still possible to identify the individual if the right additional information is included. Pseudonymisation can be implemented with data masking or encryption or tokenization. It relies on careful control of access to the additional identifying information

![image](https://user-images.githubusercontent.com/1645304/134754858-29a3c4f0-db26-4077-84e8-2c6434360293.png)


### Pseudonymisation v Anonymization
So to be clear the biggest difference between anonymization and pseudonymisation is that pseudonymized data can be reversed using an additional set of information or an encryption key while anonymization is irreversible.

![image](https://user-images.githubusercontent.com/1645304/134754889-d110828c-4892-452f-ad73-40db86bcaea4.png)

### Spectrum of Privacy Preservation

A lot of methods, mechanisms and tools have been developed over the years that produce data with various levels of both anonymity and the capability of being identified. It ranges from personally identifiable to truly anonymous data, personally identifiable data contains name, address, phone, email etc. While data which is purely anonymous and in accordance with GDP are guidelines does not include personally identifiable information or PI and cannot be connected to PII. Even with additional information, pseudonymized and de identified data form the intermediary category of the spectrum. They are indeed a way of preserving certain aspects of data privacy but not to the level of truly anonymous data. Note however, that the difference between de identified data and pseudonymized data is not well defined and many discussions will group them together as one thing.

![image](https://user-images.githubusercontent.com/1645304/134754938-4cda052b-cc68-4507-a32c-5ff1a6e0dd77.png)

### What Data Soulld be Anonymized
So what part of your data should you anonymized basically everything that is part of PI that includes any data that reveals the identity of a person which are known as identifiers and with the term identifiers. I mean any natural or legal person living or dead including their dependents, their ascendance and descendants. This also includes other related persons who might be identifiable through either direct or indirect relationships. For example, this includes features such as family names, patron names first names, maiden names, aliases, address phone, bank account details, credit cards, tax IEDs and so forth.

![image](https://user-images.githubusercontent.com/1645304/134754980-8e0d4077-348e-4ed0-9558-bcf6ea06223a.png)

## Right to be Forgotten

### What is the Right to be forgotten?
some clarification of terms. When the GDPR refers to a data subject, it means a person, and when it refers to a controller, it means a person or organization who has control over a dataset containing Personally Identifiable Information or PII. Now that we've got that out of the way, let's ask the question.

When does a person have the right to be forgotten? Well, there's a fairly long list of reasons for why an individual has the right to have their personal data erased. Rather than trying to remember these, I encourage you to refer to the GDPR. The list includes:
- the personal data is no longer necessary for the purpose an organization originally collected it or processed it, or
- an organization is relying on individual's consent as the lawful basis for processing the data and that individual withdraws their consent, or
- an organization is relying on legitimate interests as its justification for processing an individual's data and the individual objects to this processing, and there is no overriding legitimate interest for the organization to continue with the processing, or
- an organization is processing personal data for direct marketing purposes and the individual objects to this processing, or
- an organization processed an individual's personal data unlawfully, or
- an organization must erase personal data in order to comply with a legal ruling or obligation, or
- an organization has processed a child's personal data to offer them information society services, an information society might be a social network, for example. 

If any of those conditions are met, you must delete the person's data. In general, these are mostly common sense. However, in some cases, an organization's right to process someone's data might override their right to be forgotten. Here are the reasons cited in the GDPR that override the right to be forgotten:
- The data is being used to exercise the right of freedom of expression and information, or
- the data is being used to comply was a legal ruling or obligation, or
- the data is being used to perform a task that is being carried out in the public interest or when exercising an organization's official authority, or
- the data being processed is necessary for public health purposes and serves in the public interest, or
- the data being processed is necessary to perform a preventative or occupational medicine, this only applies when the data is being processed by a health professional who is subject to a legal obligation of professional secrecy, or
- the data represents important information that serves the public interest, scientific research, historical research, or statistical purposes where the eraser of the data would be likely to impair or halt progress towards the achievement that was the goal of the processing, or
- the data is being used for the establishment of a legal defense or in the exercise of other legal claims.

Furthermore, an organization can request a reasonable fee or deny a request to erase personal data if the organization can justify that the request was unfounded or excessive. 

But, in general, you should avoid overriding an individual's right to be forgotten unless you strongly meet one of these conditions. When in doubt err on the side of privacy.

![image](https://user-images.githubusercontent.com/1645304/134755137-54c5adf1-407a-4801-a44a-bd1407004d3c.png)

### Right to Reectification
You also have the right to have your personal information corrected or rectified. This might be important in situations like your credit history, health history, or employment history. 
![image](https://user-images.githubusercontent.com/1645304/134755222-679c1e16-4e9f-4bdd-ba8f-993aa103e156.png)

### Other Right of the Data Subject
The GDPR also defines a number of other rights which people or data subjects have. These include the right of access by the data subject, the right to restriction of processing, the right to data portability, and the right to object. As a general rule, it's best to err on the side of privacy and consider any personal information that you have in your data as sensitive. You should restrict access to it and keep it safe. Above all, you should think of it as the property of the person whose information it is and honor their wishes. 

![image](https://user-images.githubusercontent.com/1645304/134755260-1b5f0394-d18d-4823-bdeb-5488a756d5f1.png)


### Implementing Right To Be Forgotten: Tracking Data
When you receive a valid request to have personal information deleted, you need to identify all of the information related to the content requested to be removed, you also need to identify and remove all of the metadata associated with that person. If you've run any analysis or trained any models, the derived data and logs, and models must also be removed or corrected. The goal here is as much as possible to make it as if you never had their data.

![image](https://user-images.githubusercontent.com/1645304/134755298-143350bc-d812-425d-96cc-fe9d74a26852.png)


### Forgetting Digital Memories
There are basically two ways to delete data which will satisfy the requirements of the GDPR: First, you can anonymize the data, which as you saw previously, will make it non personally identifiable under the terms of the GDPR, and the GDPR will no longer apply to anonymize data. Second, you can do a hard delete of the data, meaning actually delete the data, including any rows in your database which might contain it. Normally, your first impulse might be to always just do a hard delete, but often there are issues with that. Anonymization is another option.
![image](https://user-images.githubusercontent.com/1645304/134755341-163b5017-ffd8-41df-abb8-a2e53280007e.png)

### Issues with Hard Delete
In a database or any other similar relational datastore, deleting records can cause havoc. Part of this is because user data is often referenced in multiple tables, so deleting those records breaks the connections, which can be difficult, especially in large complex databases. For example, it can break foreign keys. On the other hand, anonymization keeps the records and only anonymizes the fields containing PII while still satisfying the requirements of the GDPR.

![image](https://user-images.githubusercontent.com/1645304/134755371-cc3989ef-c773-4993-8194-6d97cfb0f66c.png)

### Challenges in Implementing Right to be forgotten
here are several challenges in implementing the right to be forgotten: The process of identifying whether or not data privacy has been violated is itself a challenging task, in order to enforce the GDPR, several organizational changes are needed, including policy changes and training employees in how to enforce the right to be forgotten, and one last consideration, that can be tricky; if your organization maintains multiple backups of your data, which actually you should, making sure that your personal data has been deleted from all of your backups is challenging. You might very well have to change your data storage and backup implementation to maintain compliance with a GDPR.

![image](https://user-images.githubusercontent.com/1645304/134755403-913e0180-d1f5-4a49-a151-ea783bd4d370.png)

## Reading: GDPR and Privacy

**1. Question 1**

How actual users experience your system is essential for assessing the true impact of its predictions, recommendations, and decisions. A straightforward technique to incorporate that feedback is using several metrics rather than a single one. What types of metrics can help you understand tradeoffs between errors and experiences? (Select all that apply)

- [ ] Overall system performance metrics
- [x] Short and long-term product health measures
- [x] False-positive and false-negative rates
- [ ] Automated quality characteristics measures
- [x] User surveys

**2. Question 2**

True or False: Informational harm occurs when the adversary is able to inject bad data into your model's training pool. This attack is known as poisoning.

- [x] False
- [ ] True

> Sorry, this is, in fact, false. We are describing behavioral harm. Informational harm, on the other hand, occurs when information leaks from the model. For example, an attacker predicting whether an individual's data was included in the training set, reconstructing the training set from model parameters, or duplicating an ML model through query access to a target model.

**3. Question 3**

How can a hospital network that wants to improve its models and predictions collaborate without sharing information directly between institutions and violating healthcare privacy laws?


- [ ] Using Trusted Execution Environments.
- [x] Using Confidential and Private Collaborative Learning.
- [ ] Using Differentially-Private Stochastic Gradient Descent.
- [ ] Using Cryptography.

**4. Question 4**

Does pseudonymized data cease to be “personal data” and stop requiring compliance with the GDPR?

- [x] No, because pseudonymization is a reversible process, meaning it’s still possible to identify the individual if the correct additional information or encryption key is included.
- [ ] Yes, because pseudonymization irreversibly prevents identifying the individual to whom the data relates using masking, encryption, or tokenization.
