# Model Decay

## What is Model Decay?
why this might happen and how to prevent it

### Model Decay
Production ML models often operate in dynamic environments. Over time, dynamic environments change. That's what makes them Dynamic. Think of a recommender system, for example, that is trying to recommend which music to listen to. Music changes constantly, with new music becoming popular and taste changing. If the model is static and continues to recommend music that has gone out of style, then the quality of the recommendations will decline. The model is moving away from the current ground truth. It doesn't understand the current styles because it hasn't been trained for them. So there are two main causes of model drift. Data drift and concept drift.

![image](https://user-images.githubusercontent.com/1645304/134739651-76c27b24-d854-40ca-920d-b35c90e2c5a7.png)

### Data Drift (aka Feature Drift)
Data drift occurs when statistical properties of the input, the features, changes. As the input changes, the prediction requests, the input moves farther away from the data that the model was trained with, and model accuracy suffers. Changes like these often occur in demographic features like age, which may change over time. The graph on the right shows how there is an increase in mean and variance for the age.

![image](https://user-images.githubusercontent.com/1645304/134739764-65674cbd-59a4-408b-bfe3-9355ce64bf3c.png)

### Concept Drift
Concept drift occurs when the relationship between the features and the labels changes. When a model is trained, it learns a relationship between the inputs and ground truth or labels. If the relationship between the inputs and the labels changes over time, it means that the very meaning of what you are trying to predict changes. The world has changed, but our model doesn't know it. For example, take a look at the graph on the right side. You can see that the distribution of the features for the two classes, the blue and red dots, changes over time intervals, T1, T2, and T3. If your model is still predicting for T1 when the world has moved to T3, many of its predictions will be incorrect. I should also mention here that there are related forms of drift known as prediction drift. Where drifts solely in your model's predictions and labeled drift

![image](https://user-images.githubusercontent.com/1645304/134739943-a9e59ea3-8c59-4eaf-b0f1-e618f0669112.png)


### Detecting Drift over time
 If you don't plan ahead for drift, it can slowly creep into your system over time.
 
 How fast your system drifts depends on the nature of the domain that you're working in:
 - Some domains like markets can change within hours or even minutes.
 - Others change more slowly.
 
 If drift, either data drift or concept drift or both, is not detected, then your model accuracy will suffer and you won't be aware of it. This can lead to emergency retraining of your model, which is something to avoid. So monitoring and planning ahead are important.
 
![image](https://user-images.githubusercontent.com/1645304/134740105-a3e11f7b-34e8-423c-b7b2-4d24a2fadc3b.png)

## Model Decay Detection

### Detecting Concept and Data Drift
Detecting drift, whether it's data drift or concept drift or both starts with collecting current data.
- You should collect all of the data in the incoming prediction request to your model, along with the predictions that your model makes.
- If it's possible in your application, also collect the correct label or ground truth that your model should have predicted.

This is also extremely valuable for retraining your model, but at a minimum, you should capture the prediction request data, which you can use to detect data drift using unsupervised statistical methods.

![image](https://user-images.githubusercontent.com/1645304/134741253-77ecaabb-c673-4c11-a705-7d6f068ad3df.png)

### Detecting Drift
Once you're set up to continuously monitor and log your data, you employ tools which use well-known statistical methods to compare your current data with your previous training data. You also use dashboards to monitor for trends and seasonality over time. Essentially, you'll be working with time series data since you have an ordered data that is associated with a time component.

You don't have to reinvent the wheel here, there are good tools and libraries available to help you do this kind of analysis:
- TensorFlow Data Validation or TFDV,
- and the Scikit-multiflow library. 

![image](https://user-images.githubusercontent.com/1645304/134741925-592ba2f2-d6d1-46a6-978d-8666ec6edacd.png)

### Continuous Evaluation and Labelling in Vertex Prediction
Cloud providers, including Google, offer managed services such as Google's Vertex Prediction, that help you perform continuous evaluation of your prediction requests.

Continuous evaluation regularly sample's prediction input and output from trained machine learning models that you've deployed to Vertex prediction.

Vertex data labeling service then assigns human reviewers to provide ground truth labels to your prediction input, or alternatively, you can provide your own ground truth labels. The data labeling service compares your model's predictions with the ground truth labels to provide continual feedback on how well your model is performing over time. Azure, AWS, and other cloud providers offer similar services.

![image](https://user-images.githubusercontent.com/1645304/134742051-e75113fa-b085-4666-9ff5-5009b4ebd8d2.png)

## Ways to Mitigate Model Decay

### Mitigating Model Decay
When you detect model decay you need to let others know about it. That means informing your operational and business stakeholders about the situation, along with some idea about how severe you think the drift has become. Then you work on bringing the model back to acceptable performance

![image](https://user-images.githubusercontent.com/1645304/134742324-3124145e-820f-4982-bf97-4fd0fa699d99.png)


### Steps in Mitigating Model Decay

Now that you've detected drift, what can you do about it? Well, first, try to determine which data in your previous training data set is still valid by using unsupervised methods, such as clustering or statistical methods that look at divergence. Many options exist, including:
- Kullback-Leibler or
- KL divergence,
- Jensen-Shannon or
- JS divergence or
- the Kolmogorov-Smirnov or
- K-S test.

This step is optional, but especially when you don't have a lot of new data, it can be important to try to keep as much of your old data as possible.

Another option is to simply discard that part of your training data set that was collected before a certain date and add your new data.

Or if you have enough newly labeled data then you can just create an entirely new data set.

The choice between these options will probably be dictated by the realities of your application and your ability to collect new labeled data. 

![image](https://user-images.githubusercontent.com/1645304/134742480-c9678f8d-e322-4fe1-b726-1a3da3c55b22.png)

### Fine Tune or Start Over
Now that you have a new training data set, you've basically two choices for how to train your model, fine tuning or starting over. You can either:
- continue training your model, fine tuning it from the last checkpoint using your new data, or
- start over by re-initializing your model and completely retraining it.

Either approach is valid and the choice between these two options will largely be dictated by:
- the amount of new data that you have and
- how far the world has drifted since the last time you trained your model.

Ideally, if you have enough new data, you should try both approaches and compare the results.

![image](https://user-images.githubusercontent.com/1645304/134742691-554ed96f-c265-45bb-b4e7-e94e5b442a39.png)

### Model Re-Training Policy
It's usually a good idea to establish policies around when you're going to retrain your model.

There's really no right or wrong answer here, so it will depend on what works in your particular situation. You could simply choose to retrain your model whenever it seems to be necessary.
- That includes situations where you detect a drift,
- but it also includes situations where you may need to add or remove class labels or features, for example.

You could also always retrain your model according to a schedule, whether it needs it or not. In practice, this is what many people do because it's simple to understand and in many domains it works fairly well. It can, however, incur higher training and data gathering costs unnecessary, or alternatively, it can allow for greater model decay that might be ideal depending on whether your schedule has your model training too often or not often enough.

Finally, you might be limited by the availability of new training data. This is especially true in circumstances where labeling is slow and expensive. As a result, you may be forced to try to retain as much of your old training data as possible for as long as possible, and avoid fully retraining your model.

![image](https://user-images.githubusercontent.com/1645304/134742981-2e5689a3-9823-48df-bbbb-562b3462a867.png)

### Automating Model Retraining
If you can automate the process of detecting the conditions which require model retraining, that's ideal.
- That includes being able to detect model performance degradation and triggering retraining,
- or when you detect significant data drift.

In both cases, in order to automate retraining, you should have data gathered and labeled automatically using a separate process and only retrain when sufficient data is available.

Ideally, you also have continuous training, integration and deployment setup as well, to make the process fully automated.

For some domains where change is fast and frequent retraining is required, these automated processes become requirements instead of luxuries.

![image](https://user-images.githubusercontent.com/1645304/134743186-5203214b-0476-4fea-9dfc-14a36599e4a6.png)

### Redesign Data Processing Steps and Model Architecture
When your model decay is beyond an acceptable threshold, or when the meaning of the variable you are trying to predict deviates significantly, or you need to make changes like adding or removing features or class labels, you might have to redesign your data pre-processing steps and model architecture.

I like to think of this as an opportunity to make improvements.

You may have to rethink your feature engineering, feature selection, and so forth, in order to make your model work with current data, and retrain your model from scratch rather than applying fine tuning. You might have to investigate other potential model architectures, which personally I find is a lot of fun.

The point here is that no model lives forever, and periodically, you need to go back to the drawing board and start over, applying what you've learned since the last time you updated your model.

![image](https://user-images.githubusercontent.com/1645304/134744088-0cb358c8-ad86-439f-b7c1-2aeb2bdd626f.png)

## Reading: Addressing Model Decay
Check this [blog](https://neptune.ai/blog/retraining-model-during-deployment-continuous-training-continuous-testing) out to figure out best retraining strategies toi prevent model decay.

## Quiz: Model Decay
**1. Question 1**

Suppose that you have a very accurate model for a social app that uses several features to predict whether a user is a spammer or not. You trained the model with a particular idea of what a spammer was, for example, a user who sends ten messages in one minute. Over time, the app grew and became more popular, but the outcome of the predictions has drastically changed. As people are chatting and messaging more, now sending ten messages in a minute becomes normal and not something that only spammers do. What kind of drift causes this spam-detection model's predictive ability to decay?


- [x] Concept Drift
- [ ] Data Drift

> Nailed it! The model's original idea about what it means to be a spammer has changed, and now sending ten messages in a minute becomes normal. In other words, the concept of spammers has drifted. Consequently, since you haven't updated your model, it will predict these non-spammers as spammers (a false positive).

**2. Question 2**

When mitigating data drift, if you don't have enough recently labeled data to create an entirely new dataset, how can you determine what data from your previous training dataset is still valid? (Select all that apply)


- [ ] Using a human Data-Labeling service.
- [x] Using an unsupervised learning technique like clustering.
- [x] Using a statistical measure like the Kullback–Leibler divergence.


> Correct: That's right! Clustering identifies relevant portions of the original dataset to retrain a model's functionality that has become dispersed. It is a form of restructuring, and hence it could be a way of direct preventative maintenance.


**3. Question 3**

True or False: Having restructured a new training dataset after detecting model decay, you must start over by resetting and completely retraining your model. There is no way around it.

- [x] False
- [ ] True

**4. Question 4**

If you can't automate the process of finding out when model retraining is required, what are some good ideas for policies to establish when to do it? (Select all that apply)


- [x] Retrain on schedule.
- [ ] Retrain on performance-degradation detection.
- [x] Retrain on demand.
- [x] Retrain on new data availability.

