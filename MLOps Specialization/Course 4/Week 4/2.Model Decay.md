# Model Decay

## What is Model Decay?
why this might happen and how to prevent it

### Model Decay
Production ML models often operate in dynamic environments. Over time, dynamic environments change. That's what makes them Dynamic. Think of a recommender system, for example, that is trying to recommend which music to listen to. Music changes constantly, with new music becoming popular and taste changing. If the model is static and continues to recommend music that has gone out of style, then the quality of the recommendations will decline. The model is moving away from the current ground truth. It doesn't understand the current styles because it hasn't been trained for them. So there are two main causes of model drift. Data drift and concept drift.

![image](https://user-images.githubusercontent.com/1645304/134739651-76c27b24-d854-40ca-920d-b35c90e2c5a7.png)

### Data Drift (aka Feature Drift)
Data drift occurs when statistical properties of the input, the features, changes. As the input changes, the prediction requests, the input moves farther away from the data that the model was trained with, and model accuracy suffers. Changes like these often occur in demographic features like age, which may change over time. The graph on the right shows how there is an increase in mean and variance for the age.

![image](https://user-images.githubusercontent.com/1645304/134739764-65674cbd-59a4-408b-bfe3-9355ce64bf3c.png)

### Concept Drift
Concept drift occurs when the relationship between the features and the labels changes. When a model is trained, it learns a relationship between the inputs and ground truth or labels. If the relationship between the inputs and the labels changes over time, it means that the very meaning of what you are trying to predict changes. The world has changed, but our model doesn't know it. For example, take a look at the graph on the right side. You can see that the distribution of the features for the two classes, the blue and red dots, changes over time intervals, T1, T2, and T3. If your model is still predicting for T1 when the world has moved to T3, many of its predictions will be incorrect. I should also mention here that there are related forms of drift known as prediction drift. Where drifts solely in your model's predictions and labeled drift

![image](https://user-images.githubusercontent.com/1645304/134739943-a9e59ea3-8c59-4eaf-b0f1-e618f0669112.png)


### Detecting Drift over time
 If you don't plan ahead for drift, it can slowly creep into your system over time.
 
 How fast your system drifts depends on the nature of the domain that you're working in:
 - Some domains like markets can change within hours or even minutes.
 - Others change more slowly.
 
 If drift, either data drift or concept drift or both, is not detected, then your model accuracy will suffer and you won't be aware of it. This can lead to emergency retraining of your model, which is something to avoid. So monitoring and planning ahead are important.
 
![image](https://user-images.githubusercontent.com/1645304/134740105-a3e11f7b-34e8-423c-b7b2-4d24a2fadc3b.png)

## Model Decay Detection

### Detecting Concept and Data Drift
Detecting drift, whether it's data drift or concept drift or both starts with collecting current data.
- You should collect all of the data in the incoming prediction request to your model, along with the predictions that your model makes.
- If it's possible in your application, also collect the correct label or ground truth that your model should have predicted.

This is also extremely valuable for retraining your model, but at a minimum, you should capture the prediction request data, which you can use to detect data drift using unsupervised statistical methods.

![image](https://user-images.githubusercontent.com/1645304/134741253-77ecaabb-c673-4c11-a705-7d6f068ad3df.png)

### Detecting Drift
Once you're set up to continuously monitor and log your data, you employ tools which use well-known statistical methods to compare your current data with your previous training data. You also use dashboards to monitor for trends and seasonality over time. Essentially, you'll be working with time series data since you have an ordered data that is associated with a time component.

You don't have to reinvent the wheel here, there are good tools and libraries available to help you do this kind of analysis:
- TensorFlow Data Validation or TFDV,
- and the Scikit-multiflow library. 

![image](https://user-images.githubusercontent.com/1645304/134741925-592ba2f2-d6d1-46a6-978d-8666ec6edacd.png)

### Continuous Evaluation and Labelling in Vertex Prediction
Cloud providers, including Google, offer managed services such as Google's Vertex Prediction, that help you perform continuous evaluation of your prediction requests.

Continuous evaluation regularly sample's prediction input and output from trained machine learning models that you've deployed to Vertex prediction.

Vertex data labeling service then assigns human reviewers to provide ground truth labels to your prediction input, or alternatively, you can provide your own ground truth labels. The data labeling service compares your model's predictions with the ground truth labels to provide continual feedback on how well your model is performing over time. Azure, AWS, and other cloud providers offer similar services.

![image](https://user-images.githubusercontent.com/1645304/134742051-e75113fa-b085-4666-9ff5-5009b4ebd8d2.png)

