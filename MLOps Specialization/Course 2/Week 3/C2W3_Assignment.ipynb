{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppvHP8YzmJRu"
   },
   "source": [
    "# Week 3 Assignment:  Data Pipeline Components for Production ML\n",
    "\n",
    "In this last graded programming exercise of the course, you will put together all the lessons we've covered so far to handle the first three steps of a production machine learning project - Data ingestion, Data Validation, and Data Transformation.\n",
    "\n",
    "Specifically, you will build the production data pipeline by:\n",
    "\n",
    "*   Performing feature selection\n",
    "*   Ingesting the dataset\n",
    "*   Generating the statistics of the dataset\n",
    "*   Creating a schema as per the domain knowledge\n",
    "*   Creating schema environments\n",
    "*   Visualizing the dataset anomalies\n",
    "*   Preprocessing, transforming and engineering your features\n",
    "*   Tracking the provenance of your data pipeline using ML Metadata\n",
    "\n",
    "Most of these will look familiar already so try your best to do the exercises by recall or browsing the documentation. If you get stuck however, you can review the lessons in class and the ungraded labs. \n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Imports](#1)\n",
    "- [2 - Load the Dataset](#2)\n",
    "- [3 - Feature Selection](#4)\n",
    "  - [Exercise 1 - Feature Selection](#ex-1)\n",
    "- [4 - Data Pipeline](#4)\n",
    "  - [4.1 - Setup the Interactive Context](#4-1)\n",
    "  - [4.2 - Generating Examples](#4-2)\n",
    "    - [Exercise 2 - ExampleGen](#ex-2)\n",
    "  - [4.3 - Computing Statistics](#4-3)\n",
    "    - [Exercise 3 - StatisticsGen](#ex-3)\n",
    "  - [4.4 - Inferring the Schema](#4-4)\n",
    "    - [Exercise 4 - SchemaGen](#ex-4)\n",
    "  - [4.5 - Curating the Schema](#4-5)\n",
    "    - [Exercise 5 - Curating the Schema](#ex-5)\n",
    "  - [4.6 - Schema Environments](#4-6)\n",
    "    - [Exercise 6 - Define the serving environment](#ex-6)\n",
    "  - [4.7 - Generate new statistics using the updated schema](#4-7)\n",
    "      - [Exercise 7 - ImporterNode](#ex-7)\n",
    "      - [Exercise 8 - StatisticsGen with the new schema](#ex-8)\n",
    "  - [4.8 - Check anomalies](#4-8)\n",
    "      - [Exercise 9 - ExampleValidator](#ex-9)\n",
    "  - [4.9 - Feature Engineering](#4-9)\n",
    "      - [Exercise 10 - preprocessing function](#ex-10)\n",
    "      - [Exercise 11 - Transform](#ex-11)\n",
    "- [5 - ML Metadata](#5)\n",
    "  - [5.1 - Accessing stored artifacts](#5-1)\n",
    "  - [5.2 - Tracking artifacts](#5-2)\n",
    "    - [Exercise 12 - Get parent artifacts](#ex-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZXjh6D9nOUX"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TK6SyLhQP_s5",
    "outputId": "0559a068-e05f-4ff1-9fd4-1985ab50fff8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tfx\n",
    "\n",
    "# TFX components\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Transform\n",
    "from tfx.components import ImporterNode\n",
    "\n",
    "# TFX libraries\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "# For performing feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# For feature visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from  tfx.proto import example_gen_pb2\n",
    "from tfx.types import standard_artifacts\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "# To ignore warnings from TF\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# For formatting print statements\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "# Display versions of TF and TFX related packages\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "print('TensorFlow Data Validation version: {}'.format(tfdv.__version__))\n",
    "print('TensorFlow Transform version: {}'.format(tft.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsHcencfobKL"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Load the dataset\n",
    "\n",
    "You are going to use a variant of the [Cover Type](https://archive.ics.uci.edu/ml/datasets/covertype) dataset. This can be used to train a model that predicts the forest cover type based on cartographic variables. You can read more about the *original* dataset [here](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info) and we've outlined the data columns below:\n",
    "\n",
    "| Column Name | Variable Type | Units / Range | Description |\n",
    "| --------- | ------------ | ----- | ------------------- |\n",
    "| Elevation | quantitative |meters | Elevation in meters |\n",
    "| Aspect | quantitative | azimuth | Aspect in degrees azimuth |\n",
    "| Slope | quantitative | degrees | Slope in degrees |\n",
    "| Horizontal_Distance_To_Hydrology | quantitative | meters | Horz Dist to nearest surface water features |\n",
    "| Vertical_Distance_To_Hydrology | quantitative | meters | Vert Dist to nearest surface water features |\n",
    "| Horizontal_Distance_To_Roadways | quantitative | meters | Horz Dist to nearest roadway |\n",
    "| Hillshade_9am | quantitative | 0 to 255 index | Hillshade index at 9am, summer solstice |\n",
    "| Hillshade_Noon | quantitative | 0 to 255 index | Hillshade index at noon, summer soltice |\n",
    "| Hillshade_3pm | quantitative | 0 to 255 index | Hillshade index at 3pm, summer solstice |\n",
    "| Horizontal_Distance_To_Fire_Points | quantitative | meters | Horz Dist to nearest wildfire ignition points |\n",
    "| Wilderness_Area (4 binary columns) | qualitative | 0 (absence) or 1 (presence) | Wilderness area designation |\n",
    "| Soil_Type (40 binary columns) | qualitative | 0 (absence) or 1 (presence) | Soil Type designation |\n",
    "| Cover_Type (7 types) | integer | 1 to 7 | Forest Cover Type designation |\n",
    "\n",
    "As you may notice, the qualitative data has already been one-hot encoded (e.g. `Soil_Type` has 40 binary columns where a `1` indicates presence of a feature). For learning, we will use a modified version of this dataset that shows a more raw format. This will let you practice your skills in handling different data types. You can see the code for preparing the dataset [here](https://github.com/GoogleCloudPlatform/mlops-on-gcp/blob/master/datasets/covertype/wrangle/prepare.ipynb) if you want but it is **not required for this assignment**. The main changes include:\n",
    "\n",
    "* Converting `Wilderness_Area` and `Soil_Type` to strings.\n",
    "* Converting the `Cover_Type` range to [0, 6]\n",
    "\n",
    "Run the next cells to load the **modified** dataset to your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Just in case you want to restart the lab workspace *from scratch*, you\n",
    "# # can uncomment and run this block to delete previously created files and\n",
    "# # directories. \n",
    "\n",
    "# !rm -rf pipeline\n",
    "# !rm -rf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBJqC6-HfymZ"
   },
   "outputs": [],
   "source": [
    "# Declare paths to the data\n",
    "DATA_DIR = './data'\n",
    "TRAINING_DIR = f'{DATA_DIR}/training'\n",
    "TRAINING_DATA = f'{TRAINING_DIR}/dataset.csv'\n",
    "\n",
    "# Create the directory\n",
    "!mkdir -p {TRAINING_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-o0Pk8nf7FZ",
    "outputId": "591a7532-3c8a-4111-966d-476f53396497"
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "!wget -nc https://storage.googleapis.com/workshop-datasets/covertype/full/dataset.csv -P {TRAINING_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0gqgbTF2Cgl"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Feature Selection\n",
    "\n",
    "For your first task, you will reduce the number of features to feed to the model. As mentioned in Week 2, this will help reduce the complexity of your model and save resources while training. Let's assume that you already have a baseline model that is trained on all features and you want to see if reducing the number of features will generate a better model. You will want to select a subset that has great predictive value to the label (in this case the `Cover_Type`). Let's do that in the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "I6MverMGJwg8",
    "outputId": "1d4b24f9-a17b-417b-f596-4e1723109326"
   },
   "outputs": [],
   "source": [
    "# Load the dataset to a dataframe\n",
    "df = pd.read_csv(TRAINING_DATA)\n",
    "\n",
    "# Preview the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data type of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data types of each column and the dataset description at the start of this notebook, you can see that most of the features are numeric and only two are not. This needs to be taken into account when selecting the subset of features because numeric and categorical features are scored differently. Let's create a temporary dataframe that only contains the numeric features so we can use it in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvmtVXtJeUOx"
   },
   "outputs": [],
   "source": [
    "# Copy original dataset\n",
    "df_num = df.copy()\n",
    "\n",
    "# Categorical columns\n",
    "cat_columns = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "# Label column\n",
    "label_column = ['Cover_Type']\n",
    "\n",
    "# Drop the categorical and label columns\n",
    "df_num.drop(cat_columns, axis=1, inplace=True)\n",
    "df_num.drop(label_column, axis=1, inplace=True)\n",
    "\n",
    "# Preview the resuls\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use scikit-learn's built-in modules to perform [univariate feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) on our dataset's numeric attributes. First, you need to prepare the input and target features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ShO1olUeiUb"
   },
   "outputs": [],
   "source": [
    "# Set the target values\n",
    "y = df[label_column].values\n",
    "\n",
    "# Set the input values\n",
    "X = df_num.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, you will use [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) to score each input feature against the target variable. Be mindful of the scoring function to pass in and make sure it is appropriate for the input (numeric) and target (categorical) values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QlfkDyfGKDv"
   },
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1: Feature Selection\n",
    "\n",
    "Complete the code below to select the top 8 features of the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQgcpJqSenLq",
    "outputId": "30bd52c9-452f-4c99-d107-d7c708c4f00b"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# Create SelectKBest object using f_classif (ANOVA statistics) for 8 classes\n",
    "select_k_best = None\n",
    "\n",
    "# Fit and transform the input data using select_k_best\n",
    "X_new = None\n",
    "\n",
    "# Extract the features which are selected using get_support API\n",
    "features_mask = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Print the results\n",
    "reqd_cols = pd.DataFrame({'Columns': df_num.columns, 'Retain': features_mask})\n",
    "print(reqd_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "                              Columns  Retain\n",
    "0                           Elevation    True\n",
    "1                              Aspect   False\n",
    "2                               Slope    True\n",
    "3    Horizontal_Distance_To_Hydrology    True\n",
    "4      Vertical_Distance_To_Hydrology    True\n",
    "5     Horizontal_Distance_To_Roadways    True\n",
    "6                       Hillshade_9am    True\n",
    "7                      Hillshade_Noon    True\n",
    "8                       Hillshade_3pm   False\n",
    "9  Horizontal_Distance_To_Fire_Points    True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got the expected results, you can now select this subset of features from the original dataframe and save it to a new directory in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A2jsRvhd-dR"
   },
   "outputs": [],
   "source": [
    "# Set the paths to the reduced dataset\n",
    "TRAINING_DIR_FSELECT = f'{TRAINING_DIR}/fselect'\n",
    "TRAINING_DATA_FSELECT = f'{TRAINING_DIR_FSELECT}/dataset.csv'\n",
    "\n",
    "# Create the directory\n",
    "!mkdir -p {TRAINING_DIR_FSELECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "P3f_iNgne7Tk",
    "outputId": "e25283a4-1aac-4294-a085-8371f9245684"
   },
   "outputs": [],
   "source": [
    "# Get the feature names from SelectKBest\n",
    "feature_names = list(df_num.columns[features_mask])\n",
    "\n",
    "# Append the categorical and label columns\n",
    "feature_names = feature_names + cat_columns + label_column\n",
    "\n",
    "# Select the selected subset of columns\n",
    "df_select = df[feature_names]\n",
    "\n",
    "# Write CSV to the created directory\n",
    "df_select.to_csv(TRAINING_DATA_FSELECT, index=False)\n",
    "\n",
    "# Preview the results\n",
    "df_select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HeAEQLAhg8I"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Data Pipeline\n",
    "\n",
    "With the selected subset of features prepared, you can now start building the data pipeline. This involves ingesting, validating, and transforming your data. You will be using the TFX components you've already encountered in the ungraded labs and you can look them up here in the [official documentation](https://www.tensorflow.org/tfx/api_docs/python/tfx/components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZES9v8ggpDv8"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Setup the Interactive Context\n",
    "\n",
    "As usual, you will first setup the Interactive Context so you can manually execute the pipeline components from the notebook. You will save the sqlite database in a pre-defined directory in your workspace. Please do not modify this path because you will need this in a later exercise involving ML Metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G-G-xLO3lkt",
    "outputId": "d956b6ef-fc1f-4beb-f3c4-0628a3690d90"
   },
   "outputs": [],
   "source": [
    "# Location of the pipeline metadata store\n",
    "PIPELINE_DIR = './pipeline'\n",
    "\n",
    "# Declare the InteractiveContext and use a local sqlite file as the metadata store.\n",
    "context = InteractiveContext(pipeline_root=PIPELINE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzm9d4G0hynL"
   },
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Generating Examples\n",
    "\n",
    "The first step in the pipeline is to ingest the data. Using [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen), you can convert raw data to TFRecords for faster computation in the later stages of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_jT4_QvOJOb"
   },
   "source": [
    "<a name='ex-2'></a>\n",
    "#### Exercise 2: ExampleGen\n",
    "\n",
    "Use `ExampleGen` to ingest the dataset we loaded earlier. Some things to note:\n",
    "\n",
    "* The input is in CSV format so you will need to use the appropriate type of `ExampleGen` to handle it. \n",
    "* This function accepts a *directory* path to the training data and not the CSV file path itself. \n",
    "\n",
    "This will take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mq8P5Fx3hSOB"
   },
   "outputs": [],
   "source": [
    "# # NOTE: Uncomment and run this if you get an error saying there are different \n",
    "# # headers in the dataset. This is usually because of the notebook checkpoints saved in \n",
    "# # that folder.\n",
    "# !rm -rf {TRAINING_DIR}/.ipynb_checkpoints\n",
    "# !rm -rf {TRAINING_DIR_FSELECT}/.ipynb_checkpoints\n",
    "# !rm -rf {SERVING_DIR}/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "EL3CZQcg3lku",
    "outputId": "6a51a564-647b-4bca-f21f-37736e01ebef"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE\n",
    "\n",
    "# Instantiate ExampleGen with the input CSV dataset\n",
    "example_gen = None\n",
    "\n",
    "# Run the component using the InteractiveContext instance\n",
    "None\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd3k6iIni-FE"
   },
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Computing Statistics\n",
    "\n",
    "Next, you will compute the statistics of your data. This will allow you to observe and analyze characteristics of your data through visualizations provided by the integrated [FACETS](https://pair-code.github.io/facets/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz5G_wYnVsHP"
   },
   "source": [
    "<a name='ex-3'></a>\n",
    "#### Exercise 3: StatisticsGen\n",
    "\n",
    "Use [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) to compute the statistics of the output examples of `ExampleGen`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "5u2EOMOm3lkw",
    "outputId": "c9ccf090-db40-4f7b-932f-3ddaa61282f8"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE\n",
    "\n",
    "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
    "statistics_gen = None\n",
    "    \n",
    "\n",
    "# Run the component\n",
    "None\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "exyb-VKD3lkw",
    "outputId": "5199d8aa-ca12-49ea-886b-2b8168de8614"
   },
   "outputs": [],
   "source": [
    "# Display the results\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZD97lK9UGct"
   },
   "source": [
    "Once you've loaded the display, you may notice that the `zeros` column for `Cover_type` is highlighted in red. The visualization is letting us know that this might be a potential issue. In our case though, we know that the `Cover_Type` has a range of [0, 6] so having zeros in this column is something we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6K2Wd9-tfdx"
   },
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Inferring the Schema\n",
    "\n",
    "You will need to create a schema to validate incoming datasets during training and serving. Fortunately, TFX allows you to infer a first draft of this schema with the [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6V9o7XnXKkV"
   },
   "source": [
    "<a name='ex-4'></a>\n",
    "#### Exercise 4: SchemaGen\n",
    "\n",
    "Use `SchemaGen` to infer a schema based on the computed statistics of `StatisticsGen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "_D9GZT1v3lkx",
    "outputId": "952c0c81-fbe3-45ba-b388-104c11328d06"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE\n",
    "# Instantiate SchemaGen with the output statistics from the StatisticsGen\n",
    "schema_gen = None\n",
    "    \n",
    "    \n",
    "\n",
    "# Run the component\n",
    "None\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "lW6Vhxy-3lkx",
    "outputId": "3b2bdbc4-79a0-4514-aa8f-e2bfad1d3468"
   },
   "outputs": [],
   "source": [
    "# Visualize the output\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpeVcYBbvret"
   },
   "source": [
    "<a name='4-5'></a>\n",
    "### 4.5 - Curating the schema\n",
    "\n",
    "You can see that the inferred schema is able to capture the data types correctly and also able to show the expected values for the qualitative (i.e. string) data. You can still fine-tune this however. For instance, we have features where we expect a certain range:\n",
    "\n",
    "* `Hillshade_9am`: 0 to 255\n",
    "* `Hillshade_Noon`: 0 to 255\n",
    "* `Slope`: 0 to 90\n",
    "* `Cover_Type`:  0 to 6\n",
    "\n",
    "You want to update your schema to take note of these so the pipeline can detect if invalid values are being fed to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pUvj-IgYQIL"
   },
   "source": [
    "<a name='ex-5'></a>\n",
    "#### Exercise 5: Curating the Schema\n",
    "\n",
    "Use [TFDV](https://www.tensorflow.org/tfx/data_validation/get_started) to update the inferred schema to restrict a range of values to the features mentioned above.\n",
    "\n",
    "Things to note:\n",
    "* You can use [tfdv.set_domain()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/set_domain) to define acceptable values for a particular feature.\n",
    "* These should still be INT types after making your changes.\n",
    "* Declare `Cover_Type` as a *categorical* variable. Unlike the other four features, the integers 0 to 6 here correspond to a designated label and not a quantitative measure. You can look at the available flags for `set_domain()` in the official doc to know how to set this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the schema uri\n",
    "    schema_uri = schema_gen.outputs['schema']._artifacts[0].uri\n",
    "    \n",
    "# for grading since context.run() does not work outside the notebook\n",
    "except IndexError:\n",
    "    print(\"context.run() was no-op\")\n",
    "    schema_path = './pipeline/SchemaGen/schema'\n",
    "    dir_id = os.listdir(schema_path)[0]\n",
    "    schema_uri = f'{schema_path}/{dir_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27woxdsF3lky"
   },
   "outputs": [],
   "source": [
    "# Get the schema pbtxt file from the SchemaGen output\n",
    "schema = tfdv.load_schema_text(os.path.join(schema_uri, 'schema.pbtxt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "40wynuTzstP3",
    "outputId": "af44ffe7-2113-4bd4-a078-868798127a75"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# Set the two `Hillshade` features to have a range of 0 to 255\n",
    "tfdv.set_domain(None, None, schema_pb2.IntDomain(name='Hillshade_9am', min=None, max=None))\n",
    "tfdv.set_domain(None, None, schema_pb2.IntDomain(name='Hillshade_Noon', min=None, max=None))\n",
    "\n",
    "# Set the `Slope` feature to have a range of 0 to 90\n",
    "tfdv.set_domain(None, None, schema_pb2.IntDomain(name='Slope', min=None, max=None))\n",
    "\n",
    "# Set `Cover_Type` to categorical having minimum value of 0 and maximum value of 6\n",
    "tfdv.set_domain(None, None, schema_pb2.IntDomain(name='Cover_Type', min=None, max=None, is_categorical=None))\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkkrTVG3jrKl"
   },
   "source": [
    "You should now see the ranges you declared in the `Domain` column of the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHN_j-d5mCCZ"
   },
   "source": [
    "<a name='4-6'></a>\n",
    "### 4.6 - Schema Environments\n",
    "\n",
    "In supervised learning, we train the model to make predictions by feeding a set of features with its corresponding label. Thus, our training dataset will have both the input features and label, and the schema is configured to detect these. \n",
    "\n",
    "However, after training and you serve the model for inference, the incoming data will no longer have the label. This will present problems when validating the data using the current version of the schema. Let's demonstrate that in the following cells. You will simulate a serving dataset by getting subset of the training set and dropping the label column (i.e. `Cover_Type`). Afterwards, you will validate this serving dataset using the schema you curated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxyJHhoSl4-3"
   },
   "outputs": [],
   "source": [
    "# Declare paths to the serving data\n",
    "SERVING_DIR = f'{DATA_DIR}/serving'\n",
    "SERVING_DATA = f'{SERVING_DIR}/serving_dataset.csv'\n",
    "\n",
    "# Create the directory\n",
    "!mkdir -p {SERVING_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpSqjjXeafHr"
   },
   "outputs": [],
   "source": [
    "# Read a subset of the training dataset\n",
    "serving_data = pd.read_csv(TRAINING_DATA, nrows=100)\n",
    "\n",
    "# Drop the `Cover_Type` column\n",
    "serving_data.drop(columns='Cover_Type', inplace=True)\n",
    "\n",
    "# Save the modified dataset\n",
    "serving_data.to_csv(SERVING_DATA, index=False)\n",
    "\n",
    "# Delete unneeded variable from memory\n",
    "del serving_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "dzKVqBogmG__",
    "outputId": "12ee8c71-e5eb-453b-8a24-7fb1be1aa610"
   },
   "outputs": [],
   "source": [
    "# Declare StatsOptions to use the curated schema\n",
    "stats_options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "\n",
    "# Compute the statistics of the serving dataset\n",
    "serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=stats_options)\n",
    "\n",
    "# Detect anomalies in the serving dataset\n",
    "anomalies = tfdv.validate_statistics(serving_stats, schema=schema)\n",
    "\n",
    "# Display the anomalies detected\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwIOMZYjt_Kb"
   },
   "source": [
    "As expected, the missing column is flagged. To fix this, you need to configure the schema to detect when it's being used for training or for inference / serving. You can do this by setting [schema environments](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic#schema_environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBVVj8Hum0Ue"
   },
   "source": [
    "<a name='ex-6'></a>\n",
    "#### Exercise 6: Define the serving environment\n",
    "\n",
    "Complete the code below to ignore the `Cover_Type` feature when validating in the *SERVING* environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpgqThxImAso"
   },
   "outputs": [],
   "source": [
    "schema.default_environment.append('TRAINING')\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Hint: Create another default schema environment with name SERVING (pass in a string)\n",
    "schema.default_environment.append(None)\n",
    "\n",
    "# Remove Cover_Type feature from SERVING using TFDV\n",
    "# Hint: Pass in the strings with the name of the feature and environment \n",
    "tfdv.get_feature(schema, None).not_in_environment.append(None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge78zQcpxI-T"
   },
   "source": [
    "If done correctly, running the cell below should show *No Anomalies*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57
    },
    "id": "rsDf37to4SoV",
    "outputId": "2cdbda96-0e03-48b0-a7ae-46346dcf7b81"
   },
   "outputs": [],
   "source": [
    "# Validate the serving dataset statistics in the `SERVING` environment\n",
    "anomalies = tfdv.validate_statistics(serving_stats, schema=schema, environment='SERVING')\n",
    "\n",
    "# Display the anomalies detected\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gpZl3CVyKyv"
   },
   "source": [
    "We can now save this curated schema in a local directory so we can import it to our TFX pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXnDWoGoxsmc"
   },
   "outputs": [],
   "source": [
    "# Declare the path to the updated schema directory\n",
    "UPDATED_SCHEMA_DIR = f'{PIPELINE_DIR}/updated_schema'\n",
    "\n",
    "# Create the said directory\n",
    "!mkdir -p {UPDATED_SCHEMA_DIR}\n",
    "\n",
    "# Declare the path to the schema file\n",
    "schema_file = os.path.join(UPDATED_SCHEMA_DIR, 'schema.pbtxt')\n",
    "\n",
    "# Save the curated schema to the said file\n",
    "tfdv.write_schema_text(schema, schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljco7hZ9x0ip"
   },
   "source": [
    "As a sanity check, let's display the schema we just saved and verify that it contains the changes we introduced. It should still show the ranges in the `Domain` column and there should be two environments available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "QNvH6V6yxtUm",
    "outputId": "657e1413-2db8-473a-f4dc-169144b96e10"
   },
   "outputs": [],
   "source": [
    "# Load the schema from the directory we just created\n",
    "new_schema = tfdv.load_schema_text(schema_file)\n",
    "\n",
    "# Display the schema. Check that the Domain column still contains the ranges.\n",
    "tfdv.display_schema(schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxK7GqSgybrb",
    "outputId": "81129ecf-9475-4c0c-fd7b-da93ef6e8092"
   },
   "outputs": [],
   "source": [
    "# The environment list should show `TRAINING` and `SERVING`.\n",
    "new_schema.default_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npuw7JwMyQ6I"
   },
   "source": [
    "<a name='4-7'></a>\n",
    "### 4.7 - Generate new statistics using the updated schema\n",
    "\n",
    "You will now compute the statistics using the schema you just curated. Remember though that TFX components interact with each other by getting artifact information from the metadata store. So you first have to import the curated schema file into ML Metadata. You will do that by using an [ImporterNode](https://www.tensorflow.org/tfx/guide/statsgen#using_the_statsgen_component_with_a_schema) to create an artifact representing the curated schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78EzJ7Wfm5oa"
   },
   "source": [
    "<a name='ex-7'></a>\n",
    "#### Exercise 7: ImporterNode\n",
    "\n",
    "Complete the code below to create a `Schema` artifact that points to the curated schema directory. Pass in an `instance_name` as well and name it `import_user_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "bhaE8ha13lk0",
    "outputId": "7898ddec-f1a9-4c19-e39a-cea5f30ec491"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# Use an ImporterNode to put the curated schema to ML Metadata\n",
    "user_schema_importer = None\n",
    "    \n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# Run the component\n",
    "context.run(None, enable_cache=False)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "context.show(user_schema_importer.outputs['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR1SG-KYnBTR"
   },
   "source": [
    "With the artifact successfully created, you can now use `StatisticsGen` and pass in a `schema` parameter to use the curated schema.\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "#### Exercise 8: Statistics with the new schema\n",
    "\n",
    "Use `StatisticsGen` to compute the statistics with the schema you updated in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "_5HmdXKKjXF-",
    "outputId": "5eb7bb64-211b-4e16-fc55-6fa6c467bc82"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Use StatisticsGen to compute the statistics using the curated schema\n",
    "statistics_gen_updated = None\n",
    "    \n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# Run the component\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "9e1f6cH2jiPD",
    "outputId": "079fd74a-6dda-4abc-b096-d7ed044e234f"
   },
   "outputs": [],
   "source": [
    "context.show(statistics_gen_updated.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ya6FRcLy11U"
   },
   "source": [
    "The chart will look mostly the same from the previous runs but you can see that the `Cover Type` is now under the categorical features. That shows that `StatisticsGen` is indeed using the updated schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7IurTkkwUNP"
   },
   "source": [
    "<a name='4-8'></a>\n",
    "### 4.8 - Check anomalies\n",
    "\n",
    "You will now check if the dataset has any anomalies with respect to the schema. You can do that easily with the [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx3kqz0CnEqr"
   },
   "source": [
    "<a name='ex-9'></a>\n",
    "#### Exercise 9: ExampleValidator\n",
    "\n",
    "Check if there are any anomalies using `ExampleValidator`. You will need to pass in the updated statistics and schema from the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "graded": true,
    "id": "kvBzLuyPqboL",
    "name": "training_anomalies",
    "outputId": "5bcc9773-7df7-4cd0-b816-a1d5679da0ff"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "example_validator = None\n",
    "    \n",
    "    \n",
    "\n",
    "# Run the component.\n",
    "None\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "FdBbjraG2PZf",
    "outputId": "19888998-729e-4179-fcb0-66f3b668fceb"
   },
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duwFgpTYIaYD"
   },
   "source": [
    "<a name='4-10'></a>\n",
    "### 4.10 - Feature engineering\n",
    "\n",
    "You will now proceed to transforming your features to a form suitable for training a model. This can include several methods such as scaling and converting strings to vocabulary indices. It is important for these transformations to be consistent across your training data, and also for the serving data when the model is deployed for inference. TFX ensures this by generating a graph that will process incoming data both during training and inference.\n",
    "\n",
    "Let's first declare the constants and utility function you will use for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz5VLpMF0R_s"
   },
   "outputs": [],
   "source": [
    "# Set the constants module filename\n",
    "_cover_constants_module_file = 'cover_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOkTSEia0UE3",
    "outputId": "842c7c91-bff7-4543-9d9e-25fb9fdd3719"
   },
   "outputs": [],
   "source": [
    "%%writefile {_cover_constants_module_file}\n",
    "\n",
    "SCALE_MINMAX_FEATURE_KEYS = [\n",
    "        \"Horizontal_Distance_To_Hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\",\n",
    "    ]\n",
    "\n",
    "SCALE_01_FEATURE_KEYS = [\n",
    "        \"Hillshade_9am\",\n",
    "        \"Hillshade_Noon\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\",\n",
    "    ]\n",
    "\n",
    "SCALE_Z_FEATURE_KEYS = [\n",
    "        \"Elevation\",\n",
    "        \"Slope\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "    ]\n",
    "\n",
    "VOCAB_FEATURE_KEYS = [\"Wilderness_Area\"]\n",
    "\n",
    "HASH_STRING_FEATURE_KEYS = [\"Soil_Type\"]\n",
    "\n",
    "LABEL_KEY = \"Cover_Type\"\n",
    "\n",
    "# Utility function for renaming the feature\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYZX-xB03lk8"
   },
   "source": [
    "Next you will define the `preprocessing_fn` to apply transformations to the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdXFOnB4nKRn"
   },
   "source": [
    "<a name='ex-10'></a>\n",
    "#### Exercise 10: Preprocessing function\n",
    "\n",
    "Complete the module to transform your features. Refer to the code comments to get hints on what operations to perform.\n",
    "\n",
    "Here are some links to the docs of the functions you will need to complete this function:\n",
    "\n",
    "- [`tft.scale_by_min_max`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_by_min_max)\n",
    "- [`tft.scale_to_0_1`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_0_1)\n",
    "- [`tft.scale_to_z_score`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score)\n",
    "- [`tft.compute_and_apply_vocabulary`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/compute_and_apply_vocabulary)\n",
    "- [`tft.hash_strings`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/hash_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Elp0jej91iiT"
   },
   "outputs": [],
   "source": [
    "# Set the transform module filename\n",
    "_cover_transform_module_file = 'cover_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "graded": true,
    "id": "PJpPxzh6kdNM",
    "name": "exercise_10",
    "outputId": "6000419f-1124-4d04-af50-9a77e6cdec63"
   },
   "outputs": [],
   "source": [
    "%%writefile {_cover_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import cover_constants\n",
    "\n",
    "_SCALE_MINMAX_FEATURE_KEYS = cover_constants.SCALE_MINMAX_FEATURE_KEYS\n",
    "_SCALE_01_FEATURE_KEYS = cover_constants.SCALE_01_FEATURE_KEYS\n",
    "_SCALE_Z_FEATURE_KEYS = cover_constants.SCALE_Z_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = cover_constants.VOCAB_FEATURE_KEYS\n",
    "_HASH_STRING_FEATURE_KEYS = cover_constants.HASH_STRING_FEATURE_KEYS\n",
    "_LABEL_KEY = cover_constants.LABEL_KEY\n",
    "_transformed_name = cover_constants.transformed_name\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "\n",
    "    features_dict = {}\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for feature in _SCALE_MINMAX_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using scaling of min_max function\n",
    "        # Hint: Use tft.scale_by_min_max by passing in the respective column\n",
    "        features_dict[_transformed_name(feature)] = None\n",
    "\n",
    "    for feature in _SCALE_01_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using scaling of 0 to 1 function\n",
    "        # Hint: tft.scale_to_0_1\n",
    "        features_dict[_transformed_name(feature)] = None\n",
    "\n",
    "    for feature in _SCALE_Z_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using scaling to z score\n",
    "        # Hint: tft.scale_to_z_score\n",
    "        features_dict[_transformed_name(feature)] = None\n",
    "\n",
    "    for feature in _VOCAB_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using vocabulary available in column\n",
    "        # Hint: Use tft.compute_and_apply_vocabulary\n",
    "        features_dict[_transformed_name(feature)] = None\n",
    "\n",
    "    for feature in _HASH_STRING_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform by hashing strings into buckets\n",
    "        # Hint: Use tft.hash_strings with the param hash_buckets set to 10\n",
    "        features_dict[_transformed_name(feature)] = None\n",
    "    \n",
    "    ### END CODE HERE ###  \n",
    "\n",
    "    # No change in the label\n",
    "    features_dict[_LABEL_KEY] = inputs[_LABEL_KEY]\n",
    "\n",
    "    return features_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNhFUd68nTYQ"
   },
   "source": [
    "<a name='ex-11'></a>\n",
    "#### Exercise 11: Transform\n",
    "\n",
    "Use the [TFX Transform component](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Transform) to perform the transformations and generate the transformation graph. You will need to pass in the dataset examples, *curated* schema, and the module that contains the preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "dq88l0XjkdQI",
    "outputId": "ed226bf9-c374-4494-b603-89187f93cb87"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Instantiate the Transform component\n",
    "transform = None\n",
    "    \n",
    "    \n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Run the component\n",
    "context.run(transform, enable_cache=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a few examples of the transformed dataset to see if the transformations are done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    transform_uri = transform.outputs['transformed_examples'].get()[0].uri\n",
    "\n",
    "# for grading since context.run() does not work outside the notebook\n",
    "except IndexError:\n",
    "    print(\"context.run() was no-op\")\n",
    "    examples_path = './pipeline/Transform/transformed_examples'\n",
    "    dir_id = os.listdir(examples_path)[0]\n",
    "    transform_uri = f'{examples_path}/{dir_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fU8Xa_hkdUS"
   },
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples\n",
    "train_uri = os.path.join(transform_uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYOiQrUS5Kvz",
    "outputId": "ce3b955b-9496-414c-bb97-4e47430333c5"
   },
   "outputs": [],
   "source": [
    "# import helper function to get examples from the dataset\n",
    "from util import get_records\n",
    "\n",
    "# Get 3 records from the dataset\n",
    "sample_records_xf = get_records(transformed_dataset, 3)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records_xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMFcfxSQZag-"
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5 - ML Metadata\n",
    "\n",
    "TFX uses [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd) under the hood to keep records of artifacts that each component uses. This makes it easier to track how the pipeline is run so you can troubleshoot if needed or want to reproduce results.\n",
    "\n",
    "In this final section of the assignment, you will demonstrate going through this metadata store to retrieve related artifacts. This skill is useful for when you want to recall which inputs are fed to a particular stage of the pipeline. For example, you can know where to locate the schema used to perform feature transformation, or you can determine which set of examples were used to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will start by importing the relevant modules and setting up the connection to the metadata store. We have also provided some helper functions for displaying artifact information and you can review its code in the external `util.py` module in your lab workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txI2RjkVJcU5"
   },
   "outputs": [],
   "source": [
    "# Import mlmd and utilities\n",
    "import ml_metadata as mlmd\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from util import display_types, display_artifacts, display_properties\n",
    "\n",
    "# Get the connection config to connect to the metadata store\n",
    "connection_config = context.metadata_connection_config\n",
    "\n",
    "# Instantiate a MetadataStore instance with the connection config\n",
    "store = mlmd.MetadataStore(connection_config)\n",
    "\n",
    "# Declare the base directory where All TFX artifacts are stored\n",
    "base_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSVNzeWwFGkq"
   },
   "source": [
    "<a name='5-1'></a>\n",
    "#### 5.1 -  Accessing stored artifacts\n",
    "\n",
    "With the connection setup, you can now interact with the metadata store. For instance, you can retrieve all artifact types stored with the `get_artifact_types()` function. For reference, the API is documented [here](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "pkZHeeKoJnf7",
    "outputId": "ce9594eb-84ff-4e9d-dd8d-9dba4fdcd512"
   },
   "outputs": [],
   "source": [
    "# Get the artifact types\n",
    "types = store.get_artifact_types()\n",
    "\n",
    "# Display the results\n",
    "display_types(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get a list of artifacts for a particular type to see if there are variations used in the pipeline. For example, you curated a schema in an earlier part of the assignment so this should appear in the records. Running the cell below should show at least two rows: one for the inferred schema, and another for the updated schema. If you ran this notebook before, then you might see more rows because of the different schema artifacts saved under the `./SchemaGen/schema` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "fU7gS-XnJqWx",
    "outputId": "23985fd3-1f21-4c7b-915f-1a4af6ef75f4"
   },
   "outputs": [],
   "source": [
    "# Retrieve the transform graph list\n",
    "schema_list = store.get_artifacts_by_type('Schema')\n",
    "\n",
    "# Display artifact properties from the results\n",
    "display_artifacts(store, schema_list, base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you can also get the properties of a particular artifact. TFX declares some properties automatically for each of its components. You will most likely see `name`, `state` and `producer_component` for each artifact type. Additional properties are added where appropriate. For example, a `split_names` property is added in `ExampleStatistics` artifacts to indicate which splits the statistics are generated for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "UdAs1SRNJstv",
    "outputId": "88b1f200-f9b8-4ceb-fce2-aab80c7f6315"
   },
   "outputs": [],
   "source": [
    "# Get the latest TransformGraph artifact\n",
    "statistics_artifact = store.get_artifacts_by_type('ExampleStatistics')[-1]\n",
    "\n",
    "# Display the properties of the retrieved artifact\n",
    "display_properties(store, statistics_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUPy2F4pFWyT"
   },
   "source": [
    "<a name='5-2'></a>\n",
    "#### 5.2 - Tracking artifacts\n",
    "\n",
    "For this final exercise, you will build a function to return the parent artifacts of a given one. For example, this should be able to list the artifacts that were used to generate a particular `TransformGraph` instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AskXcyG2EGcO"
   },
   "source": [
    "<a name='ex-12'></a>\n",
    "##### Exercise 12: Get parent artifacts\n",
    "\n",
    "Complete the code below to track the inputs of a particular artifact.\n",
    "\n",
    "Tips:\n",
    "\n",
    "* You may find [get_events_by_artifact_ids()](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore#get_events_by_artifact_ids) and [get_events_by_execution_ids()](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore#get_executions_by_id) useful here. \n",
    "\n",
    "* Some of the methods of the MetadataStore class (such as the two given above) only accepts iterables so remember to convert to a list (or set) if you only have an int (e.g. pass `[x]` instead of `x`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_g-QAGQJpBO"
   },
   "outputs": [],
   "source": [
    "def get_parent_artifacts(store, artifact):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Get the artifact id of the input artifact\n",
    "    artifact_id = None\n",
    "    \n",
    "    # Get events associated with the artifact id\n",
    "    artifact_id_events = None\n",
    "    \n",
    "    # From the `artifact_id_events`, get the execution ids of OUTPUT events.\n",
    "    # Cast to a set to remove duplicates if any.\n",
    "    execution_id = set( \n",
    "        None\n",
    "        for event in None\n",
    "        if None == None\n",
    "    )\n",
    "    \n",
    "    # Get the events associated with the execution_id\n",
    "    execution_id_events = None\n",
    "\n",
    "    # From execution_id_events, get the artifact ids of INPUT events.\n",
    "    # Cast to a set to remove duplicates if any.\n",
    "    parent_artifact_ids = set( \n",
    "        event.artifact_id\n",
    "        for event in execution_id_events\n",
    "        if event.type == metadata_store_pb2.Event.INPUT\n",
    "    )\n",
    "    \n",
    "    # Get the list of artifacts associated with the parent_artifact_ids\n",
    "    parent_artifact_list = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return parent_artifact_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "2n4gfM64Jujt",
    "outputId": "d96f89c7-43e3-4ceb-8724-809f0d43bc18"
   },
   "outputs": [],
   "source": [
    "# Get an artifact instance from the metadata store\n",
    "artifact_instance = store.get_artifacts_by_type('TransformGraph')[0]\n",
    "\n",
    "# Retrieve the parent artifacts of the instance\n",
    "parent_artifacts = get_parent_artifacts(store, artifact_instance)\n",
    "\n",
    "# Display the results\n",
    "display_artifacts(store, parent_artifacts, base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "*Note: The ID numbers may differ.*\n",
    "\n",
    "| artifact id | type | uri |\n",
    "| ----------- | ---- | --- |\n",
    "| 1\t| Examples | ./CsvExampleGen/examples/1 |\n",
    "| 4\t| Schema | ./updated_schema |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You have now completed the assignment for this week. You've demonstrated your skills in selecting features, performing a data pipeline, and retrieving information from the metadata store. Having the ability to put these all together will be critical when working with production grade machine learning projects. For next week, you will work on more data types and see how these can be prepared in an ML pipeline. **Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "MLEPC2W3-1X",
    "MLEPC2W3-2X",
    "MLEPC2W3-3X",
    "MLEPC2W3-4X",
    "MLEPC2W3-5X",
    "MLEPC2W3-6X",
    "MLEPC2W3-7X",
    "MLEPC2W3-8X",
    "MLEPC2W3-9X",
    "MLEPC2W3-10X",
    "MLEPC2W3-11X",
    "MLEPC2W3-12X"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
