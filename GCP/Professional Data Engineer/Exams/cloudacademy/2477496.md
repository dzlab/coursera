# Cert Prep: GCP Data Engineer
Exam Session

**#30**

Which methods can be used to reduce the number of rows processed by BigQuery?
- [ ] Putting data in partitions; using the LIMIT clause
- [ ] Splitting tables into multiple tables; using the LIMIT clause
- [ ] Splitting tables into multiple tables; putting data in partitions; using the LIMIT clause
- [x] Splitting tables into multiple tables; putting data in partitions

**Explanation**

If you split a table into multiple tables (such as one table for each day), then you can limit your query to the data in specific tables (such as for particular days). A better method is to use a partitioned table, as long as your data can be separated by the day.

If you use the LIMIT clause, BigQuery will still process the entire table.

https://cloud.google.com/bigquery/docs/partitioned-tables

Covered in this lecture: Reducing the Amount of Data Processed

Course: Optimizing Google BigQuery

**#41**

Which of these rules apply when you add preemptible workers to a Dataproc cluster? (Choose 2 answers)
- [ ] Preemptible workers cannot use persistent disk.
- [x] A Dataproc cluster cannot have only preemptible workers.
- [x] Preemptible workers cannot store data.
- [ ] If a preemptible worker is reclaimed, then a replacement worker must be added manually.

**Explanation**

The following rules will apply when you use preemptible workers with a Cloud Dataproc cluster:
- Processing only—Since preemptibles can be reclaimed at any time, preemptible workers do not store data. Preemptibles added to a Cloud Dataproc cluster only function as processing nodes.
- No preemptible-only clusters—To ensure clusters do not lose all workers, Cloud Dataproc cannot create preemptible-only clusters.
- Persistent disk size—As a default, all preemptible workers are created with the smaller of 100GB or the primary worker boot disk size. This disk space is used for local caching of data and is not available through HDFS.

The managed group automatically re-adds workers lost due to reclamation as capacity permits.

https://cloud.google.com/dataproc/docs/concepts/preemptible-vms

**#45**

Which Cloud Dataflow Beam feature should you use to aggregate data in an unbounded data source every hour based on the time when the data entered the pipeline?
- [ ] An event time trigger
- [x] A processing time trigger
- [ ] An hourly watermark
- [ ] The withAllowedLateness method

**Explanation**

When collecting and grouping data into windows, Beam uses triggers to determine when to emit the aggregated results of each window.
- **Processing time triggers.** These triggers operate on the processing time – the time when the data element is processed at any given stage in the pipeline.
- **Event time triggers.** These triggers operate on the event time, as indicated by the timestamp on each data element. Beam’s default trigger is event time-based.



https://beam.apache.org/documentation/programming-guide/#triggers
