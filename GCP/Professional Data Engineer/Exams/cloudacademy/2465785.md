# Exam Session

**#2**

What are 3 techniques you can use to reduce overfitting in a neural network? (Select 3 answers)
- [x] Add a dropout layer
- [x] Apply L1 regularization
- [x] Reduce the number of features
- [ ] Apply zero-padding

**Explanation**

- A dropout layer reduces overfitting by randomly removing neurons during the training phase.
- L1 regularization reduces overfitting by its tendency to make model weights stay at zero, which effectively reduces the number of features in the model. Alternatively, you can reduce the number of features manually in the input data.
- Zero-padding is used to make a feature map the same size as the input image. It is not used to reduce overfitting.

https://www.tensorflow.org/tutorials/wide#adding_regularization_to_prevent_overfitting

**#4**

Which Java SDK class can you use to run your Dataflow programs locally?
- [x] DirectRunner
- [ ] LocalRunner
- [ ] MachineRunner
- [ ] LocalPipelineRunner

**Explanation**

DirectRunner allows you to execute operations in the pipeline directly, without any optimization. Useful for small local execution and tests

https://cloud.google.com/dataflow/docs/guides/specifying-exec-params

**#6**

You have a job that you want to cancel. It is a streaming pipeline, and you want to ensure that any data that is in-flight is processed and written to the output. Which of the following commands can you use on the Dataflow monitoring console to stop the pipeline job?
- [x] Drain
- [ ] Cancel
- [ ] Finish
- [ ] Stop

**Explanation**

Using the Drain option to stop your job tells the Dataflow service to finish your job in its current state. Your job will immediately stop ingesting new data from input sources, but the Dataflow service will preserve any existing resources (such as worker instances) to finish processing and writing any buffered data in your pipeline.

https://cloud.google.com/dataflow/pipelines/stopping-a-pipeline

**#8**

Which of the following statements is incorrect regarding Bigtable access roles?
- [ ] You can configure access control at the instance and project level.
- [ ] Using IAM roles, you cannot give a user access to only one table in a project, rather than all tables in a project.
- [ ] To give a user access to only one table in a project, you must configure access through your application.
- [x] To give a user access to only one table in a project, grant the user the Bigtable Editor role for that table.

**Explanation**

For Cloud Bigtable, you can configure access control at the project level. For example, you can grant the ability to:
- Read from, but not write to, any table within the project.
- Read from and write to any table within the project, but not manage instances.
- Read from and write to any table within the project, and manage instances.

https://cloud.google.com/bigtable/docs/access-control

**#15**
Cloud Dataproc charges you only for what you really use with _____ billing. 
- [ ] daily
- [ ] hour-by-hour
- [ ] minute-by-minute
- [x] second-by-second

**Explanation**

One of the advantages of Cloud Dataproc is its low cost. Dataproc charges for what you really use with second-by-second billing and a one-minute-minimum billing period.

https://cloud.google.com/dataproc/docs/concepts/overview

**#19**

Which Cloud Dataflow Beam feature should you use to aggregate data in an unbounded data source every hour based on the time when the data entered the pipeline?
- [ ] An event time trigger
- [x] A processing time trigger
- [ ] An hourly watermark
- [ ] The withAllowedLateness method

**Explanation**

When collecting and grouping data into windows, Beam uses triggers to determine when to emit the aggregated results of each window.
- **Processing time triggers**. These triggers operate on the processing time – the time when the data element is processed at any given stage in the pipeline.
- **Event time triggers**. These triggers operate on the event time, as indicated by the timestamp on each data element. Beam’s default trigger is event time-based.

https://beam.apache.org/documentation/programming-guide/#triggers

**#35**

What are all of the BigQuery operations that Google charges for?
- [x] Storage, queries, and streaming inserts
- [ ] Storage, queries, and loading data from a file
- [ ] Storage, queries, and exporting data
- [ ] Queries and streaming inserts

**Explanation**

Google charges for storage, queries, and streaming inserts. Loading data from a file and exporting data are free operations.

https://cloud.google.com/bigquery/pricing

**#38**

Suppose you have a table that includes a nested column called "city" inside a column called "person", but when you try to submit the following query in BigQuery, it gives you an error.SELECT person FROM `project1.example.table1` WHERE city = "London"How would you correct the error?
- [ ] Change "person" to "person.city".
- [ ] Change "person" to "city.person".
- [ ] Add ", UNNEST(city)" before the WHERE clause.
- [x] Add ", UNNEST(person)" before the WHERE clause.

**Explanation**

To access the person.city column, you need to "UNNEST(person)" and JOIN it to table1 using a comma.

https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql#nested_repeated_results

**#43**

Which of these operations can you perform from the BigQuery Web UI?
- [x] Load data with nested and repeated fields.
- [ ] Upload a 20 MB file.
- [ ] Upload multiple files using a wildcard.
- [ ] Upload a file in SQL format.

**Explanation**

You can load data with nested and repeated fields using the Web UI.

You cannot use the Web UI to:
- Upload a file greater than 10 MB in size
- Upload multiple files at the same time
- Upload a file in SQL format

You can use the "bq" command to perform the first two operations above, but BigQuery does not support uploading files in SQL format regardless of which interface you use.

https://cloud.google.com/bigquery/loading-data

Covered in this lecture: Nested Repeated Fields
Course: Optimizing Google BigQuery

**#45**
Which of these rules apply when you add preemptible workers to a Dataproc cluster? (Choose 2 answers)
- [ ] Preemptible workers cannot use persistent disk.
- [x] A Dataproc cluster cannot have only preemptible workers.
- [x] Preemptible workers cannot store data.
- [ ] If a preemptible worker is reclaimed, then a replacement worker must be added manually.

**Explanation**

The following rules will apply when you use preemptible workers with a Cloud Dataproc cluster:
- Processing only—Since preemptibles can be reclaimed at any time, preemptible workers do not store data. Preemptibles added to a Cloud Dataproc cluster only function as processing nodes.
- No preemptible-only clusters—To ensure clusters do not lose all workers, Cloud Dataproc cannot create preemptible-only clusters.
- Persistent disk size—As a default, all preemptible workers are created with the smaller of 100GB or the primary worker boot disk size. This disk space is used for local caching of data and is not available through HDFS.

The managed group automatically re-adds workers lost due to reclamation as capacity permits.

https://cloud.google.com/dataproc/docs/concepts/preemptible-vms
