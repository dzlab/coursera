# Chapter 5 Building and Operationalizing Processing Infrastructure

## Provisioning and Adjusting Processing Resources

### Provisioning and Adjusting Compute Engine
Compute Engine supports provisioning single instances or groups of instances, known as instance groups. Instance groups are either:
- Managed instance groups (MIGs) consist of identically configured VMs;
- unmanaged instance groups allow for heterogeneous VMs

#### Provisioning Single VM Instances
range of parameters, including the following:

- Machine type, which specifies the number of vCPUs and the amount of memory
- Region and zone to create the VM
- Boot disk parameters
- Network configuration details
- Disk image
- Service account for the VM Metadata and tags

Also,  Shielded VMs for additional secu- rity or GPUs and tensor processing units (TPUs) for additional processing resources.

```sh
gcloud compute instances create instance-1 --zone=us-central1-a --machine-type=n1-standard-1 --subnet=default --network-tier=PREMIUM --image=debian-9-stretch-v20191115 --image-project=debian-cloud --boot-disk-size=10GB --boot-disk-type=pd-standard
```

#### Provisioning Managed Instance Groups
Managed instance groups are managed as a single logical resource. MIGs have several use- ful properties, including:
- Autohealing based on application-specific health checks, which replaces nonfunction- ing instances
- Support for multizone groups that provide for availability in spite of zone-level failures
- Load balancing to distribute workload across all instances in the group
- Autoscaling, which adds or removes instances in the group to accommodate increases and decreases in workloads
- Automatic, incremental updates to reduce disruptions to workload processing

The specifications for a MIG are defined in an instance template, which is a file with VM specifications. Example:
```sh
gcloud compute instance-templates create pde-exam-template-custom \
--machine-type n1-standard-4 \
--image-family debian-9 \
--image-project debian-cloud \
--boot-disk-size 250GB
```
To create an instance from this template
```sh
gcloud compute instances create pde-exam-instance --source-instance-template=pde-exam-template-custom
```

When creating an instance group, you can specify:
- the name of the instance template
- a target CPU utilization that will trigger adding instances to the group up to the maximum number of instances allowed.
- a minimum number of instances.
- the default cooldown period, which is the amount of time GCP will wait before collecting performance statistics from the instance. This gives the instance time to complete startup before its performance is considered for adjusting
the instance group.

#### Adjusting Compute Engine Resources to Meet Demand
One of the advantages of a managed instance group is that the number of VMs in the group can change according to workload. This type of horizontal scaling is readily imple- mented in MIGs because of autoscaling.

An alternative, vertical scaling, requires moving services from one VM to another VM with more or fewer resources.

Many data engineer- ing use cases have workloads that can be distributed across VMs so that horizontal scaling with MIGs is a good option.

If you have a high-performance computing (HPC) workload that must be run on a single server—for example, a monolithic simulation—then you may need to scale VMs vertically.

### Provisioning and Adjusting Kubernetes Engine

#### Provisioning a Kubernetes Engine Cluster
To create a Kubernetes cluster from the command line
```sh
gcloud container clusters create "standard-cluster-1" --zone "us-central1-a" --cluster-version "1.13.11-gke.14" --machine-type "n1-standard-1"
--image-type "COS" --disk-type "pd-standard" --disk-size "100" --num-nodes "5" --enable-autoupgrade --enable-autorepair
```

If a node has sufficient available resources, the scheduler can run the job on that node. You can control where jobs are run using the Kubernetes abstractions known as taints. By assigning taints to node pools and tolerances for taints to pods, you can control when pods are run.

#### Adjusting Kubernetes Engine Resources to Meet Demand
Adjust resources is by scaling applications running in clusters and scaling clusters themselves.

adjust the number of replicas for a deployment named pde-example-application:
```sh
kubectl scale deployment pde-example-application --replicas 6
```

Alternatively, you can configure deployments to autoscale using the kubectl autoscale command. For example, consider the following:
```sh
kubectl autoscale deployment pde-example-application --min 2 --max 8 --cpu-percent 65
```

This command will autoscale the number of replicas of the pde-example-application between two and eight replicas depending on CPU utilization. In this case, if CPU utilization across all pods is greater than 65 percent, replicas will be added up to the maximum number specified.

#### Autoscaling Clusters in Kubernetes Engine
In Kubernetes nodes are implemented as Compute Engine VMs, and node pools are imple- mented using managed instance groups.

Cluster autoscaling is done at the node pool level. When you create a node pool, you can specify a minimum and maximum number of nodes in the pool. The autoscaler adjusts the number of nodes in the pool based on resource requests, not actual utilization. If pods are unscheduled because there is not a sufficient number of nodes in the node pool to run those pods, then more nodes will be added up to the maximum number of nodes specified for that node pool. If nodes are underutilized, the autoscaler will remove nodes from the node pool down to the minimum number of nodes in the pool.

You can specify autoscaling parameters when creating a cluster. Here is an example that creates a cluster called pde-example-cluster in the us-central1-a zone, with nodes in three zones and a starting set of two nodes per node pool. Autoscaling is enabled with a minimum of one node and a maximum of four nodes.
```sh
gcloud container clusters create pde-example-cluster \
--zone us-central1-a \
--node-locations us-central1-a,us-central1-b,us-central1-f \
--num-nodes 2 --enable-autoscaling --min-nodes 1 --max-nodes 4
```

### Kubernetes YAML Configurations
Example application autoscaling based on CPU
```yaml
apiVersion: "autoscaling/v2beta1" kind: "HorizontalPodAutoscaler" metadata:
name: "nginx-1-hpa" namespace: "default" labels:
app: "nginx-1" spec:
scaleTargetRef:
  kind: "Deployment"
  name: "nginx-1" 
  apiVersion: "apps/v1"
minReplicas: 1
maxReplicas: 5 metrics:
- type: "Resource"
  resource:
    name: "cpu" 
    targetAverageUtilization: 80
```
One of the advantages of using a managed Kubernetes service like GKE is that configu- ration files like this are generated for you when using command-line and cloud console commands.

## Provisioning and Adjusting Cloud Bigtable
Cloud Bigtable is a managed wide-column NoSQL database used for applications that require high-volume, low-latency random reads and writes, such as IoT applications, and for analytic use cases, such as storing large volumes of data used to train machine learning models. Bigtable has an HBase interface, so it is also a good alternative to using Hadoop HBase on a Hadoop cluster.

### Provisioning Bigtable Instances
Options for configuring Bigtable
- The instance type can be either production or development. Production instances have clusters with a minimum of three nodes; development instances have a single node and do not provide for high availability.
- Storage types are SSD and HDD. SSDs are used when low-latency I/O is a priority. HDD instances have higher read latency.

Create Bitable with `gcloud` cli
```sh
gcloud bigtable instances create pde-bt-instance1 \
--cluster=pde-bt-cluster1 \
--cluster-zone=us-west1-a \
--display-name=pdc-bt-instance-1 \
--cluster-num-nodes=6 \
--cluster-storage-type=SSD \
--instance-type=PRODUCTION
```

Bigtable has a command-line utility called `cbt`, which can also be used to create instances along with other operations on Bigtable instances. Previous cluster can be created with:
```sh
cbt createinstance pde-bt-instance1 pdc-bt-instance-1 pde-bt-cluster1 west1-a 6 SSD
```

Once created you can modify te following
- The number of nodes in each cluster
- The number of clusters
- Application profiles, which contain replication settings
- Labels, which are used to specify metadata attributes
- The display name

### Replication in Bigtable

## Provisioning and Adjusting Cloud Dataproc
When creating a Cloud Dataproc cluster, the cluster mode determines the number of master nodes and possible worker nodes.
- The standard mode has one master and some number of workers. 
- The single mode has one master and no workers.
- The high availability mode has three master nodes and some number of worker nodes.

Master nodes and worker nodes are configured separately. For each type of node, you can specify a machine type, disk size, and disk type. For worker nodes, you can specify machine type, disk size and type, a minimum number of nodes, and optional local SSDs.

```sh
gcloud dataproc clusters create pde-cluster-1 \
--region us-central1 \
--zone us-central1-b \ 
--master-machine-type n1-standard-1 \
--master-boot-disk-size 500 \
--num-workers 4
--worker-machine-type n1-standard-1 --worker-boot-disk-size 500
```
The number of worker nodes can also be adjusted automatically by specifying an autoscal- ing policy, which specifies the maximum number of nodes and a scale-up rate and a scale- down rate.

## Configuring Cloud Dataflow
Required parameters are as follows:
- Job name
- Project ID
- Runner, which is DataflowRunner for cloud execution
- Staging locations, which is a path to a Cloud Storage location for code packages
- Temporary location for temporary job files
Other options:
- number of workers to use by default when executing a pipeline as well as a maximum number of workers to use in cases where the workload would benefit from additional workers.
- disk size to use with Compute Engine worker instances. This may be important when running batch jobs that may require large amounts of space on the boot disk.

Cloud Dataflow does not require you to specify machine types, but you can specify machine type and worker disk type if you want that level of control.

## Configuring Managed Serverless Processing Services
