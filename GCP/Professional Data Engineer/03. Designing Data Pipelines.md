# Chapter 3 Designing Data Pipelines

## Exam Essentials

**Understand the model of data pipelines.** A data pipeline is an abstract concept that cap- tures the idea that data flows from one stage of processing to another. Data pipelines are modeled as directed acyclic graphs (DAGs). A graph is a set of nodes linked by edges. A directed graph has edges that flow from one node to another.

**Know the four stages in a data pipeline.** Ingestion is the process of bringing data into the GCP environment. Transformation is the process of mapping data from the structure used in the source system to the structure used in the storage and analysis stages of the data pipeline. Cloud Storage can be used as both the staging area for storing data imme- diately after ingestion and also as a long-term store for transformed data. BigQuery and Cloud Storage treat data as external tables and query them. Cloud Dataproc can use Cloud Storage as HDFS-compatible storage. Analysis can take on several forms, from simple
SQL querying and report generation to machine learning model training and data science analysis.

**Know that the structure and function of data pipelines will vary according to the use case to which they are applied.** Three common types of pipelines are data warehousing pipe- lines, stream processing pipelines, and machine learning pipelines.

**Know the common patterns in data warehousing pipelines.** Extract, transformation, and load (ETL) pipelines begin with extracting data from one or more data sources. When mul- tiple data sources are used, the extraction processes need to be coordinated. This is because extractions are often time based, so it is important that extracts from different sources cover the same time period. Extract, load, and transformation (ELT) processes are slightly different from ETL processes. In an ELT process, data is loaded into a database before transforming the data. Extraction and load procedures do not transform data. This kind
of process is appropriate when data does not require changes from the source format. In a change data capture approach, each change is a source system that is captured and recorded in a data store. This is helpful in cases where it is important to know all changes over time and not just the state of the database at the time of data extraction.

**Understand the unique processing characteristics of stream processing.** This includes the difference between event time and processing time, sliding and tumbling windows, late- arriving data and watermarks, and missing data. Event time is the time that something occurred at the place where the data is generated. Processing time is the time that data arrives at the endpoint where data is ingested. Sliding windows are used when you want to show how an aggregate, such as the average of the last three values, change over time, and you want to update that stream of averages each time a new value arrives in the stream. Tumbling windows are used when you want to aggregate data over a fixed period of time— for example, for the last one minute.

**Know the components of a typical machine learning pipeline.** This includes data inges- tion, data preprocessing, feature engineering, model training and evaluation, and deploy- ment. Data ingestion uses the same tools and services as data warehousing and streaming data pipelines. Cloud Storage is used for batch storage of datasets, whereas Cloud Pub/Sub can be used for the ingestion of streaming data. Feature engineering is a machine learn- ing practice in which new attributes are introduced into a dataset. The new attributes are derived from one or more existing attributes.

**Know that Cloud Pub/Sub is a managed message queue service.** Cloud Pub/Sub is a real-time messaging service that supports both push and pull subscription models. It is a managed service, and it requires no provisioning of servers or clusters. Cloud Pub/Sub will automatically scale as needed. Messaging queues are used in distributed systems to decou- ple services in a pipeline. This allows one service to produce more output than the consum- ing service can process without adversely affecting the consuming service. This is especially helpful when one process is subject to spikes.

**Know that Cloud Dataflow is a managed stream and batch processing service.** Cloud Dataflow is a core component for running pipelines that collect, transform, and output data. In the past, developers would typically create a stream processing pipeline (hot path) and a separate batch processing pipeline (cold path). Cloud Dataflow is based on Apache Beam, which is a model for combined stream and batch processing. Understand these key Cloud Dataflow concepts:
- Pipelines
- PCollection Transforms
- ParDo
- Pipeline I/O
- Aggregation
- User-defined functions
- Runner
- Triggers

**Know that Cloud Dataproc is a managed Hadoop and Spark service.** Cloud Dataproc makes it easy to create and destroy ephemeral clusters. Cloud Dataproc makes it easy to migrate from on-premises Hadoop clusters to GCP. A typical Cloud Dataproc cluster is configured with commonly used components of the Hadoop ecosystem, including Hadoop, Spark, Pig, and Hive. Cloud Dataproc clusters consist of two types of nodes: master nodes and worker nodes. The master node is responsible for distributing and managing workload distribution.

**Know that Cloud Composer is a managed service implementing Apache Airflow.** Cloud Composer is used for scheduling and managing workflows. As pipelines become more complex and have to be resilient when errors occur, it becomes more important to have
a framework for managing workflows so that you are not reinventing code for handling errors and other exceptional cases. Cloud Composer automates the scheduling and moni- toring of workflows. Before you can run workflows with Cloud Composer, you will need to create an environment in GCP.

**Understand what to consider when migrating from on-premises Hadoop and Spark to GCP.** Factors include migrating data, migrating jobs, and migrating HBase to Bigtable. Hadoop and Spark migrations can happen incrementally, especially since you will be using ephem- eral clusters configured for specific jobs. There may be cases where you will have to keep an on-premises cluster while migrating some jobs and data to GCP. In those cases, you will have to keep data synchronized between environments. It is a good practice to migrate HBase databases to Bigtable, which provides consistent, scalable performance.

## Review Questions
You can find the answers in the appendix.

**1.** A large enterprise using GCP has recently acquired a startup that has an IoT platform. The acquiring company wants to migrate the IoT platform from an on-premises data center to GCP and wants to use Google Cloud managed services whenever possible. What GCP ser- vice would you recommend for ingesting IoT data?
- [ ] A. Cloud Storage
- [ ] B. Cloud SQL
- [x] C. Cloud Pub/Sub
- [ ] D. BigQuery streaming inserts

```diff
+ C. The correct answer is C, Cloud Pub/Sub, which is a scalable, managed messaging queue that is typically used for ingesting high-volume streaming data.
- Option A is incorrect; Cloud Storage does not support streaming inserts, but Cloud Pub/Sub is designed to scale for high-volume writes and has other features useful for stream processing, such as acknowledging and processing a message.
- Option B is incorrect; Cloud SQL is not designed to support high volumes of low-latency writes like the kind needed in IoT applications.
- Option D is incorrect; although BigQuery has streaming inserts, the database is designed for analytic operations.
```

**2.** You are designing a data pipeline to populate a sales data mart. The sponsor of the project has had quality control problems in the past and has defined a set of rules for filtering out bad data before it gets into the data mart. At what stage of the data pipeline would you implement those rules?
- [ ] A. Ingestion
- [x] B. Transformation
- [ ] C. Storage
- [ ] D. Analysis

```diff
+ B. The correct answer is B. The transformation stage is where business logic and filters are applied.
- Option A is incorrect; ingestion is when data is brought into the GCP environment.
- Option C is incorrect—that data should be processed, and problematic data removed before storing the data.
- Answer D is incorrect; by the analysis stage, data should be fully transformed and available for analysis.
```

**3.** A team of data warehouse developers is migrating a set of legacy Python scripts that have been used to transform data as part of an ETL process. They would like to use a service that allows them to use Python and requires minimal administration and operations sup- port. Which GCP service would you recommend?
- [ ] A. Cloud Dataproc
- [x] B. Cloud Dataflow
- [ ] C. Cloud Spanner
- [ ] D. Cloud Dataprep

```diff
+ B. The correct answer is B. Cloud Dataflow supports Python and is a serverless platform.
- Option A is incorrect because, although it supports Python, you have to create and configure clusters.
- Option C is incorrect; Cloud Spanner is a horizontally scalable global relational database.
- Option D is incorrect; Cloud Dataprep is an interactive tool for preparing data for analysis.
```

**4.** You are using Cloud Pub/Sub to buffer records from an application that generates a stream of data based on user interactions with a website. The messages are read by another service that transforms the data and sends it to a machine learning model that will use it for train- ing. A developer has just released some new code, and you notice that messages are sent repeatedly at 10-minute intervals. What might be the cause of this problem?
- [ ] A. The new code release changed the subscription ID.
- [ ] B. The new code release changed the topic ID.
- [x] C. The new code disabled acknowledgments from the consumer.
- [ ] D. The new code changed the subscription from pull to push.

```diff
+ C. The correct answer is C; the new code disabled message acknowledgments. That caused Cloud Pub/Sub to consider the message outstanding for up to the duration of the acknowledgment wait time and then resend the message.
- Options A and B are incorrect; changing the subscription or topic IDs would cause problems but not the kind described.
- Option D is incorrect because the type of subscription does not influence whether messages are delivered multiple times.
```

**5.** It is considered a good practice to make your processing logic idempotent when consuming messages from a Cloud Pub/Sub topic. Why is that?
- [x] A. Messages may be delivered multiple times.
- [ ] B. Messages may be received out of order.
- [ ] C. Messages may be delivered out of order.
- [ ] D. A consumer service may need to wait extended periods of time between the delivery of messages.

```diff
+ A. The correct answer is A; messages may be delivered multiple times and therefore processed multiple times. If the logic were not idempotent, it could leave the application in an incorrect state, such as that which could occur if you counted the same message multiple times.
- Options B and C are incorrect; the order of delivery does not require idempotent operations.
- Option D is incorrect; the time between messages is not a factor in requiring logic to be idempotent.
```

**6.** A group of IoT sensors is sending streaming data to a Cloud Pub/Sub topic. A Cloud Data- flow service pulls messages from the topic and reorders the messages sorted by event time. A message is expected from each sensor every minute. If a message is not received from a sensor, the stream processing application should use the average of the values in the last four messages. What kind of window would you use to implement the missing data logic?
- [x] A. Sliding window
- [ ] B. Tumbling window
- [ ] C. Extrapolation window
- [ ] D. Crossover window

```diff
+ A. The correct answer is A; a sliding window would have the data for the past four minutes.
- Option B is incorrect because tumbling windows do not overlap, and the requirement calls for using the last four messages so the window must slide.
- Options C and D are not actually names of window types.
```

**7.** Your department is migrating some stream processing to GCP and keeping some on prem- ises. You are tasked with designing a way to share data from on-premises pipelines that use Kafka with GPC data pipelines that use Cloud Pub/Sub. How would you do that?
- [x] A. Use CloudPubSubConnector and Kafka Connect
- [ ] B. Stream data to a Cloud Storage bucket and read from there
- [ ] C. Write a service to read from Kafka and write to Cloud Pub/Sub
- [ ] D. Use Cloud Pub/Sub Import Service

```diff
+ A. The correct answer is A; you should use CloudPubSubConnector and Kafka Connect. The connector is developed and maintained by the Cloud Pub/Sub team for this purpose.
- Option B is incorrect since this is a less direct and efficient method.
- Option C requires maintaining a service.
- Option D is incorrect because there is no such service.
```

**8.** A team of developers wants to create standardized patterns for processing IoT data. Several teams will use these patterns. The developers would like to support collaboration and facilitate the use of patterns for building streaming data pipelines. What component should they use?
- [ ] A. Cloud Dataflow Python Scripts
- [ ] B. Cloud Dataproc PySpark jobs
- [x] C. Cloud Dataflow templates
- [ ] D. Cloud Dataproc templates

```diff
+ C. The correct answer is C. Use Cloud Dataflow templates to specify the pattern and provide parameters for users to customize the template.
- Option A is incorrect since this would require users to customize the code in the script.
- Options B and D are incorrect because Cloud Dataproc should not be used for this requirement. 
- Also, Option D is incorrect because there are no Cloud Dataproc templates.
```

**9.** You need to run several map reduce jobs on Hadoop along with one Pig job and four PySpark jobs. When you ran the jobs on premises, you used the department’s Hadoop cluster. Now you are running the jobs in GCP. What configuration for running these jobs would you recommend?
- [ ] A. Create a single cluster and deploy Pig and Spark in the cluster.
- [ ] B. Create one persistent cluster for the Hadoop jobs, one for the Pig job and one for the PySpark jobs.
- [ ] C. Create one cluster for each job, and keep the cluster running continuously so that you do not need to start a new cluster for each job.
- [x] D. Create one cluster for each job and shut down the cluster when the job completes.

```diff
+ D. The correct answer is D. You should create an ephemeral cluster for each job and delete the cluster after the job completes.
- Option A is incorrect because that is a more complicated configuration.
- Option B is incorrect because it keeps the cluster running instead of shutting down after jobs complete.
- Option C is incorrect because it keeps the clusters running after the jobs complete.
```

**10.** You are working with a group of genetics researchers analyzing data generated by gene sequencers. The data is stored in Cloud Storage. The analysis requires running a series of six programs, each of which will output data that is used by the next process in the pipe- line. The final result set is loaded into BigQuery. What tool would you recommend for orchestrating this workflow?
- [x] A. Cloud Composer
- [ ] B. Cloud Dataflow
- [ ] C. Apache Flink
- [ ] D. Cloud Dataproc

```diff
+ A. The correct answer is A, Cloud Composer, which is designed to support workflow orchestration.
- Options B and C are incorrect because they are both implementations of the Apache Beam model that is used for executing stream and batch processing program. 
- Option D is incorrect; Cloud Dataproc is a managed Hadoop and Spark service.
```

**11.** An on-premises data warehouse is currently deployed using HBase on Hadoop. You want to migrate the database to GCP. You could continue to run HBase within a Cloud Dataproc cluster, but what other option would help ensure consistent performance and support the HBase API?
- [ ] A. Store the data in Cloud Storage
- [x] B. Store the data in Cloud Bigtable
- [ ] C. Store the data in Cloud Datastore
- [ ] D. Store the data in Cloud Dataflow

```diff
+ B. The correct answer is B. The data could be stored in Cloud Bigtable, which provides consistent, scalable performance.
- Option A is incorrect because Cloud Storage is an object storage system, not a database.
- Option C is incorrect, since Cloud Datastore is a document- style NoSQL database and is not suitable for a data warehouse.
- Option D is incorrect; Cloud Dataflow is not a database.
```

**12.** The business owners of a data warehouse have determined that the current design of the data warehouse is not meeting their needs. In addition to having data about the state of systems at certain points in time, they need to know about all the times that data changed between those points in time. What kind of data warehousing pipeline should be used to meet this new requirement?
- [ ] A. ETL
- [ ] B. ELT
- [ ] C. Extraction and load
- [x] D. Change data capture

```diff
+ D. The correct answer is D. With change data capture, each change is a source system captured and recorded in a data store.
- Options A, B, and C all capture the state of source systems at a point in time and do not capture changes between those times.
```

