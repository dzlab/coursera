# Chapter 3 Designing Data Pipelines

## Exam Essentials

**Understand the model of data pipelines.** A data pipeline is an abstract concept that cap- tures the idea that data flows from one stage of processing to another. Data pipelines are modeled as directed acyclic graphs (DAGs). A graph is a set of nodes linked by edges. A directed graph has edges that flow from one node to another.

**Know the four stages in a data pipeline.** Ingestion is the process of bringing data into the GCP environment. Transformation is the process of mapping data from the structure used in the source system to the structure used in the storage and analysis stages of the data pipeline. Cloud Storage can be used as both the staging area for storing data imme- diately after ingestion and also as a long-term store for transformed data. BigQuery and Cloud Storage treat data as external tables and query them. Cloud Dataproc can use Cloud Storage as HDFS-compatible storage. Analysis can take on several forms, from simple
SQL querying and report generation to machine learning model training and data science analysis.

**Know that the structure and function of data pipelines will vary according to the use case to which they are applied.** Three common types of pipelines are data warehousing pipe- lines, stream processing pipelines, and machine learning pipelines.

**Know the common patterns in data warehousing pipelines.** Extract, transformation, and load (ETL) pipelines begin with extracting data from one or more data sources. When mul- tiple data sources are used, the extraction processes need to be coordinated. This is because extractions are often time based, so it is important that extracts from different sources cover the same time period. Extract, load, and transformation (ELT) processes are slightly different from ETL processes. In an ELT process, data is loaded into a database before transforming the data. Extraction and load procedures do not transform data. This kind
of process is appropriate when data does not require changes from the source format. In a change data capture approach, each change is a source system that is captured and recorded in a data store. This is helpful in cases where it is important to know all changes over time and not just the state of the database at the time of data extraction.

**Understand the unique processing characteristics of stream processing.** This includes the difference between event time and processing time, sliding and tumbling windows, late- arriving data and watermarks, and missing data. Event time is the time that something occurred at the place where the data is generated. Processing time is the time that data arrives at the endpoint where data is ingested. Sliding windows are used when you want to show how an aggregate, such as the average of the last three values, change over time, and you want to update that stream of averages each time a new value arrives in the stream. Tumbling windows are used when you want to aggregate data over a fixed period of time— for example, for the last one minute.

**Know the components of a typical machine learning pipeline.** This includes data inges- tion, data preprocessing, feature engineering, model training and evaluation, and deploy- ment. Data ingestion uses the same tools and services as data warehousing and streaming data pipelines. Cloud Storage is used for batch storage of datasets, whereas Cloud Pub/Sub can be used for the ingestion of streaming data. Feature engineering is a machine learn- ing practice in which new attributes are introduced into a dataset. The new attributes are derived from one or more existing attributes.

**Know that Cloud Pub/Sub is a managed message queue service.** Cloud Pub/Sub is a real-time messaging service that supports both push and pull subscription models. It is a managed service, and it requires no provisioning of servers or clusters. Cloud Pub/Sub will automatically scale as needed. Messaging queues are used in distributed systems to decou- ple services in a pipeline. This allows one service to produce more output than the consum- ing service can process without adversely affecting the consuming service. This is especially helpful when one process is subject to spikes.

**Know that Cloud Dataflow is a managed stream and batch processing service.** Cloud Dataflow is a core component for running pipelines that collect, transform, and output data. In the past, developers would typically create a stream processing pipeline (hot path) and a separate batch processing pipeline (cold path). Cloud Dataflow is based on Apache Beam, which is a model for combined stream and batch processing. Understand these key Cloud Dataflow concepts:
- Pipelines
- PCollection Transforms
- ParDo
- Pipeline I/O
- Aggregation
- User-defined functions
- Runner
- Triggers

**Know that Cloud Dataproc is a managed Hadoop and Spark service.** Cloud Dataproc makes it easy to create and destroy ephemeral clusters. Cloud Dataproc makes it easy to migrate from on-premises Hadoop clusters to GCP. A typical Cloud Dataproc cluster is configured with commonly used components of the Hadoop ecosystem, including Hadoop, Spark, Pig, and Hive. Cloud Dataproc clusters consist of two types of nodes: master nodes and worker nodes. The master node is responsible for distributing and managing workload distribution.

**Know that Cloud Composer is a managed service implementing Apache Airflow.** Cloud Composer is used for scheduling and managing workflows. As pipelines become more complex and have to be resilient when errors occur, it becomes more important to have
a framework for managing workflows so that you are not reinventing code for handling errors and other exceptional cases. Cloud Composer automates the scheduling and moni- toring of workflows. Before you can run workflows with Cloud Composer, you will need to create an environment in GCP.

**Understand what to consider when migrating from on-premises Hadoop and Spark to GCP.** Factors include migrating data, migrating jobs, and migrating HBase to Bigtable. Hadoop and Spark migrations can happen incrementally, especially since you will be using ephem- eral clusters configured for specific jobs. There may be cases where you will have to keep an on-premises cluster while migrating some jobs and data to GCP. In those cases, you will have to keep data synchronized between environments. It is a good practice to migrate HBase databases to Bigtable, which provides consistent, scalable performance.

## Review Questions
You can find the answers in the appendix.

**1.** A large enterprise using GCP has recently acquired a startup that has an IoT platform. The acquiring company wants to migrate the IoT platform from an on-premises data center to GCP and wants to use Google Cloud managed services whenever possible. What GCP ser- vice would you recommend for ingesting IoT data?
- [ ] A. Cloud Storage
- [ ] B. Cloud SQL
- [x] C. Cloud Pub/Sub
- [ ] D. BigQuery streaming inserts


**2.** You are designing a data pipeline to populate a sales data mart. The sponsor of the project has had quality control problems in the past and has defined a set of rules for filtering out bad data before it gets into the data mart. At what stage of the data pipeline would you implement those rules?
- [x] A. Ingestion
- [ ] B. Transformation
- [ ] C. Storage
- [ ] D. Analysis

**3.** A team of data warehouse developers is migrating a set of legacy Python scripts that have been used to transform data as part of an ETL process. They would like to use a service that allows them to use Python and requires minimal administration and operations sup- port. Which GCP service would you recommend?
- [ ] A. Cloud Dataproc
- [x] B. Cloud Dataflow
- [ ] C. Cloud Spanner
- [ ] D. Cloud Dataprep

**4.** You are using Cloud Pub/Sub to buffer records from an application that generates a stream of data based on user interactions with a website. The messages are read by another service that transforms the data and sends it to a machine learning model that will use it for train- ing. A developer has just released some new code, and you notice that messages are sent repeatedly at 10-minute intervals. What might be the cause of this problem?
- [ ] A. The new code release changed the subscription ID.
- [ ] B. The new code release changed the topic ID.
- [x] C. The new code disabled acknowledgments from the consumer.
- [ ] D. The new code changed the subscription from pull to push.

**5.** It is considered a good practice to make your processing logic idempotent when consuming messages from a Cloud Pub/Sub topic. Why is that?
- [x] A. Messages may be delivered multiple times.
- [ ] B. Messages may be received out of order.
- [ ] C. Messages may be delivered out of order.
- [ ] D. A consumer service may need to wait extended periods of time between the delivery of messages.

**6.** A group of IoT sensors is sending streaming data to a Cloud Pub/Sub topic. A Cloud Data- flow service pulls messages from the topic and reorders the messages sorted by event time. A message is expected from each sensor every minute. If a message is not received from a sensor, the stream processing application should use the average of the values in the last four messages. What kind of window would you use to implement the missing data logic?
- [x] A. Sliding window
- [ ] B. Tumbling window
- [ ] C. Extrapolation window
- [ ] D. Crossover window

**7.** Your department is migrating some stream processing to GCP and keeping some on prem- ises. You are tasked with designing a way to share data from on-premises pipelines that use Kafka with GPC data pipelines that use Cloud Pub/Sub. How would you do that?
- [x] A. Use CloudPubSubConnector and Kafka Connect
- [ ] B. Stream data to a Cloud Storage bucket and read from there
- [ ] C. Write a service to read from Kafka and write to Cloud Pub/Sub
- [ ] D. Use Cloud Pub/Sub Import Service

**8.** A team of developers wants to create standardized patterns for processing IoT data. Several teams will use these patterns. The developers would like to support collaboration and facilitate the use of patterns for building streaming data pipelines. What component should they use?
- [ ] A. Cloud Dataflow Python Scripts
- [ ] B. Cloud Dataproc PySpark jobs
- [x] C. Cloud Dataflow templates
- [ ] D. Cloud Dataproc templates

**9.** You need to run several map reduce jobs on Hadoop along with one Pig job and four PySpark jobs. When you ran the jobs on premises, you used the department’s Hadoop cluster. Now you are running the jobs in GCP. What configuration for running these jobs would you recommend?
- [ ] A. Create a single cluster and deploy Pig and Spark in the cluster.
- [ ] B. Create one persistent cluster for the Hadoop jobs, one for the Pig job and one for the PySpark jobs.
- [ ] C. Create one cluster for each job, and keep the cluster running continuously so that you do not need to start a new cluster for each job.
- [x] D. Create one cluster for each job and shut down the cluster when the job completes.

**10.** You are working with a group of genetics researchers analyzing data generated by gene sequencers. The data is stored in Cloud Storage. The analysis requires running a series of six programs, each of which will output data that is used by the next process in the pipe- line. The final result set is loaded into BigQuery. What tool would you recommend for orchestrating this workflow?
- [x] A. Cloud Composer
- [ ] B. Cloud Dataflow
- [ ] C. Apache Flink
- [ ] D. Cloud Dataproc

**11.** An on-premises data warehouse is currently deployed using HBase on Hadoop. You want to migrate the database to GCP. You could continue to run HBase within a Cloud Dataproc cluster, but what other option would help ensure consistent performance and support the HBase API?
- [ ] A. Store the data in Cloud Storage
- [x] B. Store the data in Cloud Bigtable
- [ ] C. Store the data in Cloud Datastore
- [ ] D. Store the data in Cloud Dataflow

**12.** The business owners of a data warehouse have determined that the current design of the data warehouse is not meeting their needs. In addition to having data about the state of systems at certain points in time, they need to know about all the times that data changed between those points in time. What kind of data warehousing pipeline should be used to meet this new requirement?
- [ ] A. ETL
- [ ] B. ELT
- [ ] C. Extraction and load
- [x] D. Change data capture

