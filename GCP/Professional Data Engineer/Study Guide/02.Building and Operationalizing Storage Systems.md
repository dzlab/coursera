# Chapter 2 Building and Operationalizing Storage Systems

## Exam Essentials
**Cloud SQL supports MySQL, PostgreSQL, and SQL Server (beta).** Cloud SQL instances are created in a single zone by default, but they can be created for high availability and use instances in multiple zones. Use read replicas to improve read performance. Importing and exporting are implemented via the RDBMS-specific tool.

**Cloud Spanner is configured as regional or multi-regional instances.** Cloud Spanner is a horizontally scalable relational database that automatically replicates data. Three types of replicas are read-write replicas, read-only replicas, and witness replicas. Avoid hotspots by not using consecutive values for primary keys.

**Cloud Bigtable is a wide-column NoSQL database used for high-volume databases that require sub-10 ms latency.** Cloud Bigtable is used for IoT, time-series, finance, and simi- lar applications. For multi-regional high availability, you can create a replicated cluster in another region. All data is replicated between clusters. Designing tables for Bigtable is fundamentally different from designing them for relational databases. Bigtable tables are denormalized, and they can have thousands of columns. There is no support for joins in Bigtable or for secondary indexes. Data is stored in Bigtable lexicographically by row-key, which is the one indexed column in a Bigtable table. Keeping related data in adjacent rows can help make reads more efficient.

**Cloud Firestore is a document database that is replacing Cloud Datastore as the managed document database.** The Cloud Firestore data model consists of entities, entity groups, properties, and keys. Entities have properties that can be atomic values, arrays, or enti- ties. Keys can be used to lookup entities and their properties. Alternatively, entities can be retrieved using queries that specify properties and values, much like using a WHERE clause in SQL. However, to query using property values, properties need to be indexed.

**BigQuery is an analytics database that uses SQL as a query language.** Datasets are the basic unit of organization for sharing data in BigQuery. A dataset can have multiple tables. BigQuery supports two dialects of SQL: legacy and standard. Standard SQL supports advanced SQL features such as correlated subqueries, ARRAY and STRUCT data types, and complex join expressions. BigQuery uses the concepts of slots for allocating computing resources to execute queries. BigQuery also supports streaming inserts, which load one row at a time. Data is generally available for analysis within a few seconds, but it may be up to 90 minutes before data is available for copy and export operations. Streaming inserts provide for best effort de-duplication. Stackdriver is used for monitoring and logging in BigQuery. Stackdriver Monitoring provides performance metrics, such query counts and time, to run queries. Stackdriver Logging is used to track events, such as running jobs or creating tables. BigQuery costs are based on the amount of data stored, the amount of data streamed, and the workload required to execute queries.

**Cloud Memorystore is a managed Redis service. Redis instances can be created using the Cloud Console or gcloud commands.** Redis instances in Cloud Memorystore can be scaled to use more or less memory. When scaling a Basic Tier instance, reads and writes are blocked. When the resizing is complete, all data is flushed from the cache. Standard Tier instances can scale while continuing to support read and write operations. When the memory used by Redis exceeds 80 percent of system memory, the instance is considered under memory pressure. To avoid memory pressure, you can scale up the instance, lower the maximum memory limit, modify the eviction policy, set time-to-live (TTL) parameters on volatile keys, or manually delete data from the instance.

**Google Cloud Storage is an object storage system.** It is designed for persisting unstruc- tured data, such as data files, images, videos, backup files, and any other data. It is unstruc- tured in the sense that objects—that is, files stored in Cloud Storage—use buckets to group objects. A bucket is a group of objects that share access controls at the bucket level. The four storage tiers are Regional, Multi-regional, Nearline, and Coldline.

**When you manage your own databases, you will be responsible for an array of database and system administration tasks.** The two Stackdriver components that are used with unmanaged databases are Stackdriver Monitoring and Stackdriver Logging. Instances have built-in monitoring and logging. Monitoring includes CPU, memory, and I/O metrics. Audit logs, which have information about who created an instance, are also available by default. Once the Stackdriver Logging agent is installed, it can collect application logs, including database logs. Stackdriver Logging is configured with Fluentd, an open source data collec- tor for logs. Once the Stackdriver Monitoring agent is installed, it can collect application performance metrics.

## Review Questions
You can find the answers in the appendix.

**1.** A database administrator (DBA) who is new to Google Cloud has asked for your help con- figuring network access to a Cloud SQL PostgreSQL database. The DBA wants to ensure that traffic is encrypted while minimizing administrative tasks, such as managing SQL cer- tificates. What would you recommend?
- [ ] A. Use the TLS protocol
- [x] B. Use Cloud SQL Proxy
- [ ] C. Use a private IP address
- [ ] D. Configure the database instance to use auto-encryption

```diff
+ B. The correct answer is B, Cloud SQL Proxy. Cloud SQL Proxy provides secure access to Second Generation instances without you having to create allow lists or configure SSL. The proxy manages authentication and automatically encrypts data.
- Option A is incorrect because TLS is the successor to SSL. It can be used to encrypt traffic, but it would require the DBA to manage certificates, so it is not as good an answer as Option B.
- Option C is incorrect; using an IP address does not ensure encryption of data.
- Option D is incorrect; there is no such thing as an auto-encryption feature in Cloud SQL.
```

**2.** You created a Cloud SQL database that uses replication to improve read performance. Occasionally, the read replica will be unavailable. You haven’t noticed a pattern, but the disruptions occur once or twice a month. No DBA operations are occurring when the inci- dents occur. What might be the cause of this issue?
- [ ] A. The read replica is being promoted to a standalone Cloud SQL instance.
- [x] B. Maintenance is occurring on the read replica.
- [ ] C. A backup is being performed on the read replica.
- [ ] D. The primary Cloud SQL instance is failing over to the read replica.

```diff
+ B. The correct answer is B. Maintenance could be occurring. Maintenance on read replicas is not restricted to the maintenance window of the primary instance or to other windows, so it can occur anytime. That would make the read replica unavailable.
- Option A is incorrect because a database administrator would have to promote a read replica, and the problem stated that there is no pattern detected and DBAs were not performing database operations.
- Option C is incorrect; backups are not performed on read replicas.
- Option D is incorrect; Cloud SQL instances do not fail over to a read replica.
```

**3.** Your department is experimenting with using Cloud Spanner for a globally accessible data- base. You are starting with a pilot project using a regional instance. You would like to fol- low Google’s recommendations for the maximum sustained CPU utilization of a regional instance. What is the maximum CPU utilization that you would target?
- [ ] A. 50%
- [x] B. 65%
- [ ] C. 75%
- [ ] D. 45%

```diff
+ B. The correct answer is B, 65%.
- Options A and C are not recommended levels for any Cloud Spanner configuration.
- Option D, 45%, is the recommend CPU utilization for a multi-regional Cloud Spanner instance.
```
**4.** A Cloud Spanner database is being deployed in us-west1 and will have to store up to 20 TB of data. What is the minimum number of nodes required?
- [x] A. 10
- [ ] B. 20
- [ ] C. 5
- [ ] D. 40

```diff
+ A. The correct answer is A. Since each node can store 2 TB, it will require at least 10 nodes.
- Options B and D are incorrect because they are more nodes than needed.
- Answer C is incorrect; five is not sufficient for storing 20 TB of data.
```

**5.** A software-as-a-service (SaaS) company specializing in automobile IoT sensors collects streaming time-series data from tens of thousands of vehicles. The vehicles are owned and operated by 40 different companies, who are the primary customers of the SaaS company. The data will be stored in Bigtable using a multitenant database; that is, all customer data will be stored in the same database. The data sent from the IoT device includes a sensor ID, which is globally unique; a timestamp; and several metrics about engine efficiency. Each cus- tomer will query their own data only. Which of the following would you use as a row-key?
- [ ] A. Customer ID, timestamp, sensor ID
- [x] B. Customer ID, sensor ID, timestamp
- [ ] C. Sensor ID, timestamp, customer ID
- [ ] D. Sensor ID, customer ID, timestamp

```diff
+ B. The correct answer is B. The database is multitenant, so each tenant, or customer, will query only its own data, so all that data should be in close proximity. Using customer ID first accomplishes this. Next, the sensor ID is globally unique, so data would be distributed evenly across database storage segments when sorting based on sensor ID. Since this is time-series data, virtually all data arriving at the same time will have timestamps around the same time. Using a timestamp early in the key could create hotspots. Using sensor ID first would avoid hotspots but would require more scans to retrieve customer data because multiple customers’ data would be stored in each data block.
```

**6.** A team of game developers is using Cloud Firestore to store player data, including character description, character state, and possessions. Descriptions are up to a 60-character alpha- numeric string that is set when the character is created and not updated. Character state includes health score, active time, and passive time. When they are updated, they are all updated at the same time. Possessions are updated whenever the character acquires or loses a possession. Possessions may be complex objects, such as bags of items, where each item may be a simple object or another complex object. Simple objects are described with a char- acter string. Complex objects have multiple properties. How would you model player data in Cloud Firestore?
- [ ] A. Store description and character state as strings and possessions as entities
- [ ] B. Store description, character state, and possessions as strings
- [ ] C. Store description, character state, and possessions as entities
- [x] D. Store description as a string; character state as an entity with properties for health score, active time, and passive time; and possessions as an entity that may have embedded entities

```diff
+ D. The correct answer is D. Description can be represented in a string. Health consists of three properties that are accessed together, so they can be grouped into an entity. Possessions need a recursive representation since a possession can include sets of other possessions.
- Options A and B are incorrect; character state requires multiple properties, and so it should not be represented in a single string.
- Option B is also incorrect, because possessions are complex objects and should not be represented in strings.
- Option C is incorrect; description is an atomic property and does not need to be modeled as an entity.
```

**7.** You are querying a Cloud Firestore collection of order entities searching for all orders that were created today and have a total sales amount of greater than $100. You have not excluded any indexes, and you have not created any additional indexes using index.yaml. What do you expect the results to be?
- [ ] A. A set of all orders created today with a total sales amount greater than $100
- [ ] B. A set of orders created today and any total sales amount
- [ ] C. A set of with total sales amount greater than $100 and any sales date
- [x] D. No entities returned

```diff
+ D. The correct answer is D—no entities are returned. The query requires a composite index, but the question stated that no additional indexes were created. All other answers are wrong because querying by property other than a key will only return entities found in an index.
```

**8.** You are running a Redis cache using Cloud Memorystore. One day, you receive an alert notification that the memory usage is exceeding 80 percent. You do not want to scale up the instance, but you need to reduce the amount of memory used. What could you try?
- [x] A. Setting shorter TTLs and trying a different eviction policy.
- [ ] B. Switching from Basic Tier to Standard Tier.
- [ ] C. Exporting the cache.
- [ ] D. There is no other option—you must scale the instance.

```diff
+ A. The correct answer is A. Setting shorter TTLs will make keys eligible for eviction sooner, and a different eviction policy may lead to more evictions. For example, switching from an eviction policy that evicts only keys with a TTL to a policy that can evict any key may reduce memory use.
```

**9.** A team of machine learning engineers are creating a repository of data for training and testing machine learning models. All of the engineers work in the same city, and they all contribute datasets to the repository. The data files will be accessed frequently, usually at least once a week. The data scientists want to minimize their storage costs. They plan to use Cloud Storage; what storage class would you recommend?
- [x] A. Regional
- [ ] B. Multi-regional
- [ ] C. Nearline
- [ ] D. Coldline

```diff
+ A. The correct answer is A. Regional storage is sufficient for serving users in the same geographic location and costs less than multi-regional storage.
- Option B is incorrect because it does not minimize cost, and there is no need for multi-regional storage.
- Options C and D are incorrect because Nearline and Coldline are less expensive only for infrequently accessed data.
```

**10.** Auditors have informed your company CFO that to comply with a new regulation, your company will need to ensure that financial reporting data is kept for at least three years. The CFO asks for your advice on how to comply with the regulation with the least adminis- trative overhead. What would you recommend?
- [ ] A. Store the data on Coldline storage
- [ ] B. Store the data on multi-regional storage
- [x] C. Define a data retention policy
- [ ] D. Define a lifecycle policy

```diff
+ C. The correct answer is C. A data retention policy will ensure that files are not deleted from a storage bucket until they reach a specified age.
- Options A and B are incorrect because files can be deleted from Coldline or multi-regional data unless a data retention policy is in place.
- Option D is incorrect because a lifecycle policy will change the storage type on an object but not prevent it from being deleted.
```

**11.** As a database administrator tasked with migrating a MongoDB instance to Google Cloud, you are concerned about your ability to configure the database optimally. You want to col- lect metrics at both the instance level and the database server level. What would you do in addition to creating an instance and installing and configuring MongoDB to ensure that you can monitor key instances and database metrics?
- [ ] A. Install Stackdriver Logging agent.
- [x] B. Install Stackdriver Monitoring agent.
- [ ] C. Install Stackdriver Debug agent.
- [ ] D. Nothing. By default, the database instance will send metrics to Stackdriver.

```diff
+ B. The correct answer is B, installing the Stackdriver Monitoring agent. This will collect application-level metrics and send them to Stackdriver for alerting and charting.
- Option A is incorrect because Stackdriver Logging does not collect metrics, but you would install the Stackdriver Logging agent if you also wanted to collect database logs.
- Option C is incorrect; Stackdriver Debug is for analyzing a running program.
- Option D is incorrect; by default, you will get only instance metrics and audit logs.
```

**12.** A group of data scientists have uploaded multiple time-series datasets to BigQuery over the last year. They have noticed that their queries—which select up to six columns, apply four SQL functions, and group by the day of a timestamp—are taking longer to run and are incurring higher BigQuery costs as they add data. They do not understand why this is the case since they typically work only with the most recent set of data loaded. What would you recommend they consider in order to reduce query latency and query costs?
- [ ] A. Sort the data by time order before loading
- [ ] B. Stop using Legacy SQL and use Standard SQL dialect
- [x] C. Partition the table and use clustering
- [ ] D. Add more columns to the SELECT statement to use data fetched by BigQuery more effi- ciently

```diff
+ C. The correct answer is C. The queries are likely scanning more data than needed. Partitioning the table will enable BigQuery to scan only data within a partition, and clustering will improve the way column data is stored.
- Option A is incorrect because BigQuery organizes data according to table configuration parameters, and there is no indication that queries need to order results. 
- Option B is incorrect; Standard SQL dialect has more SQL features but none of those are used. Also, it is unlikely that the query execution plan would be more efficient with Standard SQL.
- Option D is incorrect; it would actually require more data to be scanned and fetched because BigQuery uses a columnar data storage model.
```

**13.** You are querying a BigQuery table that has been partitioned by time. You create a query and use the --dry_run flag with the bq query command. The amount of data scanned is far more than you expected. What is a possible cause of this?
- [x] A. You did not include _PARTITIONTIME in the WHERE clause to limit the amount of data that needs to be scanned.
- [ ] B. You used CSV instead of AVRO file format when loading the data.
- [ ] C. Both active and long-term data are included in the query results.
- [ ] D. You used JSON instead of the Parquet file format when loading the data.

```diff
+ A. The correct answer is A. You probably did not include the pseudo-column _PARTITIONTIME in the WHERE clause to limit the amount of data scanned.
- Options B and D are incorrect; the format of the file from which data is loaded does not affect the amount of data scanned.
- Option C is incorrect; the distinction between active and long-term data impacts only the cost of storage, not the execution of a query.
```

**14.** Your department is planning to expand the use of BigQuery. The CFO has asked you to investigate whether the company should invest in flat-rate billing for BigQuery. What tools and data would you use to help answer that question?
- [ ] A. Stackdriver Logging and audit log data
- [ ] B. Stackdriver Logging and CPU utilization metrics
- [ ] C. Stackdriver Monitoring and CPU utilization metrics
- [x] D. Stackdriver Monitoring and slot utilization metrics

```diff
+ D. The correct answer is D. Stackdriver Monitoring collects metrics, and the slot metrics are the ones that show resource utilization related to queries.
- Options A and B are incorrect; logging does not collect the metrics that are needed.
- Option C is incorrect because CPU utilization is not a metric associated with a serverless service like BigQuery.
```

**15.** You are migrating several terabytes of historical sensor data to Google Cloud Storage. The data is organized into files with one file per sensor per day. The files are named with
the date followed by the sensor ID. After loading 10 percent of the data, you realize that the data loads are not proceeding as fast as expected. What might be the cause?
- [x] A. The filenaming convention uses dates as the first part of the file name. If the files are loaded in this order, they may be creating hotspots when writing the data to Cloud Storage.
- [ ] B. The data is in text instead of Avro or Parquet format.
- [ ] C. You are using a gcloud command-line utility instead of the REST API.
- [ ] D. The data is being written to regional instead of multi-regional storage.

```diff
+ A. The correct answer is A. Since the files have sequential names, they may be loading in lexicographic order, and this can create hotspots.
- Option B is incorrect; the volume of data, not the format, will determine upload speed.
- Option C is incorrect; there should be no noticeable difference between the command-line SDK and the REST API.
- Option D is incorrect; writing to multi-regional storage would not make the uploads faster.
```
