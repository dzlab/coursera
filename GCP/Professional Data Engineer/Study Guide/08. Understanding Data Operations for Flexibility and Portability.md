# Chapter 8: Understanding Data Operations for Flexibility and Portability

## Exam Essentials
**Know that Data Catalog is a metadata service for data management.** Data Catalog is fully managed, so there are no servers to provision or configure. Its primary function is to provide a single, consolidated view of enterprise data. Metadata is collected automatically during ingest operations to BigQuery and Cloud Pub/Sub, as well through APIs and third- party tools.

**Understand that Data Catalog will collect metadata automatically from several GCP sources.** These sources include Cloud Storage, Cloud Bigtable, Google Sheets, BigQuery, and Cloud Pub/Sub. In addition to native metadata, Data Catalog can collect custom metadata through the use of tags.

**Know that Cloud Dataprep is an interactive tool for preparing data for analysis and machine learning.** Cloud Dataprep is used to cleanse, enrich, import, export, discover, structure, and validate data. The main cleansing operations in Cloud Dataprep center around altering column names, reformatting strings, and working with numeric values. Cloud Dataprep supports this process by providing for filtering data, locating outliers, deriving aggregates, calculating values across columns, and comparing strings.

**Be familiar with Data Studio as a reporting and visualization tool.** The Data Studio tool is organized around reports, and it reads data from data sources and formats the data into tables and charts. Data Studio uses the concept of a connector for working with datasets. Datasets can come in a variety of forms, including a relational database table, a Google Sheet, or a BigQuery table. Connectors provide access to all or to a subset of columns in a data source. Data Studio provides components that can be deployed in a drag-and-drop manner to create reports. Reports are collections of tables and visualization.

**Understand that Cloud Datalab is an interactive tool for exploring and transforming data.** Cloud Datalab runs as an instance of a container.** Users of Cloud Datalab create a Compute Engine instance, run the container, and then connect from a browser to a Cloud Datalab notebook, which is a Jupyter Notebook. Many of the commonly used packages are available in Cloud Datalab, but when users need to add others, they can do so by using either the conda install command or the pip install command.

**Know that Cloud Composer is a fully managed workflow orchestration service based on Apache Airflow.** Workflows are defined as directed acyclic graphs, which are specified in Python. Elements of workflows can run on premises and in other clouds as well as in GCP. Airflow DAGs are defined in Python as a set of operators and operator relationships. An operator specifies a single task in a workflow. Common operators include BashOperator and PythonOperator.

## Review Questions
You can find the answers in the appendix.

**1.** Analysts and data scientists at your company ask for your help with data preparation. They currently spend significant amounts of time searching for data and trying to understand the exact definition of the data. What GCP service would you recommend that they use?
- [ ] A. Cloud Composer
- [x] B. Data Catalog
- [ ] C. Cloud Dataprep
- [ ] D. Data Studio

**2.** Machine learning engineers have been working for several weeks on building a recommen- dation system for your companyâ€™s e-commerce platform. The model has passed testing and validation, and it is ready to be deployed. The model will need to be updated every day with the latest data. The engineers want to automate the model building process that includes running several Bash scripts, querying databases, and running some custom Python code. What GCP service would you recommend that they use?
- [x] A. Cloud Composer
- [ ] B. Data Catalog
- [ ] C. Cloud Dataprep
- [ ] D. Data Studio

**3.** A business intelligence analyst has just acquired several new datasets. They are unfamiliar with the data and are especially interested in understanding the distribution of data in each column as well as the extent of missing or misconfigured data. What GCP service would you recommend they use?
- [ ] A. Cloud Composer
- [ ] B. Cloud Catalog
- [x] C. Cloud Dataprep
- [ ] D. Data Studio

**4.** Line-of-business managers have asked your team for additional reports from data in a data warehouse. They want to have a single report that can act as a dashboard that shows key metrics using tabular data as well as charts. What GCP service would you recommend?
- [ ] A. Cloud Composer
- [ ] B. Data Catalog
- [ ] C. Cloud Dataprep
- [x] D. Data Studio

**5.** You are using Cloud Dataprep to prepare datasets for machine learning. Another team will be using the data that you prepare, and they have asked you to export your data from Cloud Dataprep. The other team is concerned about file size and asks you to compress the files using GZIP. What formats can you use in the export file?
- [ ] A. CSV only
- [x] B. CSV and JSON only
- [ ] C. CSV and AVRO only
- [ ] D. JSON and AVRO only

**6.** The finance department in your company is using Data Studio for data warehouse reporting. Their existing reports have all the information they need, but the time required to update charts and tables is longer than expected. What kind of data source would you try to improve the query performance?
- [ ] A. Live data source
- [ ] B. Extracted data source
- [ ] C. Compound data source
- [x] D. Blended data source

**7.** A DevOps team in your company uses Data Studio to display application performance data. Their top priority is timely data. What kind of connection would you recommend they use to have data updated in reports automatically?
- [x] A. Live data source
- [ ] B. Extracted data source
- [ ] C. Compound or blended data source
- [ ] D. Extracted or live data source

**8.** A machine learning engineer is using Data Studio to build models in Python. The engineer has decided to use a statistics library that is not installed by default. How would you suggest that they install the missing library?
- [ ] A. Using conda install or pip install from a Cloud shell
- [x] B. Using conda install or pip install from within a Jupyter Notebook
- [ ] C. Use the Linux package manager from within a Cloud shell
- [ ] D. Download the source from GitHub and compile locally

**9.** A DevOps engineer is working with you to build a workflow to load data from an on-premises database to Cloud Storage and then run several data preprocessing and analysis programs. After those are run, the output is loaded into a BigQuery table, an email is sent to managers indicating that new data is available in BigQuery, and temporary files are deleted. What GCP service would you use to implement this workflow?
- [ ] A. Cloud Dataprep
- [ ] B. Cloud Dataproc
- [x] C. Cloud Composer
- [ ] D. Data Studio

**10.** You have just received a large dataset. You have comprehensive documentation on the dataset and are ready to start analyzing. You will do some visualization and data filtering, but you also want to be able to run custom Python functions. You want to work interactively with the data. What GCP service would you use?
- [ ] A. Cloud Dataproc
- [x] B. Cloud Datalab
- [ ] C. Cloud Composer
- [ ] D. Data Studio

