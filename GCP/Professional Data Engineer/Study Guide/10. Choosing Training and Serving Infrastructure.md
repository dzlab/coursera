# Chapter 10 Choosing Training and Serving Infrastructure

## Edge Computing with GCP

### Cloud IoT
Cloud IoT is Google’s managed service for IoT services. The platform provides services for integrating edge computing with centralized processing services. Device data is captured by the Cloud IoT Core service, which can then publish the data to Cloud Pub/Sub for stream- ing analytics. Data can also be stored in BigQuery for analysis or used for training new machine learning models in Cloud ML. Data provided through Cloud IoT can also be used to trigger Cloud Functions and associated workflows.
Figure 10.3 shows the reference architecture for Cloud IoT.
The reference architecture for Cloud IoT includes the following:
- Cloud IoT Core for device management
- Cloud ML Engine for training and deploying machine learning models Cloud Dataflow for streaming and batch analytics
- Cloud Pub/Sub for ingestion
- BigQuery for data warehousing and ad hoc querying
- Other GCP services can be used as well, including Cloud Storage for archival storage and Cloud Firestore for storing semi-structured data that will be queried.

|Build & train ML models in the cloud|
|-|
|![image](https://user-images.githubusercontent.com/1645304/138612829-a08c8582-584f-4834-aca4-8102eb63b570.png)|

Source: https://cloud.google.com/solutions/iot/

## Exam Essentials
**Understand that single machines are useful for training small models.**
This includes when you are developing machine learning applications or exploring data using Jupyter
Notebooks or related tools. Cloud Datalab, for example, runs instances in Compute Engine virtual machines.

**Know that you also have the option of offloading some of the training load from CPUs to GPUs.**
GPUs have high-bandwidth memory and typically outperform CPUs on floating-point operations. GCP uses NVIDIA GPUs, and NVIDIA is the creator of CUDA,
a parallel computing platform that facilitates the use of GPUs.

**Know that distributing model training over a group of servers provides for scalability and improved availability.**
There are a variety of ways to use distributed infrastructure, and the best choice for you will depend on your specific requirements and development practices. One way to distribute training is to use machine learning frameworks that are designed to run in a distributed environment, such as TensorFlow.

**Understand that serving a machine learning model is the process of making the model available to make predictions for other services.**
When serving models, you need to consider latency, scalability, and version management. Serving models from a centralized location, such as a data center, can introduce latency because input data and results are sent over the network. If an application needs real-time results, it is better to serve the model closer to where it is needed, such as an edge or IoT device.

**Know that edge computing is the practice of moving compute and storage resources closer to the location at which they are needed.**
Edge computing devices can be relatively simple IoT devices, such as sensors with a small amount of memory and limited processing power. This type of device could be useful when the data processing load is light. Edge computing is used when low-latency data processing is needed—for example, to control machinery such as autonomous vehicles or manufacturing equipment. To enable edge computing, the system architecture has to be designed to provide compute, storage, and networking capa- bilities at the edge while services run in the cloud or in an on-premises data center for the centralized management of devices and centrally stored data.

**Be able to list the three basic components of edge computing.**
Edge computing consists of edge devices, gateway devices, and the cloud platform. Edge devices provide three kinds of data: metadata about the device, state information about the device, and telemetry data. Before a device is incorporated into an IoT processing system, it must be provisioned. After a device is provisioned and it starts collecting data, the data is then processed on the device. After local processing, data is transmitted to a gateway. Gateways can manage network traffic across protocols. Data sent to the cloud is ingested by one of a few different kinds of services in GCP, including Cloud Pub/Sub, IoT Core MQTT, and Stackdriver Monitoring and Logging.


**Know that an Edge TPU is a hardware device available from Google for implementing edge computing.**
This device is an application-specific integrated circuit (ASIC) designed for running AI services at the edge. Edge TPU is designed to work with Cloud TPU and Google Cloud services. In addition to the hardware, Edge TPU includes software and AI algorithms.


**Know that Cloud IoT is Google’s managed service for IoT services.**
This platform provides services for integrating edge computing with centralized processing services. Device data is captured by the Cloud IoT Core service, which can then publish data to Cloud Pub/Sub for streaming analytics. Data can also be stored in BigQuery for analysis or used for training new machine learning models in Cloud ML. Data provided through Cloud IoT can also be used to trigger Cloud Functions and associated workflows.

**Understand GPUs and TPUs.**
Graphic processing units are accelerators that have multiple arithmetic logic units (ALUs) that implement adders and multipliers. This architecture is well suited to workloads that benefit from massive parallelization, such as training deep learning models. GPUs and CPUs are both subject to the von Neumann bottleneck, which is the limited data rate between a processor and memory, and slow processing. TPUs are specialized accelerators based on ASICs and created by Google to improve training of deep neural networks. These accelerators are designed for the TensorFlow framework. TPUs reduces the impact of the von Neumann bottleneck by implementing matrix multiplication in the processor. Know the criteria for choosing between CPUs, GPUs, and TPUs.


## Review Questions
1. You are in the early stages of developing a machine learning model using a framework that requires high-precision arithmetic and benefits from massive parallelization. Your data
set fits within 32 GB of memory. You want to use Jupyter Notebooks to build the model iteratively and analyze results. What kind of infrastructure would you use?
- [ ] A. A TPU pod with at least 32 TPUs
- [ ] B. A single TPU
- [x] C. A single server with a GPU
- [ ] D. A managed instance group of 4 VMs with 64 GB of memory each

```diff
+ C. The correct answer is C. The requirements call for high-precision arithmetic and parallelization, so that indicates using a GPU. There is a small amount of data, and you want to work with it interactively, so a single machine with a GPU will suffice.
- Options A and B are incorrect because TPUs do not support high-precision arithmetic. Also, Option A requires more resources than needed for a small dataset. 
- Option D is incorrect because this is an interactive workload, so there is no need for the high availability provided by a managed instance group and there are more resources allocated than needed for this workload.
```

2. You are developing a machine learning model that will predict failures in high-precision machining equipment. The equipment has hundreds of IoT sensors that send telemetry data every second. Thousands of the machines are in use in a variety of operating conditions.
A year’s worth of data is available for model training. You plan to use TensorFlow, a synchronous training strategy, and TPUs. Which of the following strategies would you use?
- [ ] A. MirroredStrategy
- [ ] B. CentralStorageStrategy
- [ ] C. MultiWorkerMirroredStrategy
- [x] D. TPUStrategy

```diff
+ D. The correct answer is D. The TPU strategy meets all of the requirements of synchronous training on TPUs.
- The other strategies all apply to GPUs and/or CPUs and therefore do not meet the requirements.
```

3. Your client has developed a machine learning model that detects anomalies in equity trading time-series data. The model runs as a service in a Google Kubernetes Engine (GKE) cluster deployed in the us-west-1 region. A number of financial institutions in New York and London are interested in licensing the technology, but they are concerned that the total time required to make a prediction is longer than they can tolerate. The distance between the serving infrastructure and New York is about 4,800 kilometers, and the distance to London is about 8,000 kilometers. This is an example of what kind of problem with serving a machine learning model?
- [ ] A. Overfitting
- [ ] B. Underfitting
- [x] C. Latency
- [ ] D. Scalability

```diff
+ C. The correct answer is C. This is an example of a latency problem that might be resolved by serving the model closer to where the data is generated.
- Options A and B are incorrect because overfitting and underfitting are problems with model training not serving.
- Option D is incorrect; there is no indication that the volume of data processed is a problem.
```

4. A study of global climate change is building a network of environmental sensors distributed across the globe. Sensors are deployed in groups of 12 sensors and a gateway. An analytics pipeline is implemented in GCP. Data will be ingested by Cloud Pub/Sub and analyzed using the stream processing capabilities of Cloud Dataflow. The analyzed data will be stored in BigQuery for further analysis by scientists. The bandwidth between the gateways and the GCP is limited and sometimes unreliable. The scientists have determined that they need the average temperature, pressure, and humidity measurements of each group of 12 sensors for a one-minute period. Each sensor sends data to the gateway every second. This generates 720 data points (12 sensors × 60 seconds) every minute for each of the three measurements. The scientists only need the one-minute average for temperature, pressure, and humidity. What data processing strategy would you implement?
- [ ] A. Send all 720 data points for each measurement each minute to a Cloud Pub/Sub message, generate the averages using Cloud Dataflow, and write those results to BigQuery.
- [x] B. Average all 720 data points for each measurement each minute, send the average to a Cloud Pub/Sub message, and use Cloud Dataflow and write those results to BigQuery.
- [ ] C. Send all 720 data points for each measurement each minute to a BigQuery streaming insert into a partitioned table.
- [ ] D. Average all 720 data points for each measurement each minute, send the average to a Cloud Pub/Sub message, and use Cloud Dataflow and write those results to BigQuery.

```diff
+ B. The correct answer is B. The raw data does not need to be stored and with limited bandwidth; it is best to minimize the amount of data transmitted, so sending just the average is correct. Also, because the network connection is sometimes unreliable, you should use Cloud Dataflow to implement stream processing logic, such as handling late- arriving data and inserting default values for missing data.
- Option A is incorrect because it sends too much data.
- Option C is incorrect because it sends too much data and stores it directly in BigQuery without preprocessing for missing values and other business logic.
- Option D is incorrect because it stores data directly in BigQuery without preprocessing for missing values and other business logic.
```

5. Your DevOps team is deploying an IoT system to monitor and control environmental conditions in your building. You are using a standard IoT architecture. Which of the following components would you not use?
- [ ] A. Edge devices
- [ ] B. Gateways
- [x] C. Repeater
- [ ] D. Cloud platform services

```diff
+ C. The correct answer is C. Repeaters are used in networks to boost signal strength. There is no indication that this is needed, and in any case, that is a network implementation choice and not a comparable part of the IoT architecture of the other components.
- Options A, B, and D are all part of the standard IoT architecture.
```

6. In the Google Cloud Platform IoT reference model, which of the following GCP services is used for ingestion?
- [ ] A. Cloud Storage
- [ ] B. BigQuery streaming inserts
- [x] C. Cloud Pub/Sub
- [ ] D. Cloud Bigtable

```diff
+ C. The correct answer is C. Cloud Pub/Sub is a globally scalable messaging queue that can ingest large volumes of data and buffer it for other services.
- Option A is incorrect; Cloud Storage is not used for streaming high-volume data into GCP; it is used for batch uploads.
- Option B is incorrect; BigQuery streaming inserts are not used for ingestion in the reference model.
- Option D is incorrect; Cloud Bigtable is not used for ingestion.
```

7. A startup is developing a product for autonomous vehicle manufacturers that will enable its vehicles to detect objects better in adverse weather conditions. The product uses a machine learning model built on TensorFlow. Which of the following options would you choose to serve this model?
- [ ] A. On GKE using TensorFlow Training (TFJob)
- [ ] B. On Compute Engine using managed instance groups
- [x] C. On Edge TPU devices in the vehicles
- [ ] D. On GPUs in the vehicles

```diff
+ C. The correct answer is C. Edge TPU is designed for inferencing on edge devices. Since the model is used to help autonomous vehicles improve their ability to track objects in adverse weather conditions, low latency is essential.
- Options A and B are incorrect because they serve the model from a central service rather than at the edge.
- Option D is incorrect; GPUs are used when training models, not when using them for inference.
```

8. In the Google Cloud Platform IoT reference model, which of the following GCP services is used for stream processing?
- [ ] A. Cloud Storage
- [ ] B. BigQuery streaming inserts
- [ ] C. Cloud Pub/Sub
- [x] D. Cloud Dataflow

```diff
+ D. The correct answer is D. Cloud Dataflow is a stream and batch processing service based on Apache Beam.
- Option A is incorrect; Cloud Storage is not used for stream processing— it is an object storage service.
- Option B is incorrect; BigQuery streaming inserts are for storing data in BigQuery partitioned tables.
- Option C is incorrect; Cloud Pub/Sub is used for ingestion, not stream processing.
```

9. You have developed a TensorFlow model using only the most basic TensorFlow operations and no custom operations. You have a large volume of data available for training, but by your estimates it could take several weeks to train the model using a 16 vCPU Compute Engine instance. Which of the following should you try instead?
- [ ] A. A 32 vCPU Compute Engine instance
- [x] B. A TPU pod
- [ ] C. A GKE cluster using on CPUs
- [ ] D. App Engine Second Generation

```diff
+ B. The correct answer is B. This is a typical use case for TPUs because the model is built on TensorFlow using only basic operations and no custom operations, so TPUs are an option. The long training time on CPUs indicate that this is a good option for TPUs.
- Option A is incorrect; this would only cut the training time in half, assuming a linear speedup.
- Option C is incorrect because only CPUs are used and not TPUs.
- Option D is incorrect; App Engine is used for scalable web applications and not used for training models.
```

10. You have developed a machine learning model that uses a specialized Fortran library that
is optimized for highly parallel, high-precision arithmetic. You only have access to the compiled code and cannot make any changes to source code. You want to use an accelerator to reduce the training time of your model. Which of the following options would you try first?
- [x] A. A Compute Engine instance with GPUs
- [ ] B. A TPU pod
- [ ] C. A Compute Engine instance with CPUs only
- [ ] D. Cloud Functions

```diff
+ A. The correct answer is A. A Fortran library optimized for highly parallel, high-precision arithmetic that runs on GPUs would be a good option for training this model.
- Option B is incorrect; TPUs do not support high-precision arithmetic.
- Option C is not the best choice because it would not be as performant as a GPU-based solution, but it could be a backup option in case the Fortran library did not function on the GPU.
- Option D is incorrect; Cloud Functions do not run Fortran code and are not suitable to running workloads such as this.
```
