# Chapter 5 Building and Operationalizing Processing Infrastructure

## Provisioning and Adjusting Processing Resources

### Provisioning and Adjusting Compute Engine
Compute Engine supports provisioning single instances or groups of instances, known as instance groups. Instance groups are either:
- Managed instance groups (MIGs) consist of identically configured VMs;
- unmanaged instance groups allow for heterogeneous VMs

#### Provisioning Single VM Instances
range of parameters, including the following:

- Machine type, which specifies the number of vCPUs and the amount of memory
- Region and zone to create the VM
- Boot disk parameters
- Network configuration details
- Disk image
- Service account for the VM Metadata and tags

Also,  Shielded VMs for additional secu- rity or GPUs and tensor processing units (TPUs) for additional processing resources.

```sh
gcloud compute instances create instance-1 --zone=us-central1-a --machine-type=n1-standard-1 --subnet=default --network-tier=PREMIUM --image=debian-9-stretch-v20191115 --image-project=debian-cloud --boot-disk-size=10GB --boot-disk-type=pd-standard
```

#### Provisioning Managed Instance Groups
Managed instance groups are managed as a single logical resource. MIGs have several use- ful properties, including:
- Autohealing based on application-specific health checks, which replaces nonfunction- ing instances
- Support for multizone groups that provide for availability in spite of zone-level failures
- Load balancing to distribute workload across all instances in the group
- Autoscaling, which adds or removes instances in the group to accommodate increases and decreases in workloads
- Automatic, incremental updates to reduce disruptions to workload processing

The specifications for a MIG are defined in an instance template, which is a file with VM specifications. Example:
```sh
gcloud compute instance-templates create pde-exam-template-custom \
--machine-type n1-standard-4 \
--image-family debian-9 \
--image-project debian-cloud \
--boot-disk-size 250GB
```
To create an instance from this template
```sh
gcloud compute instances create pde-exam-instance --source-instance-template=pde-exam-template-custom
```

When creating an instance group, you can specify:
- the name of the instance template
- a target CPU utilization that will trigger adding instances to the group up to the maximum number of instances allowed.
- a minimum number of instances.
- the default cooldown period, which is the amount of time GCP will wait before collecting performance statistics from the instance. This gives the instance time to complete startup before its performance is considered for adjusting
the instance group.

#### Adjusting Compute Engine Resources to Meet Demand
One of the advantages of a managed instance group is that the number of VMs in the group can change according to workload. This type of horizontal scaling is readily imple- mented in MIGs because of autoscaling.

An alternative, vertical scaling, requires moving services from one VM to another VM with more or fewer resources.

Many data engineer- ing use cases have workloads that can be distributed across VMs so that horizontal scaling with MIGs is a good option.

If you have a high-performance computing (HPC) workload that must be run on a single server—for example, a monolithic simulation—then you may need to scale VMs vertically.

### Provisioning and Adjusting Kubernetes Engine

#### Provisioning a Kubernetes Engine Cluster
To create a Kubernetes cluster from the command line
```sh
gcloud container clusters create "standard-cluster-1" --zone "us-central1-a" --cluster-version "1.13.11-gke.14" --machine-type "n1-standard-1"
--image-type "COS" --disk-type "pd-standard" --disk-size "100" --num-nodes "5" --enable-autoupgrade --enable-autorepair
```

If a node has sufficient available resources, the scheduler can run the job on that node. You can control where jobs are run using the Kubernetes abstractions known as taints. By assigning taints to node pools and tolerances for taints to pods, you can control when pods are run.

#### Adjusting Kubernetes Engine Resources to Meet Demand
Adjust resources is by scaling applications running in clusters and scaling clusters themselves.

adjust the number of replicas for a deployment named pde-example-application:
```sh
kubectl scale deployment pde-example-application --replicas 6
```

Alternatively, you can configure deployments to autoscale using the kubectl autoscale command. For example, consider the following:
```sh
kubectl autoscale deployment pde-example-application --min 2 --max 8 --cpu-percent 65
```

This command will autoscale the number of replicas of the pde-example-application between two and eight replicas depending on CPU utilization. In this case, if CPU utilization across all pods is greater than 65 percent, replicas will be added up to the maximum number specified.

#### Autoscaling Clusters in Kubernetes Engine
In Kubernetes nodes are implemented as Compute Engine VMs, and node pools are imple- mented using managed instance groups.

Cluster autoscaling is done at the node pool level. When you create a node pool, you can specify a minimum and maximum number of nodes in the pool. The autoscaler adjusts the number of nodes in the pool based on resource requests, not actual utilization. If pods are unscheduled because there is not a sufficient number of nodes in the node pool to run those pods, then more nodes will be added up to the maximum number of nodes specified for that node pool. If nodes are underutilized, the autoscaler will remove nodes from the node pool down to the minimum number of nodes in the pool.

You can specify autoscaling parameters when creating a cluster. Here is an example that creates a cluster called pde-example-cluster in the us-central1-a zone, with nodes in three zones and a starting set of two nodes per node pool. Autoscaling is enabled with a minimum of one node and a maximum of four nodes.
```sh
gcloud container clusters create pde-example-cluster \
--zone us-central1-a \
--node-locations us-central1-a,us-central1-b,us-central1-f \
--num-nodes 2 --enable-autoscaling --min-nodes 1 --max-nodes 4
```

### Kubernetes YAML Configurations
Example application autoscaling based on CPU
```yaml
apiVersion: "autoscaling/v2beta1" kind: "HorizontalPodAutoscaler" metadata:
name: "nginx-1-hpa" namespace: "default" labels:
app: "nginx-1" spec:
scaleTargetRef:
  kind: "Deployment"
  name: "nginx-1" 
  apiVersion: "apps/v1"
minReplicas: 1
maxReplicas: 5 metrics:
- type: "Resource"
  resource:
    name: "cpu" 
    targetAverageUtilization: 80
```
One of the advantages of using a managed Kubernetes service like GKE is that configu- ration files like this are generated for you when using command-line and cloud console commands.

## Provisioning and Adjusting Cloud Bigtable
Cloud Bigtable is a managed wide-column NoSQL database used for applications that require high-volume, low-latency random reads and writes, such as IoT applications, and for analytic use cases, such as storing large volumes of data used to train machine learning models. Bigtable has an HBase interface, so it is also a good alternative to using Hadoop HBase on a Hadoop cluster.

### Provisioning Bigtable Instances
Options for configuring Bigtable
- The instance type can be either production or development. Production instances have clusters with a minimum of three nodes; development instances have a single node and do not provide for high availability.
- Storage types are SSD and HDD. SSDs are used when low-latency I/O is a priority. HDD instances have higher read latency.

Create Bitable with `gcloud` cli
```sh
gcloud bigtable instances create pde-bt-instance1 \
--cluster=pde-bt-cluster1 \
--cluster-zone=us-west1-a \
--display-name=pdc-bt-instance-1 \
--cluster-num-nodes=6 \
--cluster-storage-type=SSD \
--instance-type=PRODUCTION
```

Bigtable has a command-line utility called `cbt`, which can also be used to create instances along with other operations on Bigtable instances. Previous cluster can be created with:
```sh
cbt createinstance pde-bt-instance1 pdc-bt-instance-1 pde-bt-cluster1 west1-a 6 SSD
```

Once created you can modify te following
- The number of nodes in each cluster
- The number of clusters
- Application profiles, which contain replication settings
- Labels, which are used to specify metadata attributes
- The display name

### Replication in Bigtable

## Provisioning and Adjusting Cloud Dataproc
When creating a Cloud Dataproc cluster, the cluster mode determines the number of master nodes and possible worker nodes.
- The standard mode has one master and some number of workers. 
- The single mode has one master and no workers.
- The high availability mode has three master nodes and some number of worker nodes.

Master nodes and worker nodes are configured separately. For each type of node, you can specify a machine type, disk size, and disk type. For worker nodes, you can specify machine type, disk size and type, a minimum number of nodes, and optional local SSDs.

```sh
gcloud dataproc clusters create pde-cluster-1 \
--region us-central1 \
--zone us-central1-b \ 
--master-machine-type n1-standard-1 \
--master-boot-disk-size 500 \
--num-workers 4
--worker-machine-type n1-standard-1 --worker-boot-disk-size 500
```
The number of worker nodes can also be adjusted automatically by specifying an autoscal- ing policy, which specifies the maximum number of nodes and a scale-up rate and a scale- down rate.

## Configuring Cloud Dataflow
Required parameters are as follows:
- Job name
- Project ID
- Runner, which is DataflowRunner for cloud execution
- Staging locations, which is a path to a Cloud Storage location for code packages
- Temporary location for temporary job files
Other options:
- number of workers to use by default when executing a pipeline as well as a maximum number of workers to use in cases where the workload would benefit from additional workers.
- disk size to use with Compute Engine worker instances. This may be important when running batch jobs that may require large amounts of space on the boot disk.

Cloud Dataflow does not require you to specify machine types, but you can specify machine type and worker disk type if you want that level of control.

## Configuring Managed Serverless Processing Services

### Configuring App Engine
configure your service/application via:
- app.yaml: three required parameters:
  - runtime: specifies the runtime environment, such as Python 3.
  - handlers: is a set of URL patterns that specify what code is run in response to invoking a URL.
  - threadsafe: 
- cron.yaml: configure scheduled tasks for an application via when to run the task and a URL to be invoked when the task is run.
- dispatch.yaml: specifying routing rules to send incoming requests to a specific service based on the URL.

### Configuring Cloud Functions
several parameters, including:
- memory: ranges from 128 MB to 2 GB
- timeout: the maximum time that the function is allowed to run before completing or it will be terminated. The default is 1 minute, but it can be set to as high as 9 minutes.
- region: geographical region to execute the function
- max-instances number of Cloud Function instances that will exist at any one time

## Monitoring Processing Resources

### Stackdriver Monitoring
Stackdriver Monitoring collects more than 1,000 metric on the performance of infrastructure resources and applications. Resources can be in GCP as well as the AWS cloud.
- App Engine has metrics about the utilization of all VMs serving an application, the number of current active connections, and the total number of reserved cores.
- Compute Engine Autoscalers have metrics related to capacity and current utilization. BigQuery has metrics about query counts, execution times, scanned bytes, and table count.
- Cloud Bigtable collects data on CPU load, disk load, node count, bytes used, and stor- age capacity.
- Cloud Functions collects data on active instances, execution count, and execution times.

Alerts can be created using an alerting policy which include conditions, such as:
- CPU utilization exceeding 70 percent for three minutes,
- a notification channel, and
- optional documentation for those responding to the alert.

### Stackdriver Logging
used for storing and searching log data from more than 150 common applications about events in infrastructure and applications.
A log entry is a record about some event. These records may be created by an application, an operating system, middleware, or other service. A log is a collection of log entries.

Logs are maintained in Stackdriver for a specific period of time known as the **retention period** which varies of log type:
- Admin activity audit logs, sys- tem event audit logs, and access transparency logs are kept for 400 days
- Data access audit logs and other logs not related to auditing are kept 30 days.

To keep them longer export them before the end of the retention period ti a destination or sink, which could be:
- Cloud Storage,
- BigQuery, 
- Cloud Pub/Sub

Stackdriver Logging interface uses query language that supports pattern matching and Boolean expressions to search logs.

### Stackdriver Trace
a distributed tracing system designed available in Compute Engine, Kubernetes Engine, and App Engine.

it has two main components:
- a tracing client that collects data and sends it to GCP.
- a tracing interface is used to view and analyze that trace data:
  - Performance insights
  - Recent traces and times
  - Most frequent URIs
  - Daily analysis reports

## Exam Essentials
**Know that Compute Engine supports provisioning single instances or groups of instances, known as instance groups**. Instance groups are either managed or unmanaged instance groups. Managed instance groups (MIGs) consist of identically configured VMs; unman- aged instance groups allow for heterogeneous VMs, but they should be used only when migrating legacy clusters from on-premises data centers.

**Understand the benefits of MIGs. These benefits include the following:**
- Autohealing based on application-specific health checks, which replace nonfunctioning instances
- Support for multizone groups that provide for availability in spite of zone-level failures
- Load balancing to distribute workload across all instances in the group
- Autoscaling, which adds or removes instances in the group to accommodate increases and decreases in workloads
- Automatic, incremental updates to reduce disruptions to workload processing


**Know that Kubernetes Engine is a managed Kubernetes service that provides container orchestration**. Containers are increasingly used to process workloads because they have less overhead than VMs and allow for finer-grained allocation of resources than VMs. A Kubernetes cluster has two types of instances: cluster masters and nodes.

**Understand Kubernetes abstractions**. Pods are the smallest computation unit managed by Kubernetes. Pods contain one or more containers. A ReplicaSet is a controller that manages the number of pods running for a deployment. A deployment is a higher-level concept that manages ReplicaSets and provides declarative updates. PersistentVolumes is Kubernetes’ way of representing storage allocated or provisioned for use by a pod. Pods acquire access to persistent volumes by creating a PersistentVolumeClaim, which is a logical way to link
a pod to persistent storage. StatefulSets are used to designate pods as stateful and assign a unique identifier to them. Kubernetes uses them to track which clients are using which pods and to keep them paired. An Ingress is an object that controls external access to services
running in a Kubernetes cluster.

**Know how to provision Bigtable instances**. Cloud Bigtable is a managed wide-column NoSQL database used for applications that require high-volume, low-latency writes. Bigtable has an HBase interface, so it is also a good alternative to using Hadoop HBase on a Hadoop cluster. Bigtable instances can be provisioned using the cloud console, the command-line SDK, and the REST API. When creating an instance, you provide an instance name, an instance ID, an instance type, a storage type, and cluster specifications.


**Know how to provision Cloud Dataproc**. When provisioning Cloud Dataproc resources, you will specify the configuration of a cluster using the cloud console, the command-line SDK, or the REST API. When you create a cluster, you will specify a name, a region, a zone, a cluster mode, machine types, and an autoscaling policy. The cluster mode deter- mines the number of master nodes and possible worker nodes. Master nodes and worker nodes are configured separately. For each type of node, you can specify a machine type, disk size, and disk type.

**Understand that serverless services do not require conventional infrastructure provisioning but can be configured**. You can configure App Engine using the app.yaml, cron.yaml, distpatch.yaml, or queue.yaml file. Cloud Functions can be configured using parameters to specify memory, region, timeout, and max instances. Cloud Dataflow parameters include job name, project ID, running, staging location, and the default and maximum number of worker nodes.

**Understand the purpose of Stackdriver Monitoring, Stackdriver Logging, and Stackdriver Trace**. Stackdriver Metrics collect metrics on the performance of infrastructure resources and applications. Stackdriver Logging is a service for storing and searching log data about events in infrastructure and applications. Stackdriver Trace is a distributed tracing system designed to collect data on how long it takes to process requests to services.


## Review Questions
**1-** A group of data scientists wants to preprocess a large dataset that will be delivered in batches. The data will be written to Cloud Storage and processed by custom applications running on Compute Engine instances. They want to process the data as quickly as pos- sible when it arrives and are willing to pay the cost of running up to 10 instances at a time. When a batch is finished, they’d like to reduce the number of instances to 1 until the next batch arrives. The batches do not arrive on a known schedule. How would you recommend that they provision Compute Engine instances?

- [ ] A. Use a Cloud Function to monitor Stackdriver metrics, add instances when CPU utiliza- tion peaks, and remove them when demand drops.
- [ ] B. Use a script running on one dedicated instance to monitor Stackdriver metrics, add instances when CPU utilization peaks, and remove them when demand drops.
- [x] C. Use managed instance groups with a minimum of 1 instance and a maximum of 10.
- [ ] D. Use Cloud Dataproc with an autoscaling policy set to have a minimum of 1 instance
and a maximum of 10.

```diff
+ C. The correct answer is C. A managed instance group will provision instances as required to meet the load and stay within the bounds set for the number of instances.
- Option A is incorrect; Cloud Functions are for event-driven processing, not continually monitoring metrics.
- Option B is incorrect because it is not the most efficient way to scale instances.
- Option D is incorrect, since the requirements call for Compute Engine instances, not a Hadoop/Spark cluster.
```

**2-** You are running a high-performance computing application in a managed instance group. You notice that the throughput of one instance is significantly lower than that for other instances. The poorly performing instance is terminated, and another instance is created to replace it. What feature of managed instance groups is at work here?
- [ ] A. Autoscaling
- [x] B. Autohealing
- [ ] C. Redundancy
- [ ] D. Eventual consistency

```diff
+ B. The correct answer is B. Autohealing uses a health check function to determine whether an application is functioning correctly, and if not, the instance is replaced.
- Option A is incorrect; autoscaling adds or removes instances based on instance metrics.
- Option C is incorrect; redundancy is a feature of instance groups, but it is not the mechanism that replaces poorly performing nodes.
- Option D is incorrect; eventual consistency describes a model for storing writes in a way that they will eventually be visible to all queries.
```

**3-** A new engineer in your group asks for your help with creating a managed instance group. The engineer knows the configuration and the minimum and maximum number of instances in the MIG. What is the next thing the engineer should do to create the desired MIG?
- [ ] A. Create each of the initial members of the instance group using gcloud compute instance create commands
- [ ] B. Create each of the initial members of the instance group using the cloud console
- [x] C. Create an instance template using the gcloud compute instance-templates create
command
- [ ] D. Create an instance template using the cbt create instance-template command

```diff
+ C. The correct answer is C, defining an instance template using the gcloud compute instance-templates create command.
- Options A and B are incorrect, since there is no need to create each instance individually.
- Option D is incorrect. cbt is the command-line utility for working with Cloud Bigtable.
```

**4-** Your team is migrating applications from running on bare-metal servers and virtual machines to running in containers. You would like to use Kubernetes Engine to run those containers. One member of the team is unfamiliar with Kubernetes and does not under- stand why they cannot find a command to deploy a container. How would you describe the reason why there is no deploy container command?
- [x] A. Kubernetes uses pods as the smallest deployable unit, and pods have usually one but possibly multiple containers that are deployed as a unit.
- [ ] B. Kubernetes uses deployments as the smallest deployable unit, and pods have usually one but possibly multiple containers that are deployed as a unit.
- [ ] C. Kubernetes uses replicas as the smallest deployable unit, and pods have usually one but possibly multiple containers that are deployed as a unit.
- [ ] D. Kubernetes calls containers “pods,” and the command to deploy is kubectl deploy pod.

```diff
+ A. The correct answer is A; Kubernetes uses pods as the smallest deployable unit.
- Options B and C are incorrect because deployments and replicas are Kubernetes abstractions, but they are not used as the mechanism for logically encapsulating containers.
- Option D is incorrect, since pods and containers are not synonymous.
```

**5-** A Kubernetes administrator wants to improve the performance of an application running in Kubernetes. They have determined that the four replicas currently running are not enough to meet demand and want to increase the total number of replicas to six. The name of the deployment is my-app-123. What command should they use?
- [x] A. kubectl scale deployment my-app-123 --replicas 6
- [ ] B. kubectl scale deployment my-app-123 --replicas 2
- [ ] C. gcloud containers scale deployment my-app-123 --replicas 6
- [ ] D. gcloud containers scale deployment my-app-123 --replicas 2


```diff
+ A. The correct answer is A; the kubectl scale deployment command specifying the desired number of replicas is the correct command.
- Option B is incorrect, since this would set the number of replicas to 2.
- Options C and D are incorrect; there is no gcloud containers scale deployment command.
```

**6-** A Cloud Bigtable instance with one cluster is not performing as expected. The instance was created for analytics. Data is continuously streamed from thousands of sensors, and statisti- cal analysis programs run continually in a batch. What would you recommend to improve performance?
- [ ] A. Use a write-optimized operating system on the nodes
- [ ] B. Use a read-optimized operating system on the nodes
- [x] C. Add a cluster, run batch processing on one cluster, and have writes routed to the other cluster
- [ ] D. Add another node pool to the cluster in each zone that already has a node pool or that cluster

```diff
+ C. The correct answer is C, using two clusters with one dedicated for receiving write operations and the other responsible for batch processing.
- Options A and B are incorrect because you do not specify the operating system used in Bigtable.
- Option D is incorrect; a cluster cannot have multiple node pools in the same zone.
```

**7-** A Cloud Dataproc cluster is running with a single master node. You have determined that the cluster needs to be highly available. How would you increase the number of master nodes to 3?
- [ ] A. Use the gcloud dataproc clusters update command with parameter --num-masters 3
- [ ] B. Use the gcloud dataproc clusters update command with parameter --add-masters 2
- [ ] C. Use the cbt dataproc clusters update command with parameter --add-masters 2
- [x] D. The number of master nodes cannot be changed. A new cluster would have to be deployed with three master nodes.


```diff
+ D. The correct answer is D; the number of master nodes cannot be changed.
- Options A and B are incorrect; there is no --num-masters or --add-masters parameter in the gcloud dataproc clusters update command.
- Option C is incorrect; cbt is the command-line utility for working with Cloud Bigtable.
```

**8-** You have provisioned a Kubernetes Engine cluster and deployed an application. The appli- cation load varies during the day, and you have configured autoscaling to add replicas when CPU utilization exceeds 60 percent. How is that CPU utilization calculated?
- [x] A. Based on all CPUs used by the deployment
- [ ] B. Based on all CPUs in the cluster
- [ ] C. Based on all CPU utilization of the most CPU-intensive pod
- [ ] D. Based on the CPU utilization of the least CPU-intensive pod

```diff
+ A. The correct answer is A; the total CPU utilization by the deployment is used as the basis for making scaling decisions.
- Option B is incorrect; some CPUs in the cluster may be used by other deployments.
- Options C and D are incorrect because the decision is based on overall utilization, not any individual pod.
```

**9-** A team of data scientists wants to run a Python application in a Docker container. They want to minimize operational overhead, so they decide to use App Engine. They want to run the application in a Python 3.4 environment. Which configuration file would they modify to specify that runtime?
- [x] A. app.yaml
- [ ] B. queue.yaml
- [ ] C. dispatch.yaml
- [ ] D. cron.yaml

```diff
+ A. The correct answer is A; app.yaml is the configuration file used to specify the runtime.
- Option B is incorrect; queue.yaml is used to configure task queues.
- Option C is incorrect; dispatch.yaml is used to override routing rules.
- Option D is incorrect; cron.yaml is used to schedule tasks.
```

**10-** Your team is experimenting with Cloud Functions to build a pipeline to process images uploaded to Cloud Storage. During the development stage, you want to avoid sudden spikes in Cloud Functions use because of errors in other parts of the pipeline, particularly the test code that uploads test images to Cloud Storage. How would you reduce the risk of running large numbers of Cloud Functions at one time?
- [ ] A. Use the --limit parameter when deploying the function
- [x] B. Use the --max-instances parameter when deploying the function
- [ ] C. Set a label with the key max-instances and the value of the maximum number of instances
- [ ] D. Set a language-specific parameter in the function to limit the number of instances

```diff
+ B. The correct answer is B. The --max-instances parameter limits the number of concurrently executing function instances.
- Option A is incorrect; --limit is not a parameter used with function deployments.
- Option C is incorrect; labels are not used to control configuration of functions.
- Option D is incorrect; language-specific parameters are not used to configure Cloud Functions.
```

**11-** Audit control requirements at your company require that all logs be kept for at least 365 days. You prefer to keep logs and log entries in Stackdriver logging, but you understand that logs with predefined retention periods of less than 1 year will require you to set up an export to another storage system, such as Cloud Storage. Which of the following logs would you need to set up exports for to meet the audit requirement?
- [ ] A. Admin activity audit logs
- [ ] B. System event audit logs
- [ ] C. Access transparency logs
- [x] D. Data access audit logs

```diff
+ D. The correct answer is D; data access logs have a 30-day data retention period.
- Options A, B, and C are incorrect; they all have 400-day retention periods.
```

**12-** You would like to collect data on the memory utilization of instances running in a particular managed instance group. What Stackdriver service would you use?
- [ ] A. Stackdriver Debugger
- [ ] B. Stackdriver Logging
- [x] C. Stackdriver Monitoring
- [ ] D. Stackdriver Trace

```diff
+ C. The correct answer is C; Stackdriver Monitoring collects performance metrics.
- Option A is incorrect; Stackdriver Debugger is used to inspect the state of running code.
- Option B is incorrect; Stackdriver Logging is used to collect semi-structured data about events.
- Option D is incorrect; Stackdriver Trace is used to collect information about the time required to execute functions in a call stack.
```

**13-** You would like to view information recorded by an application about events prior to the application crashing. What Stackdriver service would you use?
- [ ] A. Stackdriver Debugger
- [x] B. Stackdriver Logging
- [ ] C. Stackdriver Monitoring
- [ ] D. Stackdriver Trace

```diff
+ B. The correct answer is B; Stackdriver Logging is used to collect semi-structured data about events.
- Option A is incorrect; Stackdriver Debugger is used to inspect the state of running code.
- Option C is incorrect; Stackdriver Monitoring is used to collect performance metrics.
- Option D is incorrect; Stackdriver Trace is used to collect information about the time required to execute functions in a call stack.
```

**14-** Customers are complaining of long waits while your e-commerce site processes orders. There are many microservices in your order processing system. You would like to view information about the time each microservice takes to run. What Stackdriver service would you use?
- [ ] A. Stackdriver Debugger
- [ ] B. Stackdriver Logging
- [ ] C. Stackdriver Monitoring
- [x] D. Stackdriver Trace

```diff
+ D. The correct answer is D; Stackdriver Trace is used to collect information about the time required to execute functions in a call stack.
- Option A is incorrect; Stackdriver Debugger is used to inspect the state of running code.
- Option B is incorrect; Stackdriver Logging is used to collect semi-structured data about events.
- Option C is incorrect; Stackdriver Monitoring is used to collect performance metrics.
```

**15-** You have created a test environment for a group of business analysts to run several Cloud Dataflow pipelines. You want to limit the processing resources any pipeline can consume. What execution parameter would you specify to limit processing resources?
- [ ] A. numWorkers
- [x] B. maxNumWorkers
- [ ] C. streaming
- [ ] D. maxResources

```diff
+ B. The correct answer is B; maxNumWorkers specifies the maximum number of instances that can be run for a Cloud Dataflow pipeline.
- Option A is incorrect; numWorkers is the initial number of workers.
- Option C is incorrect; streaming specifies whether streaming mode is enabled.
- Option D is incorrect; it is not an actual parameter.
```
