# Chapter 4: Designing a Data Processing Solution

## Designing Infrastructure

Summary of compute option features
![image](https://user-images.githubusercontent.com/1645304/135768028-ed245a0a-d201-401b-8612-e6355fb7c4f0.png)

### Availability, Reliability, and Scalability of Infrastructure
When we design systems, we need to consider three nonfunctional requirements: availability, reliability, and scalability.
- **Availability** is defined as the ability of a user to access a resource at a specific time. Availability is usually measured as the percentage of time that a system is operational.
- **Reliability**, the probability that a system will meet service-level objectives for some duration of time. Reliability is often measured as the mean time between failures.
- **Scalability** is the ability of a system to handle increases in workload by adding resources to the system as needed. It implies that:
  - As workload increases, resources will be available to process that workload.
  - As workload decreases, the amount of allo- cated resources will decrease to a level sufficient to meet the workload plus some marginal extra capacity.

### Making Compute Resources Available, Reliable, and Scalable
employ clusters of machines or virtual machines with load balancers and autoscalers to distribute workload and adjust the size of the cluster to meet demand.

#### Compute Engine
- Managed instance groups (MIGs): MIGs are defined using a template. Templates include specifications for the machine type, boot disk image or container image, labels, and other instance properties. All members of a MIG are identical. When an instance in a MIG fails, it is replaced with an identically configured VM.
- Load balancers direct traffic only to responsive instances
  - Global load balancers: HTTP(S) Load Balancing, SSL Proxy, and TCP Proxy.
  - Regional load balancers are Network TCP/UDP, Internal TCP/UDP, and Internal HTTP(S)
- Instance groups can be either:
  - Zonal instance groups, all instances are created in the same zone.
  - Regional instance groups place instances in multiple zones in the same region. higher availability because the MIG could withstand a zone-level failure
- Autoscalers add and remove instances according to workload. Uses a policy that specifies the criteria for adjusting the size of the group:
  - CPU utilization and other metrics collected by Stackdriver,
  - load-balancing capacity, and
  - the number of messages in a queue.

#### Kubernetes Engine
Kubernetes deploys containers in an abstraction known as a pod. When pods fail, they are replaced much like failed instances in a managed instance group.

Nodes in Kubernetes Engine belong to a pool, and with the autorepair fea- ture turned on, failed nodes will be reprovisioned automatically.
When using Kubernetes Engine, you can specify:
- whether the endpoint for accessing the cluster is zonal or regional (cluster accessible even if there is a failure in a zone).
- A high availability cluster configuration that replicates master and worker nodes across multiple zones.

#### App Engine
Depending on how you have configured App Engine policies for scaling , when the scheduler has a request, it can:
- send it to an existing instance,
- add it to a queue, or
- start another instance.

Policies for scaling by specifying values for target:
- CPU utilization,
- throughput utilization, and
- maximum concurrent requests.


#### Cloud Functions
Designed so that each instance of a cloud function handles one request at a time. If there is a spike in workload, additional function instances can be created.

You have some control over this with the ability to set a maximum number of concurrently running instances.

### Making Storage Resources Available, Reliable, and Scalable

- **Memorystore** is an in-memory Redis cache. Standard Tier is automatically configured to maintain a replica in a different zone.
  - The replica is used only for high availability, not scalability.
  - when Redis detects a failure and triggers a failover to the replica
- **Persistent disks** are used with Compute Engine and Kubernetes Engine to provide network- based disk storage to VMs and containers.
  - They have built-in redundancy for high availability and reliability.
  - Users can create snapshots of disks and store them in Cloud Storage for additional risk mitigation.
- **Cloud SQL** is a managed relational database that can operate in high-availability mode by maintaining a primary instance in one zone and a standby instance in another zone within the same region. Synchronous replication keeps the data up to date in both instances.
  - If you require multi-regional redundancy in your relational database, you should consider **Cloud Spanner**.
- **Cloud Storage** stores replicas of objects within a region when using standard storage and across regions when using multi-regional storage.

### Making Network Resources Available, Reliable, and Scalable
- Standard Tier uses the public Internet network to transfer data between Google data centers, data is subject to the reliability of the public Internet.
- Premium Tier routes traffic only over Google’s global network.

For high throughput:
- Cloud Interconnect is available as a dedicated interconnect in which an enterprise directly con- nects to a Google endpoint and traffic flows directly between the two networks.
- Partner interconnect, in which case data flows through a third-party net- work but not over the Internet.

### Hybrid Cloud and Edge Computing

#### Analytics Hybrid Cloud
The analytics hybrid cloud is used when transaction processing systems continue to run on premises and data is extracted and transferred to the cloud for analytic processing.

Populate a data warehouse or data lake initially in GCP:
- large on-premises data warehouse or data lake, may need to use the **Cloud Transfer Appliance**.
- smaller data warehouses and data marts can be migrated over the net- work if there is sufficient bandwidth


For additional details on how long it takes to trans- fer different volumes of data, see Google’s helpful matrix on transfer volumes, network throughputs, and time required to transfer data at https://cloud.google.com/products/ data-transfer/

#### Edge Cloud
Edge computing brings some computation outside the cloud and closer to where the results of computation are applied.
![image](https://user-images.githubusercontent.com/1645304/135769725-435acf47-5b65-4644-a6a6-2387c71af613.png)

## Designing for Distributed Processing

### Distributed Processing: Messaging
- Message Brokers
- Message Queues
- Event Processing Models
  - At Least Once Delivery
  - At Least Once, in Order Delivery
  - Exactly Once

### Distributed Processing: Services
- Service-Oriented Architectures: driven by busi- ness operations and delivering business value.
- Microservices: use multiple, independent components and common communica- tion protocols to provide higher-level business services.
- Serverless Functions: extend the principles of microservices by removing concerns for containers and managing runtime environments.

## Migrating a Data Warehouse
Data warehouses include:
- extraction, transformation, and load scripts;
- views and embedded user-defined functions;
- reporting and visualization tools.
- Identity management information and access control policies used to protect the confidentiality, integrity, and availability of a data warehouse.

Different kinds of migrations:
- *off-loading data warehouse migration* involves copying data and schema from the on-premises data warehouse to the cloud to get a solution up and running in the cloud as fast as possible. This is a reasonable choice when the business needs the extra storage and compute capacity of the cloud, or if you want end users to have access to cloud-based reporting and analysis tools as fast as possible.
- *full data warehouse migration* includes all the steps in the scope of an off-loading migration plus moving data pipelines so that data can be loaded directly into the data ware- house from source systems. This approach allows you to take advantage of cloud tools for extraction, transformation, and load.

At a high level, the process of migrating (for any kinds of migrations) a data warehouse involves four stages:
- Assessing the current state of the data warehouse
- Designing the future state
- Migrating data, jobs, and access controls to the cloud
- Validating the cloud data warehouse

### Assessing the Current State of a Data Warehouse
#### Technical Requirements
Gathering technical requirements should include the following:
- A list of data sources, including metadata about data sources, such as the frequency at which the data source is updated
- A data catalog, which includes both attributes collected from source systems and derived attributes, such as summaries by time and location
- A data model, including schemas, tables, views, indexes, and stored procedures
- A list of ETL scripts and their purposes
- A list of reports and visualization generated from the data in the data warehouse
- A list of roles and associated access controls, including administrators, developers, and end users

In addition also include some less well-defined details about the existing data warehouse, such as:
- limitations in the data or reporting tools that limit analysis;
- constraints, such as when data is available from sources and when it has to be available for querying from the data warehouse;
- and any unmet technical require- ments, like sufficiently fine-grained adequate access controls.

#### Business Benefits
The business benefits of a data warehouse migration should be assessed in the early stages of a migration project.
Business value can be derived from cost savings, reduction in back- log of ETL and reporting work, and increased agility and ability to deliver insights under changing conditions.

### Designing the Future State of a Data Warehouse
Define the key performance indicators (KPIs) that are used to measure how well the migration process is meeting objectives. KPIs may include:
- the amount of data migrated,
- the number of reports now available in the cloud warehouse,
- the number of workloads completely migrated.

### Migrating Data, Jobs, and Access Controls
few ways to prioritize data and jobs migration are as follows:
- Exploiting current opportunities: use cases that demonstrate clear business value
- Migrating analytical workloads first
- Focusing on the user experience first
- Prioritizing low-risk use cases first: use cases that do not have other systems depending on them, and they are not needed for high-priority business processes that have to be available and reliable.

### Validating the Data Warehouse
includes testing and verifying that:
- Schemas are defined correctly and completely
- All data expected to be in the data warehouse is actually loaded
- Transformations are applied correctly and data quality checks pass
- Queries, reports, and visualizations run as expected
- Access control policies are in place
- Other governance practices are in place

## Exam Essentials
**Know the four main compute GCP products.** Compute Engine is GCP’s infrastructure-as- a-service (IaaS) product.

- With Compute Engine, you have the greatest amount of control over your infra- structure relative to the other GCP compute services.
- Kubernetes is a container orchestration system, and Kubernetes Engine is a managed Kubernetes service. With Kubernetes Engine, Google maintains the cluster and assumes responsibility for installing and configuring the Kubernetes platform on the cluster. Kubernetes Engine deploys Kubernetes on managed instance groups.
- App Engine is GCP’s original platform-as-a-service (PaaS) offering. App Engine is designed to allow developers to focus on application development while mini- mizing their need to support the infrastructure that runs their applications. App Engine has two versions: App Engine Standard and App Engine Flexible.
- Cloud Functions is a serverless, managed compute service for running code in response to events that occur in the cloud. Events are supported for Cloud Pub/ Sub, Cloud Storage, HTTP events, Firebase, and Stackdriver Logging.

**Understand the definitions of availability, reliability, and scalability.** Availability is defined as the ability of a user to access a resource at a specific time. Availability is usu- ally measured as the percentage of time a system is operational. Reliability is defined as the probability that a system will meet service-level objectives for some duration of time. Reliability is often measured as the mean time between failures. Scalability is the ability of a system to meet the demands of workloads as they vary over time.

**Know when to use hybrid clouds and edge computing.** The analytics hybrid cloud is used when transaction processing systems continue to run on premises and data is extracted and transferred to the cloud for analytic processing. A variation of hybrid clouds is an edge cloud, which uses local computation resources in addition to cloud platforms. This archi- tecture pattern is used when a network may not be reliable or have sufficient bandwidth to transfer data to the cloud. It is also used when low-latency processing is required.

**Understand messaging.** Message brokers are services that provide three kinds of function- ality: message validation, message transformation, and routing. Message validation is the process of ensuring that messages received are correctly formatted. Message transformation is the process of mapping data to structures that can be used by other services. Message brokers can receive a message and use data in the message to determine where the message should be sent. Routing is used when hub-and-spoke message brokers are used.

**Know distributed processing architectures.** SOA is a distributed architecture that is driven by business operations and delivering business value. Typically, an SOA system serves a discrete business activity. SOAs are self-contained sets of services. Microservices are a variation on SOA architecture. Like other SOA systems, microservice architectures use multiple, independent components and common communication protocols to provide higher-level business services. Serverless functions extend the principles of microservices by removing concerns for containers and managing runtime environments.

**Know the steps to migrate a data warehouse.** At a high level, the process of migrating a data warehouse involves four stages:
- Assessing the current state of the data warehouse
- Designing the future state
- Migrating data, jobs, and access controls to the cloud
- Validating the cloud data warehouse


## Questions
1- A startup is designing a data processing pipeline for its IoT platform. Data from sensors will stream into a pipeline running in GCP. As soon as data arrives, a validation process, written in Python, is run to verify data integrity. If the data passes the validation, it
is ingested; otherwise, it is discarded. What services would you use to implement the validation check and ingestion?

- [ ] A. Cloud Storage and Cloud Pub/Sub
- [ ] B. Cloud Functions and Cloud Pub/Sub
- [x] C. Cloud Functions and BigQuery
- [ ] D. Cloud Storage and BigQuery

2- Your finance department is migrating a third-party application from an on-premises physical server. The system was written in C, but only the executable binary is available. After the migration, data will be extracted from the application database, transformed, and stored in a BigQuery data warehouse. The application is no longer actively supported by the original developer, and it must run on an Ubuntu 14.04 operating system that has been configured with several required packages. Which compute platform would you use?
- [x] A. Compute Engine
- [ ] B. Kubernetes Engine
- [ ] C. App Engine Standard
- [ ] D. Cloud Functions

3- A team of developers has been tasked with rewriting the ETL process that populates an enterprise data warehouse. They plan to use a microservices architecture. Each microservice will run in its own Docker container. The amount of data processed during a run can
vary, but the ETL process must always finish within one hour of starting. You want to minimize the amount of DevOps tasks the team needs to perform, but you do not want to sacrifice efficient utilization of compute resources. What GCP compute service would you recommend?
- [ ] A. Compute Engine
- [x] B. Kubernetes Engine
- [ ] C. App Engine Standard
- [ ] D. Cloud Functions

4- Your consulting company is contracted to help an enterprise customer negotiate a contract with a SaaS provider. Your client wants to ensure that they will have access to the SaaS service and it will be functioning correctly with only minimal downtime. What metric would you use when negotiating with the SaaS provider to ensure that your client’s reliability requirements are met?
- [ ] A. Average CPU utilization
- [ ] B. A combination of CPU and memory utilization
- [x] C. Mean time between failure
- [ ] D. Mean time to recovery

5- To ensure high availability of a mission-critical application, your team has determined that it needs to run the application in multiple regions. If the application becomes unavailable in one region, traffic from that region should be routed to another region. Since you are designing a solution for this set of requirements, what would you expect to include?
- [ ] A. Cloud Storage bucket
- [ ] B. Cloud Pub/Sub topic
- [x] C. Global load balancer
- [ ] D. HA VPN

6- A startup is creating a business service for the hotel industry. The service will allow hotels to sell unoccupied rooms on short notice using the startup’s platform. The startup wants to make it as easy as possible for hotels to share data with the platform, so it uses a message queue to collect data about rooms that are available for rent. Hotels send a message for each room that is available and the days that it is available. Room identifier and dates are the keys that uniquely identify a listing. If a listing exists and a message is received with the same room identifier and dates, the message is discarded. What are the minimal guarantees that you would want from the message queue?
- [ ] A. Route randomly to any instance that is building a machine learning model
- [ ] B. Route based on the sensor identifier so identifiers in close proximity are used in the
same model
- [ ] C. Route based on machine type so only data from one machine type is used for each model
- [ ] D. Route based on timestamp so metrics close in time to each other are used in the same model

7- Sensors on manufacturing machines send performance metrics to a cloud-based service that uses the data to build models that predict when a machine will break down. Metrics are sent in messages. Messages include a sensor identifier, a timestamp, a machine type, and a set of measurements. Different machine types have different characteristics related to failures, and machine learning engineers have determined that for highest accuracy, each machine type should have its own model. Once messages are written to a message broker, how should they be routed to instances of a machine learning service?
- [ ] A. Route randomly to any instance that is building a machine learning model
- [ ] B. Route based on the sensor identifier so that identifiers in close proximity are used in
the same model
- [x] C. Route based on machine type so that only data from one machine type is used for each model
- [ ] D. Route based on timestamp so that metrics close in time to one another are used in the same model

8- As part of a cloud migration effort, you are tasked with compiling an inventory of existing applications that will move to the cloud. One of the attributes that you need to track for each application is a description of its architecture. An application used by the finance department is written in Java, deployed on virtual machines, has several distinct services, and uses the SOAP protocol for exchanging messages. How would you categorize this architecture?
- [ ] A. Monolithic
- [ ] B. Service-oriented architecture (SOA)
- [x] C. Microservice
- [ ] D. Serverless functions

9- As part of a migration to the cloud, your department wants to restructure a distributed application that currently runs several services on a cluster of virtual machines. Each service implements several functions, and it is difficult to update one function without disrupting operations of the others. Some of the services require third-party libraries to be installed. Your company has standardized on Docker containers for deploying new services. What kind of architecture would you recommend?
- [ ] A. Monolithic
- [ ] B. Hub-and-spoke
- [x] C. Microservices
- [ ] D. Pipeline architecture

10- The CTO of your company is concerned about the rising costs of maintaining your com- pany’s enterprise data warehouse. Some members of your team are advocating to migrate to a cloud-based data warehouse such as BigQuery. What is the first step for migrating from the on-premises data warehouse to a cloud-based data warehouse?
- [x] A. Assessing the current state of the data warehouse
- [ ] B. Designing the future state of the data warehouse
- [ ] C. Migrating data, jobs, and access controls to the cloud
- [ ] D. Validating the cloud data warehouse

11- When gathering requirements for a data warehouse migration, which of the following would you include in a listing of technical requirements?
- [x] A. Data sources, data model, and ETL scripts
- [ ] B. Data sources, data model, and business sponsor roles
- [ ] C. Data sources only
- [ ] D. Data model, data catalog, ETL scripts, and business sponsor roles

12- In addition to concerns about the rising costs of maintaining an on-premises data warehouse, the CTO of your company has complained that new features and reporting are not being rolled out fast enough. The lack of adequate business intelligence has been blamed for a drop in sales in the last quarter. Your organization is incurring what kind of cost because of the backlog?
- [ ] A. Capital
- [ ] B. Operating
- [x] C. Opportunity
- [ ] D. Fiscal

13- The data modelers who built your company’s enterprise data warehouse are asking for your guidance to migrate the data warehouse to BigQuery. They understand that BigQuery is an analytical database that uses SQL as a query language. They also know that BigQuery supports joins, but reports currently run on the data warehouse are consuming significant amounts of CPU because of the number and scale of joins. What feature of BigQuery would you suggest they consider in order to reduce the number of joins required?
- [ ] A. Colossus filesystem
- [ ] B. Columnar data storage
- [x] C. Nested and repeated fields
- [ ] D. Federated storage

14- While the CTO is interested in having your enterprise data warehouse migrated to the cloud as quickly as possible, the CTO is particularly risk averse because of errors in reporting in the past. Which prioritization strategy would you recommend?
- [ ] A. Exploiting current opportunities
- [ ] B. Migrating analytical workloads first
- [ ] C. Focusing on the user experience first
- [x] D. Prioritizing low-risk use cases first

15- The enterprise data warehouse has been migrated to BigQuery. The CTO wants to shut down the on-premises data warehouse but first wants to verify that the new cloud-based data warehouse is functioning correctly. What should you include in the verification process?
- [ ] A. Verify that schemas are correct and that data is loaded
- [x] B. Verify schemas, data loads, transformations, and queries
- [ ] C. Verify that schemas are correct, data is loaded, and the backlog of feature requests is prioritized
- [ ] D. Verify schemas, data loads, transformations, queries, and that the backlog of feature requests is prioritized

