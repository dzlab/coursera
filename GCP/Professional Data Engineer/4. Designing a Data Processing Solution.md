# Chapter 4: Designing a Data Processing Solution

## Designing Infrastructure

Summary of compute option features
![image](https://user-images.githubusercontent.com/1645304/135768028-ed245a0a-d201-401b-8612-e6355fb7c4f0.png)

### Availability, Reliability, and Scalability of Infrastructure
When we design systems, we need to consider three nonfunctional requirements: availability, reliability, and scalability.
- **Availability** is defined as the ability of a user to access a resource at a specific time. Availability is usually measured as the percentage of time that a system is operational.
- **Reliability**, the probability that a system will meet service-level objectives for some duration of time. Reliability is often measured as the mean time between failures.
- **Scalability** is the ability of a system to handle increases in workload by adding resources to the system as needed. It implies that:
  - As workload increases, resources will be available to process that workload.
  - As workload decreases, the amount of allo- cated resources will decrease to a level sufficient to meet the workload plus some marginal extra capacity.

### Making Compute Resources Available, Reliable, and Scalable
employ clusters of machines or virtual machines with load balancers and autoscalers to distribute workload and adjust the size of the cluster to meet demand.

#### Compute Engine
- Managed instance groups (MIGs): MIGs are defined using a template. Templates include specifications for the machine type, boot disk image or container image, labels, and other instance properties. All members of a MIG are identical. When an instance in a MIG fails, it is replaced with an identically configured VM.
- Load balancers direct traffic only to responsive instances
  - Global load balancers: HTTP(S) Load Balancing, SSL Proxy, and TCP Proxy.
  - Regional load balancers are Network TCP/UDP, Internal TCP/UDP, and Internal HTTP(S)
- Instance groups can be either:
  - Zonal instance groups, all instances are created in the same zone.
  - Regional instance groups place instances in multiple zones in the same region. higher availability because the MIG could withstand a zone-level failure
- Autoscalers add and remove instances according to workload. Uses a policy that specifies the criteria for adjusting the size of the group:
  - CPU utilization and other metrics collected by Stackdriver,
  - load-balancing capacity, and
  - the number of messages in a queue.

#### Kubernetes Engine
Kubernetes deploys containers in an abstraction known as a pod. When pods fail, they are replaced much like failed instances in a managed instance group.

Nodes in Kubernetes Engine belong to a pool, and with the autorepair fea- ture turned on, failed nodes will be reprovisioned automatically.
When using Kubernetes Engine, you can specify:
- whether the endpoint for accessing the cluster is zonal or regional (cluster accessible even if there is a failure in a zone).
- A high availability cluster configuration that replicates master and worker nodes across multiple zones.

#### App Engine
Depending on how you have configured App Engine policies for scaling , when the scheduler has a request, it can:
- send it to an existing instance,
- add it to a queue, or
- start another instance.

Policies for scaling by specifying values for target:
- CPU utilization,
- throughput utilization, and
- maximum concurrent requests.


#### Cloud Functions
Designed so that each instance of a cloud function handles one request at a time. If there is a spike in workload, additional function instances can be created.

You have some control over this with the ability to set a maximum number of concurrently running instances.

### Making Storage Resources Available, Reliable, and Scalable

- **Memorystore** is an in-memory Redis cache. Standard Tier is automatically configured to maintain a replica in a different zone.
  - The replica is used only for high availability, not scalability.
  - when Redis detects a failure and triggers a failover to the replica
- **Persistent disks** are used with Compute Engine and Kubernetes Engine to provide network- based disk storage to VMs and containers.
  - They have built-in redundancy for high availability and reliability.
  - Users can create snapshots of disks and store them in Cloud Storage for additional risk mitigation.
- **Cloud SQL** is a managed relational database that can operate in high-availability mode by maintaining a primary instance in one zone and a standby instance in another zone within the same region. Synchronous replication keeps the data up to date in both instances.
  - If you require multi-regional redundancy in your relational database, you should consider **Cloud Spanner**.
- **Cloud Storage** stores replicas of objects within a region when using standard storage and across regions when using multi-regional storage.

### Making Network Resources Available, Reliable, and Scalable
- Standard Tier uses the public Internet network to transfer data between Google data centers, data is subject to the reliability of the public Internet.
- Premium Tier routes traffic only over Google’s global network.

For high throughput:
- Cloud Interconnect is available as a dedicated interconnect in which an enterprise directly con- nects to a Google endpoint and traffic flows directly between the two networks.
- Partner interconnect, in which case data flows through a third-party net- work but not over the Internet.

### Hybrid Cloud and Edge Computing

#### Analytics Hybrid Cloud
The analytics hybrid cloud is used when transaction processing systems continue to run on premises and data is extracted and transferred to the cloud for analytic processing.

Populate a data warehouse or data lake initially in GCP:
- large on-premises data warehouse or data lake, may need to use the **Cloud Transfer Appliance**.
- smaller data warehouses and data marts can be migrated over the net- work if there is sufficient bandwidth


For additional details on how long it takes to trans- fer different volumes of data, see Google’s helpful matrix on transfer volumes, network throughputs, and time required to transfer data at https://cloud.google.com/products/ data-transfer/

#### Edge Cloud
Edge computing brings some computation outside the cloud and closer to where the results of computation are applied.
![image](https://user-images.githubusercontent.com/1645304/135769725-435acf47-5b65-4644-a6a6-2387c71af613.png)

## Designing for Distributed Processing

### Distributed Processing: Messaging
- Message Brokers
- Message Queues
- Event Processing Models
  - At Least Once Delivery
  - At Least Once, in Order Delivery
  - Exactly Once

### Distributed Processing: Services
- Service-Oriented Architectures: driven by busi- ness operations and delivering business value.
- Microservices: use multiple, independent components and common communica- tion protocols to provide higher-level business services.
- Serverless Functions: extend the principles of microservices by removing concerns for containers and managing runtime environments.

## Migrating a Data Warehouse
Data warehouses include:
- extraction, transformation, and load scripts;
- views and embedded user-defined functions;
- reporting and visualization tools.
- Identity management information and access control policies used to protect the confidentiality, integrity, and availability of a data warehouse.

Different kinds of migrations:
- *off-loading data warehouse migration* involves copying data and schema from the on-premises data warehouse to the cloud to get a solution up and running in the cloud as fast as possible. This is a reasonable choice when the business needs the extra storage and compute capacity of the cloud, or if you want end users to have access to cloud-based reporting and analysis tools as fast as possible.
- *full data warehouse migration* includes all the steps in the scope of an off-loading migration plus moving data pipelines so that data can be loaded directly into the data ware- house from source systems. This approach allows you to take advantage of cloud tools for extraction, transformation, and load.

At a high level, the process of migrating (for any kinds of migrations) a data warehouse involves four stages:
- Assessing the current state of the data warehouse
- Designing the future state
- Migrating data, jobs, and access controls to the cloud
- Validating the cloud data warehouse

### Assessing the Current State of a Data Warehouse
#### Technical Requirements
Gathering technical requirements should include the following:
- A list of data sources, including metadata about data sources, such as the frequency at which the data source is updated
- A data catalog, which includes both attributes collected from source systems and derived attributes, such as summaries by time and location
- A data model, including schemas, tables, views, indexes, and stored procedures
- A list of ETL scripts and their purposes
- A list of reports and visualization generated from the data in the data warehouse
- A list of roles and associated access controls, including administrators, developers, and end users

In addition also include some less well-defined details about the existing data warehouse, such as:
- limitations in the data or reporting tools that limit analysis;
- constraints, such as when data is available from sources and when it has to be available for querying from the data warehouse;
- and any unmet technical require- ments, like sufficiently fine-grained adequate access controls.

#### Business Benefits
The business benefits of a data warehouse migration should be assessed in the early stages of a migration project.
Business value can be derived from cost savings, reduction in back- log of ETL and reporting work, and increased agility and ability to deliver insights under changing conditions.

### Designing the Future State of a Data Warehouse
Define the key performance indicators (KPIs) that are used to measure how well the migration process is meeting objectives. KPIs may include:
- the amount of data migrated,
- the number of reports now available in the cloud warehouse,
- the number of workloads completely migrated.

### Migrating Data, Jobs, and Access Controls
few ways to prioritize data and jobs migration are as follows:
- Exploiting current opportunities: use cases that demonstrate clear business value
- Migrating analytical workloads first
- Focusing on the user experience first
- Prioritizing low-risk use cases first: use cases that do not have other systems depending on them, and they are not needed for high-priority business processes that have to be available and reliable.

### Validating the Data Warehouse
includes testing and verifying that:
- Schemas are defined correctly and completely
- All data expected to be in the data warehouse is actually loaded
- Transformations are applied correctly and data quality checks pass
- Queries, reports, and visualizations run as expected
- Access control policies are in place
- Other governance practices are in place

## Exam Essentials
- Know the four main compute GCP products
- Understand the definitions of availability, reliability, and scalability.
- Know when to use hybrid clouds and edge computing.
- Understand messaging.
- Know distributed processing architectures.
- Know the steps to migrate a data warehouse.
