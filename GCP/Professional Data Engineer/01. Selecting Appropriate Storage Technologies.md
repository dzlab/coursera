# Chapter 1 Selecting Appropriate Storage Technologies

## Exam Essentials
**Know the four stages of the data lifecycle: ingest, storage, process and analyze, and explore and visualize.** Ingestion is the process of bringing application data, streaming data, and batch data into the cloud. The storage stage focuses on persisting data to an appropriate storage system. Processing and analyzing is about transforming data into a form suitable for analysis. Exploring and visualizing focuses on testing hypotheses and drawing insights from data.

**Understand the characteristics of streaming data.** Streaming data is a set of data that is sent in small messages that are transmitted continuously from the data source. Streaming data may be telemetry data, which is data generated at regular intervals, and event data, which is data generated in response to a particular event. Stream ingestion services need to deal with potentially late and missing data. Streaming data is often ingested using Cloud Pub/Sub.

**Understand the characteristics of batch data.** Batch data is ingested in bulk, typically in files. Examples of batch data ingestion include uploading files of data exported from one application to be processed by another. Both batch and streaming data can be transformed and processed using Cloud Dataflow.

**Know the technical factors to consider when choosing a data store.** These factors include the volume and velocity of data, the type of structure of the data, access control require- ments, and data access patterns.

**Know the three levels of structure of data.** These levels are structured, semi-structured, and unstructured. Structured data has a fixed schema, such as a relational database table. Semi-structured data has a schema that can vary; the schema is stored with data. Unstructured data does not have a structure used to determine how to store data.

**Know which Google Cloud storage services are used with the different structure types.** Structured data is stored in Cloud SQL and Cloud Spanner if it is used with a transaction processing system; BigQuery is used for analytical applications of structured data. Semi-structured data is stored in Cloud Datastore if data access requires full index- ing; otherwise, it can be stored in Bigtable. Unstructured data is stored in Cloud Storage.

**Know the difference between relational and NoSQL databases.** Relational databases are used for structured data whereas NoSQL databases are used for semi-structured data. The four types of NoSQL databases are key-value, document, wide-column, and graph databases.

## Review Questions

**1.** A developer is planning a mobile application for your company’s customers to use to track information about their accounts. The developer is asking for your advice on storage tech- nologies. In one case, the developer explains that they want to write messages each time a significant event occurs, such as the client opening, viewing, or deleting an account. This data is collected for compliance reasons, and the developer wants to minimize administra- tive overhead. What system would you recommend for storing this data?
- [ ] A. Cloud SQL using MySQL
- [ ] B. Cloud SQL using PostgreSQL
- [ ] C. Cloud Datastore
- [x] D. Stackdriver Logging

```diff
+ D. The correct answer is D. Stackdriver Logging is the best option because it is a managed service designed for storing logging data.
- Neither Option A nor B is as good a fit because the developer would have to design and maintain a relational data model and user interface to view and manage log data.
- Option C, Cloud Datastore, would not require a fixed data model, but it would still require the developer to create and maintain a user interface to manage log events.
```

**2.** You are responsible for developing an ingestion mechanism for a large number of IoT sen- sors. The ingestion service should accept data up to 10 minutes late. The service should also perform some transformations before writing the data to a database. Which of the managed services would be the best option for managing late arriving data and performing transfor- mations?
- [ ] A. Cloud Dataproc
- [x] B. Cloud Dataflow
- [ ] C. Cloud Dataprep
- [ ] D. Cloud SQL

```diff
+ B. The correct answer is B. Cloud Dataflow is a stream and batch processing service that is used for transforming data and processing streaming data.
- Option A, Cloud Dataproc, is a managed Hadoop and Spark service and not as well suited as Cloud Dataflow for the kind of stream processing specified.
- Option C, Cloud Dataprep, is an interactive tool for exploring and preparing data sets for analysis.
- Option D, Cloud SQL, is a relational database service, so it may be used to store data, but it is not a service specifically for ingesting and transforming data before writing to a database.
```

**3.** A team of analysts has collected several CSV datasets with a total size of 50 GB. They plan to store the datasets in GCP and use Compute Engine instances to run RStudio, an interac- tive statistical application. Data will be loaded into RStudio using an RStudio data loading tool. Which of the following is the most appropriate GCP storage service for the datasets?
- [x] A. Cloud Storage
- [ ] B. Cloud Datastore
- [ ] C. MongoDB
- [ ] D. Bigtable

```diff
+ A. The correct answer is A, Cloud Storage, because the data in the files is treated as an atomic unit of data that is loaded into RStudio.
- Options B and C are incorrect because those are document databases and there is no requirement for storing the data in semi- structured format with support for fully indexed querying. Also, MongoDB is not a GCP service.
- Option D is incorrect because, although you could load CSV data into a Bigtable table, the volume of data is not sufficient to warrant using Bigtable.
```

**4.** A team of analysts has collected several terabytes of telemetry data in CSV datasets. They plan to store the datasets in GCP and query and analyze the data using SQL. Which of the following is the most appropriate GCP storage service for the datasets?
- [ ] A. Cloud SQL
- [ ] B. Cloud Spanner
- [x] C. BigQuery
- [ ] D. Bigtable

```diff
+ C. The correct answer is C, BigQuery, which is a managed analytical database service that supports SQL and scales to petabyte volumes of data.
- Options A and B are incorrect because both are used for transaction processing applications, not analytics.
- Option D is incorrect because Bigtable does not support SQL.
```

**5.** You have been hired to consult with a startup that is developing software for self-driving vehicles. The company’s product uses machine learning to predict the trajectory of persons and vehicles. Currently, the software is being developed using 20 vehicles, all located in the same city. IoT data is sent from vehicles every 60 seconds to a MySQL database running on a Compute Engine instance using an n2-standard-8 machine type with 8 vCPUs and 16 GB of memory. The startup wants to review their architecture and make any necessary changes to support tens of thousands of self-driving vehicles, all transmitting IoT data every second. The vehicles will be located across North America and Europe. Approximately 4 KB of data is sent in each transmission. What changes to the architecture would you recommend?
- [ ] A. None. The current architecture is well suited to the use case.
- [ ] B. Replace Cloud SQL with Cloud Spanner.
- [x] C. Replace Cloud SQL with Bigtable.
- [ ] D. Replace Cloud SQL with Cloud Datastore.

```diff
+ C. The correct answer is C. Bigtable is the best storage service for IoT data, especially when a large number of devices will be sending data at short intervals.
- Option A is incorrect, because Cloud SQL is designed for transaction processing at a regional level.
- Option B is incorrect because Cloud Spanner is designed for transaction processing, and although it scales to global levels, it is not the best option for IoT data.
- Option D is incorrect because there is no need for indexed, semi-structured data.
```

**6.** As a member of a team of game developers, you have been tasked with devising a way to track players’ possessions. Possessions may be purchased from a catalog, traded with other players, or awarded for game activities. Possessions are categorized as clothing, tools, books, and coins. Players may have any number of possessions of any type. Players can search for other players who have particular possession types to facilitate trading. The game designer has informed you that there will likely be new types of possessions and ways to acquire them in the future. What kind of a data store would you recommend using?
- [ ] A. Transactional database
- [ ] B. Wide-column database
- [x] C. Document database
- [ ] D. Analytic database

```diff
+ C. The correct answer is C because the requirements call for a semi-structured schema. You will need to search players’ possessions and not just look them up using a single key because of the requirement for facilitating trading.
- Option A is not correct. Transactional databases have fixed schemas, and this use case calls for a semi-structured schema.
- Option B is incorrect because it does not support indexed lookup, which is needed for searching.
- Option D is incorrect. Analytical databases are structured data stores.
```

**7.** The CTO of your company wants to reduce the cost of running an HBase and Hadoop cluster on premises. Only one HBase application is run on the cluster. The cluster currently supports 10 TB of data, but it is expected to double in the next six months. Which of the following managed services would you recommend to replace the on-premises cluster in order to minimize migration and ongoing operational costs?
- [x] A. Cloud Bigtable using the HBase API
- [ ] B. Cloud Dataflow using the HBase API
- [ ] C. Cloud Spanner
- [ ] D. Cloud Datastore

```diff
+ A. The correct answer is A. Cloud Bigtable using the HBase API would minimize migration efforts, and since Bigtable is a managed service, it would help reduce operational costs.
- Option B is incorrect. Cloud Dataflow is a stream and batch processing service, not a database.
- Options C and D are incorrect. Relational databases are not likely to be appropriate choices for an HBase database, which is a wide-column NoSQL database, and trying to migrate from a wide-column to a relational database would incur unnecessary costs.
```

**8.** A genomics research institute is developing a platform for analyzing data related to genetic diseases. The genomics data is in a specialized format known as FASTQ, which stores nucleotide sequences and quality scores in a text format. Files may be up to 400 GB and are uploaded in batches. Once the files finish uploading, an analysis pipeline runs, reads the data in the FASTQ file, and outputs data to a database. What storage system is a good option for storing the uploaded FASTQ data?
- [ ] A. Cloud Bigtable
- [ ] B. Cloud Datastore
- [x] C. Cloud Storage
- [ ] D. Cloud Spanner

```diff
+ C. The correct answer is C because the FASTQ files are unstructured since their internal format is not used to organize storage structures. Also, 400 GB is large enough that it is not efficient to store them as objects in a database.
- Options A and B are incorrect because a NoSQL database is not needed for the given requirements.
- Similarly, there is no need to store the data in a structured database like Cloud Spanner, so Option D is incorrect.
```

**9.** A genomics research institute is developing a platform for analyzing data related to genetic diseases. The genomics data is in a specialized format known as FASTQ, which stores nucleotide sequences and quality scores in a text format. Once the files finish uploading, an analysis pipeline runs, reads the data in the FASTQ file, and outputs data to a database. The output is in tabular structure, the data is queried using SQL, and typically queries retrieve only a small number of columns but many rows. What database would you recom- mend for storing the output of the workflow?
- [ ] A. Cloud Bigtable
- [ ] B. Cloud Datastore
- [ ] C. Cloud Storage
- [x] D. BigQuery

```diff
+ D. The correct answer is D because the output is structured, will be queried with SQL, and will retrieve a large number of rows but few columns, making this a good use case for columnar storage, which BigQuery uses.
- Options A and B are not good options because neither database supports SQL.
- Option C is incorrect because Cloud Storage is used for unstructured data and does not support querying the contents of objects.
```

**10.** You are developing a new application and will be storing semi-structured data that will only be accessed by a single key. The total volume of data will be at least 40 TB. What GCP database service would you use?
- [ ] A. BigQuery
- [x] B. Bigtable
- [ ] C. Cloud Spanner
- [ ] D. Cloud SQL

```diff
+ B. The correct answer is B. Bigtable is a wide-column NoSQL database that supports semi- structured data and works well with datasets over 1 TB.
- Options A, D, and C are incorrect because they all are used for structured data.
- Option D is also incorrect because Cloud SQL does not currently scale to 40 TB in a single database.
```

**11.** A group of climate scientists is collecting weather data every minute from 10,000 sensors across the globe. Data often arrives near the beginning of a minute, and almost all data arrives within the first 30 seconds of a minute. The data ingestion process is losing some data because servers cannot ingest the data as fast as it is arriving. The scientists have scaled up the number of servers in their managed instance group, but that has not com- pletely eliminated the problem. They do not wish to increase the maximum size of the man- aged instance group. What else can the scientists do to prevent data loss?
- [ ] A. Write data to a Cloud Dataflow stream
- [x] B. Write data to a Cloud Pub/Sub topic
- [ ] C. Write data to Cloud SQL table
- [ ] D. Write data to Cloud Dataprep

```diff
+ B. The correct answer is B, write data to a Cloud Pub/Subtopic, which can scale automatically to existing workloads. The ingestion process can read data from the topic and data and then process it. Some data will likely accumulate early in every minute, but the ingestion process can catch up later in the minute after new data stops arriving.
- Option A is incorrect; Cloud Dataflow is a batch and stream processing service—it is not a message queue for buffering data.
- Option C is incorrect; Cloud SQL is not designed to scale for ingestion as needed in this example.
- Option D is incorrect; Cloud Dataprep is a tool for cleaning and preparing datasets for analysis.
```

**12.** A software developer asks your advice about storing data. The developer has hundreds of thousands of 1 KB JSON objects that need to be accessed in sub-millisecond times if pos- sible. All objects are referenced by a key. There is no need to look up values by the contents of the JSON structure. What kind of NoSQL database would you recommend?
- [x] A. Key-value database
- [ ] B. Analytical database
- [ ] C. Wide-column database
- [ ] D. Graph database

```diff
+ A. The correct answer is A. This is a good use case for key-value databases because the value is looked up by key only and the value is a JSON structure.
- Option B is incorrect. Analytical databases are not a type of NoSQL database.
- Option C is not a good option because wide-column databases work well with larger databases, typically in the terabyte range.
- Option D is incorrect because the data is not modeled as nodes and links, such as a network model.
```

**13.** A software developer asks your advice about storing data. The developer has hundreds of thousands of 10 KB JSON objects that need to be searchable by most attributes in the JSON structure. What kind of NoSQL database would you recommend?
- [ ] A. Key-value database
- [ ] B. Analytical database
- [ ] C. Wide-column database
- [x] D. Document database

```diff
+ D. The correct answer is D. A document database could store the volume of data, and it provides for indexing on columns other than a single key.
- Options A and C do not support indexing on non-key attributes.
- Option B is incorrect because analytical is not a type of NoSQL database.
```

**14.** A data modeler is designing a database to support ad hoc querying, including drilling down and slicing and dicing queries. What kind of data model is the data modeler likely to use?
- [ ] A. OLTP
- [x] B. OLAP
- [ ] C. Normalized
- [ ] D. Graph

```diff
+ B. The correct answer is B; OLAP data models are designed to support drilling down and slicing and dicing.
- Option A is incorrect; OLTP models are designed to facilitate storing, searching, and retrieving individual records in a database.
- Option C is incorrect; OLAP databases often employ denormalization.
- Option D is incorrect; graph data models are used to model nodes and their relationships, such as those in social networks.
```

**15.** A multinational corporation is building a global inventory database. The database will sup- port OLTP type transactions at a global scale. Which of the following would you consider as possible databases for the system?
- [ ] A. Cloud SQL and Cloud Spanner
- [ ] B. Cloud SQL and Cloud Datastore
- [x] C. Cloud Spanner only
- [ ] D. Cloud Datastore only

```diff
+ C. The correct answer is C. Cloud Spanner is the only globally scalable relational database for OLTP applications.
- Options A and B are incorrect because Cloud SQL will not meet the scaling requirements.
- Options B and D are incorrect because Cloud Datastore does not support OLTP models.
```
